
==========

>> Time travel concept: 
-- Query the state of the table as it was 5 minutes ago
SELECT * FROM my_dataset.my_table
FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE);

-- Query the state of the table as it was at a specific timestamp
SELECT * FROM my_dataset.my_table
FOR SYSTEM_TIME AS OF TIMESTAMP '2023-07-29 12:34:56 UTC';

===========

>> Creation of view and auth view: efficiently manage and secure your data access in BigQuery

Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data access / 
Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
Authorized views and authorized materialized views: let you share query results with particular users and groups without giving them access to the underlying source data.

CREATE VIEW my_dataset.joined_view AS
SELECT
  a.id,
  a.name,
  b.order_id,
  b.order_date
FROM
  my_dataset.customers a
JOIN
  my_dataset.orders b
ON
  a.id = b.customer_id;

============
>> Create BQ procedure:

CREATE OR REPLACE PROCEDURE `hsbc-9460919-opsanalytics-prod.WDP_work_prod.wdp_cin_account_global_proc`()
BEGIN

# Create BigQuery external table from dynamic file placed in GCS bucket
CREATE EXTERNAL TABLE IF NOT EXISTS            
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin` 
  OPTIONS ( format='PARQUET',
    uris=['gs://hadoop-gcs-centric-prod/cis_mas_con/WDP_CIN_Account_Global_*.parquet'] );

# Create BigQuery staging table from the external BigQuery Table to get NewFileDate
CREATE OR REPLACE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin` AS
SELECT
  *,
  _FILE_NAME AS file_name,
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date,
  CURRENT_TIMESTAMP() AS receive_timestamp
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin`; --create BigQuery staging table

# Iterate files from GCS bucket and apprnd data into target only when file data is new
FOR file_date IN (SELECT DISTINCT file_date FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin`)
DO
INSERT INTO `hsbc-9460919-opsanalytics-prod.WDP_data_prod.wdp_cin_account_global`
SELECT 
*
 FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin` 
 WHERE (SELECT file_date AS stagging_file_date) NOT IN (SELECT DISTINCT file_date FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.wdp_cin_account_global`); 
END FOR; -- if not empty, then newer data gets Appended and for existing data will not get action

# Drop external and staging tables from BigQuery
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin` ;
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin`;

END;


CREATE OR REPLACE PROCEDURE `hsbc-9460919-opsanalytics-prod.WDP_work_prod.cis_constituents_mda_global_proc`()
BEGIN

declare NewFileDate date default null;      --> in our case it will be the temp table where Suresh will insert the GCS file
declare ExistingFileDate date default null; --> in our case it will be our destination dimension mastergroup table 

# Create BigQuery external table from dynamic file placed in GCS bucket
CREATE EXTERNAL TABLE IF NOT EXISTS            
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents` 
  OPTIONS ( format='PARQUET',
    uris=['gs://hadoop-gcs-centric-prod/cis_mas_con/cis_constituents_mda_GLOBAL_*.parquet'] );

# Create BigQuery staging table from the external BigQuery Table to get NewFileDate
CREATE TABLE IF NOT EXISTS
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` AS
SELECT
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents`;

--------------------------------------------------------------------------------------------------------------------------------------------
set NewFileDate =      ( select max(file_date) from `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` );    # NewFile (has to be the same as chosen above

set ExistingFileDate = ( select max(file_date) from `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global` );    # ExistingFile

# SCENARIO 1: if not empty, but older data no action
if NewFileDate <= ExistingFileDate 
then select 'SC1 | GSC bucket not empty | file_date same or older than existing.';
end if; 

# SCENARIO 2: if not empty, and newer data then truncate & load
if NewFileDate > ExistingFileDate or ExistingFileDate is null
THEN 

SELECT 'SC2| GSC bucket not empty | file_date newer than existing.';

CREATE OR REPLACE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` AS
SELECT
  *,
  _FILE_NAME AS file_name,
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date,
  CURRENT_TIMESTAMP() AS receive_timestamp
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents`; --create BigQuery staging table

TRUNCATE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global`;   --truncate table EXISTING

INSERT INTO `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global`
SELECT
  *
FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` where file_date = NewFileDate; --insert into EXISTING 
end if; 

# Drop external and staging tables from BigQuery
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents` ;
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents`;

END;

=========================
GCP Bigquery:
DDL(Data Definition Language) – define structure/schema of database (Create/alter/drop/truncate)
DML(Data Manipulation Language) – manipulate data (Insert/update/delete/merge)
DQL(Data Query Language) – select 
TCL(Transaction Control Language) – commit transaction and role back in case of any errors
DCL(Data Control Language) – for security and access control 

==================
Create table with metadata from existing table – use where 1 = 2 or false

==================
Keywords: 
except / distinct / limit / Round /
concat / left / right / substring / upper / lower / replace / ltrim / rtrim/ trim / contains_substr() / length() / starts_with() / ends_with() / strpos()
Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

====================

Permanent table: save query results on BQ storage
Temporary table/cache – Query results saved on cache and valid for 24 hrs.
Internal/native table/managed tables: table in BQ storage
External table: BQ querying data from bigtable, gcs, google drive

====================
Partitioning and clustering:

Improved Query Performance: By scanning only relevant partitions and clustered blocks, queries run faster.
Cost Efficiency: Reduced data scanned means lower costs, as BigQuery charges based on the amount of data processed.

>> based on ingestion-time
CREATE TABLE my_dataset.my_table
PARTITION BY _PARTITIONTIME
AS SELECT name, value FROM source_table;

>> based on timestame/ date
CREATE TABLE my_dataset.my_table
PARTITION BY date
CLUSTER BY name, value
AS SELECT name, value, date FROM source_table;

>> based on integer column
CREATE TABLE my_dataset.my_table
PARTITION BY RANGE_BUCKET(range, GENERATE_ARRAY(1, 100, 10))
AS SELECT name, value, range FROM source_table;

===================

Joins: -  combine rows from two or more tables based on a related column between them.

INNER JOIN / left join (or) left outer join / right join (or) right outer join
/ full join (or) full outer join / cross join / self join /

SELECT a.*, b.*
FROM table_a a
INNER JOIN table_b b
ON a.common_column = b.common_column;

>> Best Practices for Joins in BigQuery
Use Appropriate Join Types: / Filter Early / Avoid Cross Joins When Possible / 
Use Partitioned and Clustered Tables: If joining large tables / Monitor Query Performance /

 ====================

>> With statement:
also known as Common Table Expressions (CTEs), allows you to define temporary result sets.
Adv: code more readable, resusebility - referenced in several places within the main SQL query. 

WITH sales_data AS (
  SELECT product_id, quantity, price, quantity * price AS total_sales
  FROM sales
),
product_totals AS (
  SELECT product_id, SUM(total_sales) AS total_sales
  FROM sales_data
  GROUP BY product_id
),
top_selling_products AS (
  SELECT product_id, total_sales
  FROM product_totals
  ORDER BY total_sales DESC
  LIMIT 10
)
SELECT p.product_name, tsp.total_sales
FROM top_selling_products tsp
JOIN products p ON tsp.product_id = p.product_id;

======================

conversion functions:
>> The CAST function performs a conversion between compatible data types. 
If the conversion fails due to data type incompatibility or data loss, the query will fail

>> The SAFE_CAST function also performs a conversion between compatible data types, 
but it handles conversion errors differently. Instead of failing the query, it returns NULL when the conversion cannot be performed.

======================

commenting:
-- or # or /* LINES */

==================
If condition
then
sql statement;
end if;

for var in list
do 
sql statement;
end for;


/ labels /repeat / while /leave/ break/ continue/ iterate 



 
======================

>> Pivoting and unpivoting: help in reshaping your data for analysis or reporting

>> Pivot: row into columns

>> unpivot: columns into rows
SELECT
  product,
  region,
  sales
FROM
  sales_pivoted
UNPIVOT(sales FOR region IN (North, South));

========================

>> Arrays – list of items having same data type
SELECT element
FROM mydataset.mytable, UNNEST(my_array) AS element;

>> Struct – record with nested fields having different data types
SELECT person.id, person.name
FROM mydataset.mytable;

Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
De Normalization – redundant data (high storage cost) – better Query performance 
Why array & structs? – for better query per and lesser storage costs
 
==========================

Window functions:

>> max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn)
SELECT
  column1,
  column2,
  SUM(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total,
  AVG(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS moving_avg
FROM
  mydataset.mytable;

>> Row_number() / rank() / dense_rank() 
SELECT
  column1,
  column2,
  ROW_NUMBER() OVER (PARTITION BY column1 ORDER BY column2) AS row_num
FROM
  mydataset.mytable;

>> lag(cn) / lead(cn): Accesses data from a previous or subsequent row in the same result set without the use of a self-join.

SELECT
  column1,
  column2,
  LAG(column2, 1) OVER (PARTITION BY column1 ORDER BY column2) AS previous_value,
  LEAD(column2, 1) OVER (PARTITION BY column1 ORDER BY column2) AS next_value
FROM
  mydataset.mytable;

>> first_value(cn) / last_value(cn) / nth_value(cn, n) 
SELECT
  column1,
  column2,
  FIRST_VALUE(column2) OVER (PARTITION BY column1 ORDER BY column2) AS first_value,
  LAST_VALUE(column2) OVER (PARTITION BY column1 ORDER BY column2) AS last_value
FROM
  mydataset.mytable;

>> ntile(n) – n groups 
SELECT
  column1,
  column2,
  NTILE(4) OVER (PARTITION BY column1 ORDER BY column2) AS quartile
FROM
  mydataset.mytable;

>> cum_dist() / percent_rank() 

Rows consider current row even if duplicates / range considers bottom row of duplicates.

=====================

Regular expressions: pattern matching
= . match any single char except \n
= \. Match symbol after \
= [0-3]
= [^a-c]
= ^ MATCH AT BEGINNING
= $ MATCH AT END
= ab*c match 0 or more occurrences
= ab+c match 1 or more occurrences
= ab?c match 0 or one time
= {m, n} repitations preceding m to n
= () group

===================

>> Sql query execution order: 
From / where / group by / having / select / order by / limit

==================

>> Big Query compute optimization: on-demaned analysis, bq editions - decide based on your workloads.
>> BQ data storage – (Billing model: logical / physical) - based on compression ratio

>> Query optimization 
Use Partitioned Tables and Clustered Tables
select columns instead of *
Use WHERE Clauses for Filtering
Optimize JOIN Operations
Avoid Nondeterministic Functions: CURRENT_TIMESTAMP()) in SELECT statements; 
Aggregate Data at the Source: Use Approximate Aggregations:  
Optimize Window Functions
Monitor and Tune Query Performance

>> Cost Controls: Set budget alerts and quotas
>> Schedule queries during off-peak hours to take advantage of low on-demand pricing typically at mid-night/early morning

==================









