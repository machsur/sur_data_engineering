
==========

>> Time travel concept: 
-- Query the state of the table as it was 5 minutes ago
SELECT * FROM my_dataset.my_table
FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE);

-- Query the state of the table as it was at a specific timestamp
SELECT * FROM my_dataset.my_table
FOR SYSTEM_TIME AS OF TIMESTAMP '2023-07-29 12:34:56 UTC';

===========

>> Creation of view and auth view: efficiently manage and secure your data access in BigQuery

CREATE VIEW my_dataset.joined_view AS
SELECT
  a.id,
  a.name,
  b.order_id,
  b.order_date
FROM
  my_dataset.customers a
JOIN
  my_dataset.orders b
ON
  a.id = b.customer_id;

============
Create BQ procedure:

CREATE OR REPLACE PROCEDURE `hsbc-9460919-opsanalytics-prod.WDP_work_prod.wdp_cin_account_global_proc`()
BEGIN

# Create BigQuery external table from dynamic file placed in GCS bucket
CREATE EXTERNAL TABLE IF NOT EXISTS            
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin` 
  OPTIONS ( format='PARQUET',
    uris=['gs://hadoop-gcs-centric-prod/cis_mas_con/WDP_CIN_Account_Global_*.parquet'] );

# Create BigQuery staging table from the external BigQuery Table to get NewFileDate
CREATE OR REPLACE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin` AS
SELECT
  *,
  _FILE_NAME AS file_name,
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date,
  CURRENT_TIMESTAMP() AS receive_timestamp
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin`; --create BigQuery staging table

# Iterate files from GCS bucket and apprnd data into target only when file data is new
FOR file_date IN (SELECT DISTINCT file_date FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin`)
DO
INSERT INTO `hsbc-9460919-opsanalytics-prod.WDP_data_prod.wdp_cin_account_global`
SELECT 
*
 FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin` 
 WHERE (SELECT file_date AS stagging_file_date) NOT IN (SELECT DISTINCT file_date FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.wdp_cin_account_global`); 
END FOR; -- if not empty, then newer data gets Appended and for existing data will not get action

# Drop external and staging tables from BigQuery
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin` ;
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin`;

END;


CREATE OR REPLACE PROCEDURE `hsbc-9460919-opsanalytics-prod.WDP_work_prod.cis_constituents_mda_global_proc`()
BEGIN

declare NewFileDate date default null;      --> in our case it will be the temp table where Suresh will insert the GCS file
declare ExistingFileDate date default null; --> in our case it will be our destination dimension mastergroup table 

# Create BigQuery external table from dynamic file placed in GCS bucket
CREATE EXTERNAL TABLE IF NOT EXISTS            
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents` 
  OPTIONS ( format='PARQUET',
    uris=['gs://hadoop-gcs-centric-prod/cis_mas_con/cis_constituents_mda_GLOBAL_*.parquet'] );

# Create BigQuery staging table from the external BigQuery Table to get NewFileDate
CREATE TABLE IF NOT EXISTS
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` AS
SELECT
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents`;

--------------------------------------------------------------------------------------------------------------------------------------------
set NewFileDate =      ( select max(file_date) from `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` );    # NewFile (has to be the same as chosen above

set ExistingFileDate = ( select max(file_date) from `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global` );    # ExistingFile

# SCENARIO 1: if not empty, but older data no action
if NewFileDate <= ExistingFileDate 
then select 'SC1 | GSC bucket not empty | file_date same or older than existing.';
end if; 

# SCENARIO 2: if not empty, and newer data then truncate & load
if NewFileDate > ExistingFileDate or ExistingFileDate is null
THEN 

SELECT 'SC2| GSC bucket not empty | file_date newer than existing.';

CREATE OR REPLACE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` AS
SELECT
  *,
  _FILE_NAME AS file_name,
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date,
  CURRENT_TIMESTAMP() AS receive_timestamp
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents`; --create BigQuery staging table

TRUNCATE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global`;   --truncate table EXISTING

INSERT INTO `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global`
SELECT
  *
FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` where file_date = NewFileDate; --insert into EXISTING 
end if; 

# Drop external and staging tables from BigQuery
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents` ;
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents`;

END;

=========================
GCP Bigquery:
DDL(Data Definition Language) – define structure/schema of database (Create/alter/drop/truncate)
DML(Data Manipulation Language) – manipulate data (Insert/update/delete/merge)
DQL(Data Query Language) – select 
TCL(Transaction Control Language) – commit transaction and role back in case of any errors
DCL(Data Control Language) – for security and access control 

==================
Create table with metadata from existing table – use where 1 = 2 or false

==================
Keywords: 
except / distinct / limit / Round /
concat / left / right / substring / upper / lower / replace / ltrim / rtrim/ trim / contains_substr() / length() / starts_with() / ends_with() / strpos()
Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

====================

Permanent table: save query results on BQ storage
Temporary table/cache – Query results saved on cache and valid for 24 hrs.
Internal/native table/managed tables: table in BQ storage
External table: BQ querying data from bigtable, gcs, google drive

====================
Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data access / 
Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
Authorized views and authorized materialized views: let you share query results with particular users and groups without giving them access to the underlying source data.

====================
Partitioning and clustering:

 
===================

Joins: -  INNER JOIN (or) JOIN / right join (or) right outer join / left join (or) left outer join / full join (or) full outer join / cross join

====================
With statement
sub-query block - which can be referenced in several places within the main SQL query. 
Adv: better performance, code more readable.

======================

conversion functions:
The CAST function performs a conversion between compatible data types. If the conversion fails due to data type incompatibility or data loss, the query will fail

The SAFE_CAST function also performs a conversion between compatible data types, but it handles conversion errors differently. Instead of failing the query, it returns NULL when the conversion cannot be performed.


======================
commenting:
-- or # or /* LINES */

==================
If / labels /repeat / while /leave/ break/ continue/ iterate / for
 


 

 



Pivot / unpivot

Arrays – list of items having same data type
Struct – record with nested fields having different data types
 

Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
De Normalization – redundant data (high storage cost) – better Query performance 
Why array & structs? – for better query per and lesser storage costs

 


Window functions:
= max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn)
= Row_number() / rank() / dense_rank() 
= lag(cn) / lead(cn) 
= first_value(cn) / last_value(cn) / nth_value(cn, n) 
= ntile(n) – n groups 
= cum_dist() / percent_rank() /
SUM(column2) OVER (PARTITION BY column3 ORDER BY column4 ROWS BETWEEN UNBOUNDED PRECEDING or 2 PRECEDING or CURRENT ROW AND CURRENT ROW or UNBOUNDED FOLLOWING or 1 FOLLOWING) AS sum_column2
Rows consider current row even if duplicates / range considers bottom row of duplicates.

Regular expressions: pattern matching
= . match any single char except \n
= \. Match symbol after \
= [0-3]
= [^a-c]
= ^ MATCH AT BEGINNING
= $ MATCH AT END
= ab*c match 0 or more occurrences
= ab+c match 1 or more occurrences
= ab?c match 0 or one time
= {m, n} repitations preceding m to n
= () group


Conditions & loops:

= if / case in column use
= if
= while
= for

Sql query execution order: 

From / where / group by / having / select / order by / limit



 
 


Big Query cost optimization:
BQ data storage –(Billing model: logical / physical)
Partitioning & clustering
Data lifecycle management

Query optimization 
(Use Partitioned Tables: Clustered Tables: Selectively Project Columns: Optimize JOIN Operations: Use WHERE Clauses for Filtering: Avoid Nondeterministic Functions: CURRENT_TIMESTAMP()) in SELECT statements; Aggregate Data at the Source: Use Approximate Aggregations: Utilize approximate aggregation functions (e.g., APPROX_COUNT_DISTINCT(), APPROX_QUANTILES()); Optimize Window Functions: Monitor and Tune Query Performance: )

Big query compute costs(on-demand, flat-rate/bq editions)
Query Caching
Cost Controls: Set budget alerts and quotas
Schedule queries during off-peak hours to take advantage of low on-demand pricing









