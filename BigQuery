
==========

>> Time travel concept: 
-- Query the state of the table as it was 5 minutes ago
SELECT * FROM my_dataset.my_table
FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 5 MINUTE);

-- Query the state of the table as it was at a specific timestamp
SELECT * FROM my_dataset.my_table
FOR SYSTEM_TIME AS OF TIMESTAMP '2023-07-29 12:34:56 UTC';

===========

>> Creation of view and auth view: efficiently manage and secure your data access in BigQuery

Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data access / 
Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
Authorized views and authorized materialized views: let you share query results with particular users and groups without giving them access to the underlying source data.

CREATE VIEW my_dataset.joined_view AS
SELECT
  a.id,
  a.name,
  b.order_id,
  b.order_date
FROM
  my_dataset.customers a
JOIN
  my_dataset.orders b
ON
  a.id = b.customer_id;

============
>> Create BQ procedure:

CREATE OR REPLACE PROCEDURE `hsbc-9460919-opsanalytics-prod.WDP_work_prod.wdp_cin_account_global_proc`()
BEGIN

# Create BigQuery external table from dynamic file placed in GCS bucket
CREATE EXTERNAL TABLE IF NOT EXISTS            
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin` 
  OPTIONS ( format='PARQUET',
    uris=['gs://hadoop-gcs-centric-prod/cis_mas_con/WDP_CIN_Account_Global_*.parquet'] );

# Create BigQuery staging table from the external BigQuery Table to get NewFileDate
CREATE OR REPLACE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin` AS
SELECT
  *,
  _FILE_NAME AS file_name,
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date,
  CURRENT_TIMESTAMP() AS receive_timestamp
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin`; --create BigQuery staging table

# Iterate files from GCS bucket and apprnd data into target only when file data is new
FOR file_date IN (SELECT DISTINCT file_date FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin`)
DO
INSERT INTO `hsbc-9460919-opsanalytics-prod.WDP_data_prod.wdp_cin_account_global`
SELECT 
*
 FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin` 
 WHERE (SELECT file_date AS stagging_file_date) NOT IN (SELECT DISTINCT file_date FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.wdp_cin_account_global`); 
END FOR; -- if not empty, then newer data gets Appended and for existing data will not get action

# Drop external and staging tables from BigQuery
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_account_cin` ;
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_account_cin`;

END;


CREATE OR REPLACE PROCEDURE `hsbc-9460919-opsanalytics-prod.WDP_work_prod.cis_constituents_mda_global_proc`()
BEGIN

declare NewFileDate date default null;      --> in our case it will be the temp table where Suresh will insert the GCS file
declare ExistingFileDate date default null; --> in our case it will be our destination dimension mastergroup table 

# Create BigQuery external table from dynamic file placed in GCS bucket
CREATE EXTERNAL TABLE IF NOT EXISTS            
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents` 
  OPTIONS ( format='PARQUET',
    uris=['gs://hadoop-gcs-centric-prod/cis_mas_con/cis_constituents_mda_GLOBAL_*.parquet'] );

# Create BigQuery staging table from the external BigQuery Table to get NewFileDate
CREATE TABLE IF NOT EXISTS
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` AS
SELECT
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents`;

--------------------------------------------------------------------------------------------------------------------------------------------
set NewFileDate =      ( select max(file_date) from `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` );    # NewFile (has to be the same as chosen above

set ExistingFileDate = ( select max(file_date) from `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global` );    # ExistingFile

# SCENARIO 1: if not empty, but older data no action
if NewFileDate <= ExistingFileDate 
then select 'SC1 | GSC bucket not empty | file_date same or older than existing.';
end if; 

# SCENARIO 2: if not empty, and newer data then truncate & load
if NewFileDate > ExistingFileDate or ExistingFileDate is null
THEN 

SELECT 'SC2| GSC bucket not empty | file_date newer than existing.';

CREATE OR REPLACE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` AS
SELECT
  *,
  _FILE_NAME AS file_name,
  PARSE_DATE('%Y%m%d', REGEXP_EXTRACT(_FILE_NAME, r'_(\d{8})\.parquet')) AS file_date,
  CURRENT_TIMESTAMP() AS receive_timestamp
FROM
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents`; --create BigQuery staging table

TRUNCATE TABLE `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global`;   --truncate table EXISTING

INSERT INTO `hsbc-9460919-opsanalytics-prod.WDP_data_prod.cis_constituents_mda_global`
SELECT
  *
FROM `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents` where file_date = NewFileDate; --insert into EXISTING 
end if; 

# Drop external and staging tables from BigQuery
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.external_table_constituents` ;
DROP TABLE
  `hsbc-9460919-opsanalytics-prod.WDP_data_prod.stagging_table_constituents`;

END;

=========================
GCP Bigquery:
DDL(Data Definition Language) – define structure/schema of database (Create/alter/drop/truncate)
DML(Data Manipulation Language) – manipulate data (Insert/update/delete/merge)
DQL(Data Query Language) – select 
TCL(Transaction Control Language) – commit transaction and role back in case of any errors
DCL(Data Control Language) – for security and access control 

==================
Create table with metadata from existing table – use where 1 = 2 or false

==================
Keywords: 
except / distinct / limit / Round /
concat / left / right / substring / upper / lower / replace / ltrim / rtrim/ trim / contains_substr() / length() / starts_with() / ends_with() / strpos()
Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

====================

Permanent table: save query results on BQ storage
Temporary table/cache – Query results saved on cache and valid for 24 hrs.
Internal/native table/managed tables: table in BQ storage
External table: BQ querying data from bigtable, gcs, google drive

====================
Partitioning and clustering:

Improved Query Performance: By scanning only relevant partitions and clustered blocks, queries run faster.
Cost Efficiency: Reduced data scanned means lower costs, as BigQuery charges based on the amount of data processed.

>> based on ingestion-time
CREATE TABLE my_dataset.my_table
PARTITION BY _PARTITIONTIME
AS SELECT name, value FROM source_table;

>> based on timestame/ date
CREATE TABLE my_dataset.my_table
PARTITION BY date
CLUSTER BY name, value
AS SELECT name, value, date FROM source_table;

>> based on integer column
CREATE TABLE my_dataset.my_table
PARTITION BY RANGE_BUCKET(range, GENERATE_ARRAY(1, 100, 10))
AS SELECT name, value, range FROM source_table;

===================

Joins: -  combine rows from two or more tables based on a related column between them.

INNER JOIN / left join (or) left outer join / right join (or) right outer join
/ full join (or) full outer join / cross join / self join /

SELECT a.*, b.*
FROM table_a a
INNER JOIN table_b b
ON a.common_column = b.common_column;

>> Best Practices for Joins in BigQuery
Use Appropriate Join Types: / Filter Early / Avoid Cross Joins When Possible / 
Use Partitioned and Clustered Tables: If joining large tables / Monitor Query Performance /

 ====================

>> With statement:
also known as Common Table Expressions (CTEs), allows you to define temporary result sets.
Adv: code more readable, resusebility - referenced in several places within the main SQL query. 

WITH sales_data AS (
  SELECT product_id, quantity, price, quantity * price AS total_sales
  FROM sales
),
product_totals AS (
  SELECT product_id, SUM(total_sales) AS total_sales
  FROM sales_data
  GROUP BY product_id
),
top_selling_products AS (
  SELECT product_id, total_sales
  FROM product_totals
  ORDER BY total_sales DESC
  LIMIT 10
)
SELECT p.product_name, tsp.total_sales
FROM top_selling_products tsp
JOIN products p ON tsp.product_id = p.product_id;

======================

conversion functions:
>> The CAST function performs a conversion between compatible data types. 
If the conversion fails due to data type incompatibility or data loss, the query will fail

>> The SAFE_CAST function also performs a conversion between compatible data types, 
but it handles conversion errors differently. Instead of failing the query, it returns NULL when the conversion cannot be performed.

======================

commenting:
-- or # or /* LINES */

==================
If / labels /repeat / while /leave/ break/ continue/ iterate / for



 
======================

Pivot / unpivot

Arrays – list of items having same data type
Struct – record with nested fields having different data types
 

Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
De Normalization – redundant data (high storage cost) – better Query performance 
Why array & structs? – for better query per and lesser storage costs

 
==========================

Window functions:
= max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn)
= Row_number() / rank() / dense_rank() 
= lag(cn) / lead(cn) 
= first_value(cn) / last_value(cn) / nth_value(cn, n) 
= ntile(n) – n groups 
= cum_dist() / percent_rank() /
SUM(column2) OVER (PARTITION BY column3 ORDER BY column4 ROWS BETWEEN UNBOUNDED PRECEDING or 2 PRECEDING or CURRENT ROW AND CURRENT ROW or UNBOUNDED FOLLOWING or 1 FOLLOWING) AS sum_column2
Rows consider current row even if duplicates / range considers bottom row of duplicates.

=====================

Regular expressions: pattern matching
= . match any single char except \n
= \. Match symbol after \
= [0-3]
= [^a-c]
= ^ MATCH AT BEGINNING
= $ MATCH AT END
= ab*c match 0 or more occurrences
= ab+c match 1 or more occurrences
= ab?c match 0 or one time
= {m, n} repitations preceding m to n
= () group

===================

>> Sql query execution order: 
From / where / group by / having / select / order by / limit

==================

>> Big Query cost optimization:
BQ data storage –(Billing model: logical / physical)
Partitioning & clustering
Data lifecycle management

Query optimization 
(Use Partitioned Tables: Clustered Tables: Selectively Project Columns: Optimize JOIN Operations: Use WHERE Clauses for Filtering: Avoid Nondeterministic Functions: CURRENT_TIMESTAMP()) in SELECT statements; Aggregate Data at the Source: Use Approximate Aggregations: Utilize approximate aggregation functions (e.g., APPROX_COUNT_DISTINCT(), APPROX_QUANTILES()); Optimize Window Functions: Monitor and Tune Query Performance: )

Big query compute costs(on-demand, flat-rate/bq editions)
Query Caching
Cost Controls: Set budget alerts and quotas
Schedule queries during off-peak hours to take advantage of low on-demand pricing

==================









