
======================================================================== BigQuery ==================================================================

>> Bigquery arch: storage and compute - decoupled/isolated (cost efficient)
   colossus - distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus;    borg - scheduling jobs like run queries

>> Fact Table: Contains numeric values (quantitative data - sales, revenue, profit, etc), foreign keys, Grows rapidly
   Dimension Table: contains textual or categorical data, smaller and changes less frequently.

>> Star Schema: Multiple Dimension Tables directly connected to the fact table
   Snowflake Schema: Dimension tables are split into sub-dimensions, reducing redundancy, more normalized form. 

>> Change Data Capture -->> identify & capture changes made to data in DB --> log based - very efficient, query based on timestamp, triggers load changes into new tables

>> SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
   SCD1 - Overwrite - Updates the existing record with new data - No history is preserved.
   SCD2 - Add New Row  -- Keeps full history by adding a new row with versioning or effective dates.
   SCD3 - Add New Column -- Keeps limited history by adding a new column for the previous value | SCD4 - scd1+scd2 - Current Table & History Table

>> to get expensive queries, metadata about datasets:  SELECT * FROM `project_id`.`region-europe-west2`.INFORMATION_SCHEMA..JOBS/SCHEMATA;
   to get metadata about tables/columns: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.TABLES/.COLUMNS;

>> Temporary table - intermediate data proce and caching | Internal table- freq accessed and processed data | External table - for occasional queries on large datasets.

>>  Normalization – avoid redundant data, poor query performance |  De Normalization – redundant data (high storage cost) – better Query performance 
    Why array & structs? – for better query per and lesser storage costs
    Arrays – list of items having same data type  -  SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element; 
    Struct – record with nested fields having different data types  -   SELECT person.id, person.name FROM mydataset.mytable;

>> bq slots: units of computational capacity -CPU and memory, Reservations: allow you to purchase dedicated slots (called commitments) 

>> Big Query cost optimization
   bq compute optimization: on-demaned pricing, Capacity Pricing (slots reserve)
   BQ data storage: Billing model: logical/physical, Use Table Expiration, Use Long-Term Storage Pricing (> 90 days)
   query Opti: Agg at Source,  use avro, dataset at cust loc, Parti & Clust, preview, select col, use trun, Join, where, late agg, Caching, mat view for freq queries

>> Cost Controls: budget alerts, Set query usage limits, export Billing reports, Schedule queries during off-peak hours

=========================
>> Syntax
DATE(DC in iso) / PARSE_DATE(format, DC) / SAFE.PARSE_DATE(format, DC)	-	Y, y / m, B, b / d, A, a / 
FORMAT_DATE('%B %d, %Y', sale_date)
extract(year from date)      						-     	year / month / day / hour / minute / second / microsecond /
DATE_TRUNC(order_date, MONTH) 
DATE_DIFF(order_date, previous_date, day) 
Date_add(order_date, INTERVAL 5 day) / Date_sub(order_date, INTERVAL 5 day)
>> SQL Execution order: FROM / WHERE / GROUP BY$Agg / HAVING / WINDOW functions like ROW_NUMBER() / SELECT / DISTINCT / ORDER BY / LIMIT or OFFSET
>> view, materialized view and auth view (share results without giving access to source data): efficiently manage and secure your data access in BigQuery
>> Partitioning (single column, 4000 max) and clustering (sorts parti data in storage blocks, four col): Improved Query Performance, Cost Efficiency
>> Joins: - INNER JOIN / left join / right join / full outer join / cross join /
>> Time travel concept: SELECT * FROM `your_dataset.your_table` FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)
>> Window:   SUM(sales) OVER (PARTITION BY id ORDER BY updated_at ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rolling_sum
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /  first_value(cn) / last_value(cn) / nth_value(cn, n) /
   row_number() / rank() / dense_rank() /   lag(cn) / lead(cn) / ntile(n) – n groups /
   CUME_DIST() = Number of rows with values less than or equal to the current row / Total number of rows /   PERCENT_RANK() = Rank of current row−1 / Total rows−1
>> WITH CustomOrder AS (SELECT * FROM `p.d.table`) | SELECT * FROM CustomOrder ORDER BY emp_dept_sort, row_num;
>> MERGE INTO target_table AS T  USING source_table AS S  ON T.id = S.id
   WHEN MATCHED THEN UPDATE SET T.name = S.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);
>> Pivot: row into columns 	- product| month| sales  	- select * from table pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar')); --> or use case statement
   SELECT customer_id, SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales, .... FROM orders GROUP BY customer_id;
   unpivot: columns into rows 	- product | Jan | Feb | Mar	- select product, month, sales from table unpivot(sales for month in (Jan, Feb, Mar));
>> Union all / Union distinct  / Except distinct / Intersect distinct / Table wildcards: test_*` where _Table_Suffix > 5
>> SPLIT(full_name, ' ')[OFFSET(0)]
>> Length	Upper	Lower	substr 	Trim	Replace		Concat		STRPOS		Starts_With 		Ends_With	
	SAFE_CAST/CAST    	coalesce		ifnull/E   	E/concat_ws		
	round 	abs		ceil 	floor	safe_devide/when	
	if		case	
	generate_uuid()

===========================

>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> What are the different ways to remove duplicate records ---> distinct() / row_number() / groupBy /
>> find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 
>> split the data into two columns (Even, Odd) 
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> Retrieve employees who joined in the last 6 months.
>> Identify employees whose salary is greater than the average salary of their department.
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> Get the first and last transaction date for each customer.
>> Replace null values in a column with the last non-null value in that partition. -----> last_value('cn', ignorenulls=True).over(window_spec)
>> Categorize employees based on years of experience into Junior, Mid, and Senior levels.
>> Fill the mean salary value in Null. ---> avg_sal = df.select(mean(col('sal'))).collect()[0][0] --> df.fillna({'salary': avg_salary}).show() / avg_sal and cross join
>> ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]
	Replace all NULL values in the Quantity column with 0 / Price column with the average price of the existing data / Fill missing Sales_Date with '2025-01-01'. / 
    Drop rows where the Product column is NULL / Drop rows where all columns are NULL.
>> find max id excluding duplicates | 2, 6, 5, 6, 9, 9, 8: with unique_ids AS (SELECT id FROM id_list GROUP BY id HAVING COUNT(*) = 1) | SELECT MAX(id) FROM unique_ids
>> table1: 1 2 3 null ""  & table2: 2 2  3 null null "" : inner join - 2 2, 2 2, 3 3, empty empty |  Null doesnt include matching, empty string includes matching
>> employees who hired in last 90 days: 
>> number of patients per doctor including unassigned patients: docter_id | pationt_id, doctor_id
>> Handling NULL values using the average salary | emp_id, salary: IFNULL(cn, rep_val) COALESCE(cn, rep_val1, rep_val2, ...) 
>> emp earning more than avg sal in their department: 

>> column Country: India Australia Pakistan | Output: India vs Australia India vs Pakistan Australia vs Pakistan
	df.alias("a").crossJoin(df.alias("b")).filter(col("a.Country") < col("b.Country")).selectExpr("concat(a.Country, ' vs ', b.Country) as Matchup")
    SELECT a.country || ' vs ' || b.country as country FROM country a  join country b on a.country > b.country;  
>> ORDER_DAY, ORDER_ID, PRODUCT_ID, QUANTITY, PRICE | Get me all products that got sold both the days and the number of times the product is sold.  
>> find customers who made transactions in every month of the year | cust_id, date, purchage_amount
>> employees sal > managers | EMPID, EMPNAME, MANAGERID, SLARY: 
>> Extract the Domain from the Email column:	substring(email, strpos(email, '@')+1, length-opt) 
>> Find all employees whose names contain the letters "a" exactly twice: SELECT * FROM employees WHERE LENGTH(name) - LENGTH(REPLACE(LOWER(name),'a','')) = 2
>> Write a query to count how many employees share the same salary.
>> Word count program in pyspark rdd & DF
>> count the number of null values in each column of df. 	df.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()
>> display only last 4 characters of card number. data = [(1,'Rahul',1234567891234567),(2,'Raj',1234567892345678)] schema = "id int, name string, card_no long"
	maskfun_udf = udf(maskfun, StringType()) 	 /	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))
>> extract filename: df = df.withColumn("state", element_at(split(input_file_name(), "/"), -1)).withColumn("state", split("state", "\\.").getItem(0))
