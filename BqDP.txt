
======================================================================== BigQuery ==================================================================

===============> Concepts

>> Bigquery arch: storage and compute - decoupled/isolated (cost efficient)
   colossus - distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus;    borg - scheduling jobs like run queries
>> Fact Table: Contains numeric values (quantitative data - sales, revenue, profit, etc), foreign keys, Grows rapidly
   Dimension Table: contains textual or categorical data, smaller and changes less frequently.
>> Star Schema: Multiple Dimension Tables directly connected to the fact table
   Snowflake Schema: Dimension tables are split into sub-dimensions, reducing redundancy, more normalized form. 
>> Change Data Capture -->> identify & capture changes made to data in DB --> log based - very efficient, query based on timestamp, triggers load changes into new tables
>> SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
   SCD1 - Overwrite - Updates the existing record with new data - No history is preserved.
   SCD2 - Add New Row  -- Keeps full history by adding a new row with versioning or effective dates.
   SCD3 - Add New Column -- Keeps limited history by adding a new column for the previous value | SCD4 - scd1+scd2 - Current Table & History Table
>> to get expensive queries, metadata about datasets:  SELECT * FROM `project_id`.`region-europe-west2`.INFORMATION_SCHEMA..JOBS/SCHEMATA;
   to get metadata about tables/columns: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.TABLES/.COLUMNS;
>> Temporary table - intermediate data proce and caching | Internal table- freq accessed and processed data | External table - for occasional queries on large datasets.
>>  Normalization â€“ avoid redundant data, poor query performance |  De Normalization â€“ redundant data (high storage cost) â€“ better Query performance 
    Why array & structs? â€“ for better query per and lesser storage costs
    Arrays â€“ list of items having same data type  -  SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element; 
    Struct â€“ record with nested fields having different data types  -   SELECT person.id, person.name FROM mydataset.mytable;
>> bq slots: units of computational capacity -CPU and memory, Reservations: allow you to purchase dedicated slots (called commitments) 
>> Big Query cost optimization
   bq compute optimization: on-demaned pricing, Capacity Pricing (slots reserve)
   BQ data storage: Billing model: logical/physical, Use Table Expiration, Use Long-Term Storage Pricing (> 90 days)
   query Opti: Agg at Source,  use avro, dataset at cust loc, Parti & Clust, preview, select col, use trun, Join, where, late agg, Caching, mat view for freq queries
>> Cost Controls: budget alerts, Set query usage limits, export Billing reports, Schedule queries during off-peak hours
 

================>> Syntax

DATE(DC in iso) / PARSE_DATE(format, DC) / SAFE.PARSE_DATE(format, DC)	-	Y, y / m, B, b / d, A, a / 
FORMAT_DATE('%B %d, %Y', sale_date)
extract(year from date)      						-     	year / month / day / hour / minute / second / microsecond /
DATE_TRUNC(order_date, MONTH) 
DATE_DIFF(order_date, previous_date, day) 
Date_add(order_date, INTERVAL 5 day) / Date_sub(order_date, INTERVAL 5 day)

>> SQL Execution order: FROM / WHERE / GROUP BY$Agg / HAVING / WINDOW functions like ROW_NUMBER() / SELECT / DISTINCT / ORDER BY / LIMIT or OFFSET
>> view, materialized view and auth view (share results without giving access to source data): efficiently manage and secure your data access in BigQuery
>> Partitioning (single column, 4000 max) and clustering (sorts parti data in storage blocks, four col): Improved Query Performance, Cost Efficiency
>> Joins: - INNER JOIN / left join / right join / full outer join / cross join /
>> Time travel concept: SELECT * FROM `your_dataset.your_table` FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)
>> Window:   SUM(sales) OVER (PARTITION BY id ORDER BY updated_at ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rolling_sum
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /  first_value(cn) / last_value(cn) / nth_value(cn, n) /
   row_number() / rank() / dense_rank() /   lag(cn) / lead(cn) / ntile(n) â€“ n groups /
   CUME_DIST() = NumberÂ ofÂ rowsÂ withÂ valuesÂ lessÂ thanÂ orÂ equalÂ toÂ theÂ currentÂ row / TotalÂ numberÂ ofÂ rows /   PERCENT_RANK() = RankÂ ofÂ currentÂ rowâˆ’1 / TotalÂ rowsâˆ’1
>> WITH CustomOrder AS (SELECT * FROM `p.d.table`) | SELECT * FROM CustomOrder ORDER BYÂ emp_dept_sort, row_num;
>> MERGE INTO target_table AS T  USING source_table AS S  ON T.id = S.id
   WHEN MATCHED THEN UPDATE SET T.name = S.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);
>> Pivot: row into columns 	- product| month| sales  	- select * from table pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar')); --> or use case statement
   SELECT customer_id, SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales, .... FROM orders GROUP BY customer_id;
   unpivot: columns into rows 	- product | Jan | Feb | Mar	- select product, month, sales from table unpivot(sales for month in (Jan, Feb, Mar));
>> Union all / Union distinct  / Except distinct / Intersect distinct / Table wildcards: test_*` where _Table_Suffix > 5
>> Length	Upper	Lower	substr 	Trim	Replace		Concat		Position:STRPOS		Split	Starts_With 	Ends_With	CAST	SAFE_CAST 
   Length	Upper	Lower	substr 	Trim	Replace		Concat		Position:instr		Split	StartsWith 		EndsWith	CAST	CAST 

   coalesce/ifnull
   coalesce


   initcap	concat_ws

==================================================================DataProc======================================================================================

===================> links:

https://github.com/afaqueahmad7117/spark-experiments/tree/main/spark
https://allthingdata.substack.com/p/essential-linux-commands-for-data         

=====================> hdfs commands:

>> linux system / hdfs system
hdfs -help
hdfs dfs -help
hdfs dfs -help ls

===================> low-level api Syntax

>> rdd = spark.sparkContext.parallelize(data)  /  rdd = spark.sparkContext.textFile(hdfs_gcs_path) 
>> rdd1 = rdd.filter(lambda row: 'Frieza' in row) / rdd1 = rdd.filter(lambda row : row!=header)  - header = rdd.first()
   rdd1 = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)   ---> word count programme
   rdd1 = rdd.map(lambda row:row.split(',')).map(lambda row:(row[2],1)).groupByKey().map(lambda row:(row[0],len(row[1]))).collect()
   rdd1 = rdd.map(parse_row) / def parse_row(row):
>> rdd.first() / rdd.collect() / rdd.take(3) / rdd1.countByValue()
   
===================>> high-level api syntax

>> year(col("timestamp_str"))  -  month / day / hour / minute / second 
	date_trunc("month", "date_col") 
	date_diff("end_date", "start_date") / round((unix_timestamp("end_ts") - unix_timestamp("start_ts")) / 3600, 2)
	date_sub("start_date", 10) / date_add("start_date", 10) / expr("ts_col + interval -2 year")

>> custom_schema = StructType([StructField("id", IntegerType(), True), ....]) or ''' ID Integer, Name String '''
   df = spark.read.option("header", True).option("inferSchema", True)/schema(cust_schema).json("file:///path_to_file/data_files/")
   option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record") / option("mode", "DROPMALFORMED") / option("mode", "FAILFAST") 
   df.write.format('parquet').mode('overwrite').save('gs://surdatabuk/data/olist_op/')
   df.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket', 'gcs_path').option('table', 'p.d.t').save()
>> join_df = df1.join(df2, df1.id == df2.id/on='cn', how='inner/left/right/outer/left_semi/left_anti') / df1.crossjoin(df2)  / df1.join(broadcast(df2), "id", "inner")
>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  &|   /
>> df.select("customer_id", col("Name"), df.email, df["city"])    /    df.selectExpr("id*2 as nid", "name as nname").show()
>> sorted_df = df.orderBy(asc('department')) / col("country).desc())  / asc_nulls_last('value') /
>> df.show(n=3, truncate=25, vertical=True) / df.collect() / df.count() / df.printSchema() / df.columns / df.limit(10) / 
>> union tables: union/subtract/intersect/   -->> / col name, position, schema match /
   union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col name / position, schema can differ / missing col handle with null values
>> add new column, drop columns, column name change, column type change 
>> df.dropna(how="all/any", subset=["age"]).show()
   df.fillna({'name': 'suresh', 'age': 40}).show()   /   avg_value = df.select(avg("age")).collect()[0][0]
   df.replace(to_replace=['Alice', 'Bob'], value=['suresh', 'unknown']).show()
>> when&otherwise -->> df = df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult"))
>> drop duplicates: df.distinct() / df.orderBy('Age').dropDuplicates(['Name']) / row_number()  / 
>> agg 	 	   ----> df.select(sum('CN')).show()  		   ------> note: ignore null values ----> sumDistinct, countDistanct
   grouping and agg----> df.groupBy('CN').agg(sum('CN')).show()   	
   window functions----> win_spe=Window.partitionBy("category").orderBy(asc("timestamp")).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) / (-2, 3)
				df = df.withColumn("row_number", row_number().over(window_spec)) 
   	max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) / collect_list('cn') / collect_set('cn') / first_value('cn') / last_value(cn) / nth_value(cn, n)  /
	/ row_number() / rank() / dense_rank() / lag(cn) / lead(cn, 2) / ntile(n) / cume_dist() / percent_rank()
>> splitng column, array_length (size), word in array (array_contains), explode/explode_outer, indexing: [0] or .getItem(0)
>> Pivot: df.groupBy('Month').pivot('product').sum('sales').show()
   unpivot: unpivot_exp = "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (Product, Sales)"  ------> df1.selectExpr("Month", unpivot_exp).show()

==================> spark-sql 

>> spark temp table (exists only for SS) / global temp table (exists all SS within same app)
   spark.sql('show databases') --> spark.sql('use db_name') --> spark.sql('show tables') --> spark.sql('show tables in global_temp')
   df.createOrReplaceTempView('customers') -->	df.createOrReplaceGlobalTempView('gcustomers')
   spark.sql('select * from customers limit 5') or spark.table('customers') --> spark.sql('describe extended customers') --> spark.sql('drop table customers') 		
>> Spark Persistant: Managed table (spark owns both metadata and data)
   spark.sql('''CREATE TABLE custm (customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) USING CSV''')
   df.write.mode('overwrite').saveAsTable('default.customers_sur')
   data: hdfs://sur-dp-clu11-m/user/hive/warehouse/customers 
   metadata: vi etc/hive/conf/hive-site.xml -> connect mysql -u hive -p  -> Pw1+fgC9gJI+ -> show databases; use metastore; show tables; select * from TBLS; exit
>> Spark Persistant: external table (spark owns metadata and gcs/hdfs owns data)
   spark.sql(''' create external table custe (customer_id STRING, ...) using csv location 'gs://surdatabuk/data/olist/olist_customers_dataset.csv' ''')

=================>>  HiveQL: 

>> sql like interface (HQL) --> translator ---> MR/spark/tej code (is abstraction for java or MR programe) >> cli: set hive.execution.engine=tez/spark/mr;
   table --> data (hdfs, s3, gcs) + metadata (hive stores metastore in it). 
>>  Hive external table (data file in GCS)
   create external table custe (customer_id string, ...) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location 'gs://surdatabuk/data/olist/cust_exe/'
>> Hive managed table (data in hdfs --> hdfs dfs -ls /user/hive/warehouse/  --> set hive.metastore.warehouse.dir; )
   create table custm (	order_id string, ....) STORED AS TEXTFILE;
>> connect: 1.hive & 2.beeline : prod rec --> !connect jdbc:hive2://<hostname>:<port>/<database> / !connect jdbc:hive2://localhost:10000/default (hive & Pw1+fgC9gJI+)

-- hive is database like mysql?		its not a database. but, its DWH tool built on top of Hadoop to query large datasets from HDFS storage
-- hive is replacement for Hadoop? 	No, buit on top of hadoop core components (hdfs, yarn, mr/spark/tej)
-- hive queries like normal sql queries in db?  	HQL queries --> translatior --> mr/spark/tej code which runs on distributed system
-- hive can perform row-level transaction like mysql? 	yes, but, hive is not designed for it (if u change one row- it will recreate entire partition file)
-- hive can works with only HDFS?  			no. HDFS/s3/gcs/data lake
-- hive tables work like noraml database tables? 	hive is metastore and not storing any tables


==============>> Spark performance tuning

1. master in reading query plan
	syntax check ---> unresolved logical plan --> catalyst optimizer --> optimal logical plan (filter push down & req col) --> several pysical plan -->
	based on cost model select best pysical plan  --> executes and create RDD/DF
	df.explain(True)

2. master in reading spark dags

3. Tune no of shuffle partitions (optimal SP size b/w 1 to 200 mb )
	* if data per SP is large - cores = 20, SP = 200, data = 300 gb - data/sp = 1.5gb - no of SP = 300*1000/200 = 1500 SP
	* if data per SP is large - cores = 12, SP = 200, data = 50 mb - data/sp = 0.25 mb - no of SP=50/5=10 or data/SP = 50/12 = 4.5 mb

4. Data Skew (job taking time, uneven resource utilization, OOM/data spill)
	AQE(>3.0) - uses runtime statistics - choose most efficient query plan
		-> tuning no of SP - 15 keys - coalesce(15) - spark.sql.adaptive.enabled, spark.sql.adaptive.coalescePartitions.enabled
		-> optimize joins - SMJ to BJ - df1.repartiton(4), df2 broadcasted on all executors - spark.sql.autoBroadcastJoinThreshold
		-> optimize Skew joins - breaks larger part into smaller part - spark.sql.adaptive.skewJoin.enabled
	Salt - adding randomness (key) to distribute uneven data evenly.
		-> add salt columnn - hash(key, salt)% SP

5. Partitioning: create folders for each country within files are equal to no of partitions - Filtering	(Adv: fast access, parallesium/resource utilization)
   df.write.mode('overwrite').partitionBy('country').parquet('/content/ecommer/') >> high cordinality - small file problem - buckting is rec
   Bucketing: Joins and aggregations, filter - hash(product_id)%4  
   df.write.mode('overwrite').bucketBy(2, 'product_id').sortBy("product_id").format("parquet").saveAsTable('bucketed_table')  - 2buc*2part=4files
   Q/A - optimal number of buckets = size of dataset / optimal bucket size (128-200mb)  
   Q/A - once df is bucketed, no shuffle in groupby & join, scan one buckets when you filter instead of all 

6. executor tuning: 
   Rules:leave 1C1gbM per Node for hadoop/yarn/os - leave 1Exe or 1C1gbM for app master container at cluster level - 3 t0 5 cores/exe - overhead mem max(384 or 10% exe mem)
   12c48gb/node - 11c47gb/node-55c235gb-54c234gb-total exe=54/5=10 & mem/exe=234/10=23.4-actual memory/exe=23-2.3=20gb

7.Spark memory management
	submit spark app in yarn cluster --> yarn rm allocates app container and starts driver JVM 
	spark.driver.memory --> JVM memory 
	spark.driver.memoryOverhead --> 10% 0r 384mb max one --> used for container processes, pyspark app
	>> Spark executor container: sum of below 3 are the total memory of executor container
	1. on-heap memory managed by JVM (8 gb) [spark.executor.memory ] - when on-heap memory is full, operations paused and do gcc then resume operation - perfor reduce
		reserved memory 300 mb --> fixed reserve for spark engine
		spark memory (unified memory) [spark.memory.fraction] (0.6 default, u can change)--> used DF operation & caching
			storage memory pool [spark.memory.storageFraction], (0.5 default, u can change)
			executor memory pool --> 
		user memory --> used for spark internal metada, udf, rdd operations, variables, objects
	2. overhead [spark.executor.memoryOverhead  --> 10% of spark executor memory 0r 384mb max one], for container processes, n/w transfer, read shuffle, python worker 
	   spark.executor.pyspark.memory --> pyspark memory --> default zero (pyspark is non jvm. so it will take from overhead memory)
	3. off-heap mwmory: managed by OS [spark.memory.offHeap.enabled, spark.memory.offHeap.size - default 0, 10to20% of on-heap memory].
	   Off-heap memory can reduce garbage collection overhead, leading to better performance and lower latency

	yarn.scheduler.maximum-allocation-mb --> pysical memory limit at the worker node
	yarn.nodemanager.resource.memory-mb 

8. static partition pruning: partition on listen date data on disk - this scans only listen date that user eants
   dynamic partition pruning: send partition that is supossed to be scanned
	release date table --> filter to get requ data which comes to know during run time --> spark uses results to scan on other dataset based on listen date
	Adv: reduce time to scan/process)
	limits: one of dataset must be partitioned and based on column that is filered from other datase	
	SS: .config("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true") \

9. cache/persist, Use DataFrame/Dataset over RDD, Avoid User Defined Functions (UDFs)


==============>> spark config

spark = SparkSession.builder.appName('surapp11').config('spark.sql.files.maxPartitionBytes', '3mb').getOrCreate()
spark.sparkContext.applicationId, spark.stop(), spark application -list, spark application -kill app_id
df.explain(True)
spark_new_app_session = spark.newSession()
df.rdd.getNumPartitions()
spark.conf.get('spark.sql.files.maxPartitionBytes') / rdd.getNumPartitions() / spark.sparkContext.defaultMinPartitions 

gcloud: --properties=spark:spark.executor.cores=4,spark:spark.executor.memory=4g,spark:spark.executor.instances=2
.config('spark.driver.memory', '4g')  \ .config('spark.driver.maxResultSize', '2g') \
.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores
.config('spark.memory.fraction', 0.8) \ .config('spark.memory.storageFraction', 0.2) \
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10 * 1024 * 1024)  # 10 MB

>> dataproc cluster: stagging bucket --> cluster metainfo, notebooks	temp bucket --> MR/spark job history, YARN logs
>> validate pyspark exit() / spark-sql exit; / 
>> execute gcs script: master node: spark-submit gs://buk/pysp1.py   >> submit pyspark job using UI >> gcloud dataproc jobs submit pyspark pysp1.py --cluster --region


===============> Interview Quetions

>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
   what is transformation and action?      
   How does Lazy Evaluation work in PySpark?      
   What are wide and narrow transformations in PySpark?
   reduceByKey vs groupByKey?		
   increase and decrease no of partions?		
   spark read data and partitioning?
   spark jobs (no of actions), stages (no of wide trans + 1), tasks (no of partitions used) 
>> key-benfits compared to low-level api  
   reading data in spark is action or transformation?
   read modes: option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record") // option("mode", DROPMALFORMED/FAILFAST")
>> df.cache()  # Stores in memory, spills to disk if needed  -->> df.persist(StorageLevel.MEMORY_AND_DISK_SER_3)   (6)
>> shuffle operations - redistributing data across different nodes in a cluster, which is necessary for certain transformations like groupBy, join, and distinct.
>> broadcast join - small dataset is broadcasted to all worker nodes - allows to perform join locally - reduced data shuffling across the network - improved performance
>> map() returns a single value for each input element / flatMap() return multiple values for each input element
>> .ðœð¨ð¥ð¥ðžðœð­() work?  --> gathers all partitions of RDD and brings them to the driver node. - data is no longer distributed, instead stored in the memory of the driver program
>> Difference between sortby and orderby? - sortBy is used with RDDs and orderBy is used with DataFrames.
>> what is the default block size (128 MB) in hdfs and how to change it?:   Edit the hdfs-site.xml File
>> SparkContext: entry point for RDD operations/functionality. SparkSession: Provides a unified entry point for DataFrame and Dataset APIs, SQL queries, and more. 
>> What is SparkSQL, and how do you perform SQL operations on DataFrames? 

==============> coding QA



>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> What are the different ways to remove duplicate records ---> distinct() / dropDuplicates(["name", 'id']) / row_number() / groupBy /
>> find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 
>> split the data into two columns (Even, Odd) 
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> Retrieve employees who joined in the last 6 months.
>> Identify employees whose salary is greater than the average salary of their department.
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> Get the first and last transaction date for each customer.
>> Replace null values in a column with the last non-null value in that partition. -----> last_value('cn', ignorenulls=True).over(window_spec)
>> Categorize employees based on years of experience into Junior, Mid, and Senior levels.
>> Fill the mean salary value in Null. ---> avg_sal = df.select(mean(col('sal'))).collect()[0][0] --> df.fillna({'salary': avg_salary}).show() / avg_sal and cross join
>> ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]
	Replace all NULL values in the Quantity column with 0 / Price column with the average price of the existing data / Fill missing Sales_Date with '2025-01-01'. / 
    Drop rows where the Product column is NULL / Drop rows where all columns are NULL.
>> find max id excluding duplicates | 2, 6, 5, 6, 9, 9, 8: with unique_ids AS (SELECT id FROM id_list GROUP BY id HAVING COUNT(*) = 1) | SELECT MAX(id) FROM unique_ids
>> table1: 1 2 3 null ""  & table2: 2 2  3 null null "" : inner join - 2 2, 2 2, 3 3, empty empty |  Null doesnt include matching, empty string includes matching
>> column Country: India Australia Pakistan | Output: India vs Australia India vs Pakistan Australia vs Pakistan
	df.alias("a").crossJoin(df.alias("b")).filter(col("a.Country") < col("b.Country")).selectExpr("concat(a.Country, ' vs ', b.Country) as Matchup")
    SELECT a.country || ' vs ' || b.country as country FROM country a  join country b on a.country > b.country;  
>> number of patients per doctor including unassigned patients: docter_id | pationt_id, doctor_id
>> Handling NULL values using the average salary | emp_id, salary: IFNULL(cn, rep_val) COALESCE(cn, rep_val1, rep_val2, ...) 
>> ORDER_DAY, ORDER_ID, PRODUCT_ID, QUANTITY, PRICE | Get me all products that got sold both the days and the number of times the product is sold.  
>> employees who hired in last 90 days: 
>> find customers who made transactions in every month of the year | cust_id, date, purchage_amount
>> employees sal > managers | EMPID, EMPNAME, MANAGERID, SLARY: 
>> Extract the Domain from the Email column:	substring(email, strpos(email, '@')+1, length-opt) 
>> Find all employees whose names contain the letters "a" exactly twice: SELECT * FROM employees WHERE LENGTH(name) - LENGTH(REPLACE(LOWER(name),'a','')) = 2
>> Write a query to count how many employees share the same salary.
>> emp earning more than avg sal in their department: 
>> pivot & unpivot data: cricket_data = [("Virat Kohli", 'Match1', 75), ..........], col = ["Player", "Match", "score"]
>> Word count program in pyspark rdd & DF
>> count the number of null values in each column of df. 	df.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()
>> display only last 4 characters of card number. data = [(1,'Rahul',1234567891234567),(2,'Raj',1234567892345678)] schema = "id int, name string, card_no long"
	maskfun_udf = udf(maskfun, StringType()) 	 /	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))
>> extract filename: df = df.withColumn("state", element_at(split(input_file_name(), "/"), -1)).withColumn("state", split("state", "\\.").getItem(0))








