===========
>> SQL: FROM: JOIN: WHERE: GROUP BY: Aggregate: HAVING: SELECT: ORDER BY: LIMIT: 
    SELECT d.department, COUNT(e.id) AS employee_count 
    FROM `project.dataset.employees` e JOIN `project.dataset.departments` d ON e.department_id = d.id
    WHERE e.age > 30 GROUP BY d.department HAVING COUNT(e.id) > 1 ORDER BY d.department LIMIT 10;
>> Pandas: Read Data: Merge: Filter Rows: Select Columns: Group By: Aggregate: Filter Groups: Sort: Limit:
    join_df = pd.merge(employees, departments, left_on='department_id', right_on='id', how='left')
    filtered_df = df[df['age'] > 30]
    selected_df = filtered_df[['name', 'department']]
    grouped_df = selected_df.groupby('department').size().reset_index(name='count')
    having_df = grouped_df[grouped_df['count'] > 1]
    sorted_df = having_df.sort_values(by='department')
    result = sorted_df.head(10)
    print(result)
>> PySpark: read : Join : filter : groupBy: agg : filter : select : orderBy : Limit: 
    df_employees = spark.createDataFrame(employees) 
    outer_join_df = df_employees.join(df_departments, df_employees.department_id == df_departments.id, how='outer')
    filtered_df = joined_df.filter(joined_df['age'] > 30)  ---> joined_df['age'] or joined_df.age or col('age')
    grouped_df = filtered_df.groupBy('department').count()
    having_df = grouped_df.filter(grouped_df['count'] > 1)
    selected_df = having_df.select('department', 'count')
    sorted_df = selected_df.orderBy('department')
    result = sorted_df.limit(10)
    result.show()

=====================
 >>   SELECT column1 FROM table1
    UNION DISTINCT/UNION ALL/EXCEPT DISTINCT/INTERSECT DISTINCT
    SELECT column1 FROM table2;
>> union_distinct = pd.concat([df1, df2]).drop_duplicates()  / union_all = pd.concat([df1, df2])
>> union_distinct = df1.union(df2).distinct()  /union/subtract/intersect/

