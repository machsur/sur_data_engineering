
======================================================================== BigQuery ==================================================================

>> Bigquery arch: storage and compute - decoupled/isolated (cost efficient)
   colossus - distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus;    borg - scheduling jobs like run queries
>> Fact Table: Contains numeric values (quantitative data - sales, revenue, profit, etc), foreign keys, Grows rapidly
   Dimension Table: contains textual or categorical data, smaller and changes less frequently.
>> Star Schema: Multiple Dimension Tables directly connected to the fact table
   Snowflake Schema: Dimension tables are split into sub-dimensions, reducing redundancy, more normalized form. 
>> Change Data Capture -->> identify & capture changes made to data in DB --> log based - very efficient, query based on timestamp, triggers load changes into new tables
>> (SCD) is a dimension where the data changes slowly, not every day.  ex: Customer changes address
   SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
   SCD1 - Overwrite - Updates the existing record with new data - No history is preserved.
   SCD2 - Add New Row  -- Keeps full history by adding a new row with versioning or effective dates.
   SCD3 - Add New Column -- Keeps limited history by adding a new column for the previous value | SCD4 - scd1+scd2 - Current Table & History Table
>> Temporary table - intermediate data proce and caching | Internal table- freq accessed and processed data | External table - for occasional queries on large datasets.
>> Normalization – avoid redundant data, poor query performance |  De Normalization – redundant data (high storage cost) – better Query performance 
    Why array & structs? – for better query per and lesser storage costs
    Arrays – list of items having same data type  -  SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element; 
    Struct – record with nested fields having different data types  -   SELECT person.id, person.name FROM mydataset.mytable;
>> bq slots: units of computational capacity -CPU and memory, Reservations: allow you to purchase dedicated slots (called commitments) 
>> Big Query cost optimization
   bq compute optimization: on-demaned pricing, Capacity Pricing (slots reserve)
   BQ data storage: Billing model: logical/physical, Use Table Expiration, Use Long-Term Storage Pricing (> 90 days)
   query Opti: Agg at Source,  use avro, dataset at cust loc, Parti & Clust, preview, select col, use trun, Join, where, late agg, Caching, mat view for freq queries
>> Cost Controls: budget alerts, Set query usage limits, export Billing reports, Schedule queries during off-peak hours

=========================
>> Syntax
DATE(DC in iso) / PARSE_DATE(format, stringDC) / SAFE.PARSE_DATE(format, DC)	-	Y 2026, y 26/ m 01-12, B January, b Jan / d 01-31, A Monday, a Mon / 
FORMAT_DATE('%B %d, %Y', sale_date)
extract(year from date)      						-     	year / month / day / hour / minute / second / microsecond /
DATE_TRUNC(order_date, MONTH) 
DATE_DIFF(order_date, previous_date, day) 
Date_add(order_date, INTERVAL 5 day) / Date_sub(order_date, INTERVAL 5 day)
>> SQL Execution order: FROM / WHERE / GROUP BY$Agg / HAVING / WINDOW functions like ROW_NUMBER() / SELECT / DISTINCT / ORDER BY / LIMIT or OFFSET
>> view, materialized view and auth view (share results without giving access to source data): efficiently manage and secure your data access in BigQuery
>> Partitioning (single column, 4000 max) and clustering (sorts parti data in storage blocks, four col): Improved Query Performance, Cost Efficiency
>> Joins: - INNER JOIN / left join / right join / full outer join / cross join /
>> Time travel concept: SELECT * FROM `your_dataset.your_table` FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)
>> Window:   SUM(sales) OVER (PARTITION BY id ORDER BY updated_at ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rolling_sum
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /  first_value(cn) / last_value(cn) / nth_value(cn, n) /
   row_number() / rank() / dense_rank() /   lag(cn) / lead(cn) / ntile(n) – n groups /
   CUME_DIST() = Number of rows with values less than or equal to the current row / Total number of rows /   PERCENT_RANK() = Rank of current row−1 / Total rows−1
>> WITH CustomOrder AS (SELECT * FROM `p.d.table`) | SELECT * FROM CustomOrder ORDER BY emp_dept_sort, row_num;
>> MERGE INTO target_table AS T  USING source_table AS S  ON T.id = S.id
   WHEN MATCHED THEN UPDATE SET T.name = S.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);
>> Pivot: row into columns 	- product| month| sales  	- select * from table pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar')); --> or use case statement
   SELECT customer_id, SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales, .... FROM orders GROUP BY customer_id;
   unpivot: columns into rows 	- product | Jan | Feb | Mar	- select product, month, sales from table unpivot(sales for month in (Jan, Feb, Mar));
>> Union all / Union distinct  / Except distinct / Intersect distinct / Table wildcards: test_*` where _Table_Suffix > 5
>> CASE WHEN years_of_experience < 3 THEN 'Junior' WHEN years_of_experience BETWEEN 3 AND 7 THEN 'Mid' WHEN years_of_experience > 7 THEN 'Senior'  ELSE 'Unknown' END AS El
>> SPLIT(full_name, ' ')[OFFSET(0)]
>> +	 -	 * 	/ 	MOD(10, 3)		ROUND(value, digits) 	CEIL(value) 	FLOOR(value) 	ABS(value) 	POWER(x, y) 
>> coalesce(NULL, NULL, 'siva')		ifnull(cn, 'siva')
>> mask card number: CONCAT(REPEAT('*', 12), RIGHT(card_number, 4))
>> extract filename: SELECT _FILE_NAME AS file_name, * FROM `project.dataset.external_table`;
>> col null count: COUNTIF(column1 IS NULL) AS column1_null_count
>> Length /	Upper / Lower / Replace / substr / strpos / Trim / Starts_With / Ends_With	/SAFE_CAST / CAST    	   
	round / abs	/ ceil / floor / safe_devide / 	if / case / generate_uuid()

===========================

>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> What are the different ways to remove duplicate records ---> distinct() / row_number() / groupBy /
>> find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 
>> split the data into two columns (Even, Odd) 
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> Retrieve employees who joined in the last 6 months.
>> Identify employees whose salary is greater than the average salary of their department.
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> Get the first and last transaction date for each customer.
>> Replace null values in a column with the last non-null value in that partition. -----> last_value('cn', ignorenulls=True).over(window_spec)
>> Categorize employees based on years of experience into Junior, Mid, and Senior levels.
>> Replace NULL values in Quantity column with 0 / Price with avg price/Sales_Date with '2025-01-01'/ Drop rows where Product col is NULL & all columns are NULL.
>> find max id excluding duplicates | 2, 6, 5, 6, 9, 9, 8: 
>> table1: 1 2 3 null ""  & table2: 2 2  3 null null "" : inner join - 2 2, 2 2, 3 3, empty empty |  Null doesnt include matching, empty string includes matching
>> employees who hired in last 90 days: 
>> number of patients per doctor including unassigned patients: docter_id | pationt_id, doctor_id
>> Handling NULL values using the average salary | emp_id, salary: IFNULL(cn, rep_val) COALESCE(cn, rep_val1, rep_val2, ...) 
>> emp earning more than avg sal in their department: 
>> column Country: India Australia Pakistan | Output: India vs Australia India vs Pakistan Australia vs Pakistan
>> ORDER_DAY, ORDER_ID, PRODUCT_ID, QUANTITY, PRICE | Get me all products that got sold both the days and the number of times the product is sold.  
>> find customers who made transactions in every month of the year | cust_id, date, purchage_amount
>> employees sal > managers | EMPID, EMPNAME, MANAGERID, SLARY: 
>> Extract the Domain from the Email column:	substring(email, strpos(email, '@')+1, length-opt) 
>> Find all employees whose names contain the letters "a" exactly twice: SELECT * FROM employees WHERE LENGTH(name) - LENGTH(REPLACE(LOWER(name),'a','')) = 2
>> Write a query to count how many employees share the same salary.
>> count the number of null values in each column: 
>> display only last 4 characters of card number. 

>> Three options to load more data if the number partition is 4000/full in BigQuery #googlecloud: bq rm | delete | turn on partition expiration
>> Interview Question What happens if u load 11 years of data into BigQuery with Partitioning enabled? 
   cant load data because > 4000 partitions
>> What happens when a virus infected object is uploaded to Cloud Storage? ntg happens its object strorage not for scanning
>> SCD-1 & SCD-2 --> scd2.sql script
>> derive the overall journey (start → end) for that customer. cust_id, from_city, to_city, journy_order
 	use first_value & last_value
>>  match_id, 	team1, 		team2, 		winner, 	result, 	match_date
	1,  		'IND', 		'AUS', 		'IND',  	'WIN',  	'2024-01-01'
    output:
	team 	played	 wins  draws 	losses		points
>> why avro format recommends for bigquery load?
	schema support in the file itself, nested & repeated fields, more compression, splittable for parrallel processing - leads to faster load
>> EXECUTE IMMEDIATE FORMAT('your SQL %s', value);   --> to run dynamic sql queries


>> Common error found during proc execution:
failed to parse input string "2026-01-15 :" / "202502023" : use safe_cast instead of cast
cant replace table with different partitioning spec, instead drop table, then recreate it.
divizon by zero error: if(den = 0, null, num/den)  /  nullif(den, 0) / 
bad int64 value: 1.0 --> cast(col as int64) or asfe_cast(col as int64) 
no dataset was found


