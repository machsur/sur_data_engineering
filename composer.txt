================>> composer
>> Cloud Composer: Apache airflow version in GCP - for workflow orchestration and defined by DAG
>> Create composer environment: runs on GKE cluster:   region, image version, SA, env resources (scheduler, web server, worker), N/W
>> Architexture of composer: Dags dir in gcs - each dag associated with schedulr & scheduler will trigger the dags based on scheduled time 
                           - dags runs on worker nodes - dags & dag runs info stored in metadata db - Web server (for airflow UI)

>> create dag file: multiple DAGs are defined in py script, same dag can be in multiple py scipts.
   1. import modules & operators    2. contruct dag object    3. define tasks    4. set task depencies
   UI > admin > configurations 
   
>> variables in airflow dag: 
   set variable: UI: Airflow UI → Admin → Variables → + (stores in metadata DB). 
   access: var1 = Variable.get("project_id", default_var="sur_project")  # default
   Xcom: review runtime variables for dag run using xcom

>> * update/review packages in composer environment
   composer2: gcs based approach
   composer3: gcloud composer environments update <ENV_NAME> --location <REGION> --update-pypi-packages="pandas==2.1.4,pyarrow==15.0.0"
              gcloud composer environments update <ENV_NAME> --location <REGION> --remove-pypi-packages="pandas"
   * nexus credential in env vars: 
      gcloud composer environments update <ENV_NAME> --location <REGION> --update-env-variables=NEXUS_USER=myuser,NEXUS_PASS=mypass,NEXUS_HOST=nexus.hsbc.net
      gcloud composer environments update <ENV_NAME> --location <REGION> --update-pypi-repositories="{\"pypi_index_url\":                                                                  \"https://${NEXUS_USER}:${NEXUS_PASS}@${NEXUS_HOST}/repository/pypi/simple/\",    \"pypi_trusted_hosts\": [\"${NEXUS_HOST}\"]}"
  * If credentials or repo URLs are needed BEFORE Airflow starts (e.g., to install packages) → Use ENV VARS
    If values are needed DURING DAG execution → Use Airflow Variables


5.
>> Advantage of cloud composer:
   | DB -- via python --> datalake (gcs) | --> File coneverter csv to parquet --> tarnsform --> load into BQ |
   | py app                             |       dataproc workflow                                            |
   |              cloud composer                                                                              |

>> What max_active_runs=1 -> applies to ONE DAG only ->same DAG cannot have more than one active run at the same time
  Two different DAGs can: Run at the same time, Run in parallel, Use different or even same resources (unless you restrict them) 
  what controls parallel runs 1️⃣ Airflow environment limits (Worker count, CPU / memory 2️⃣ Pools (IMPORTANT) Used when DAGs share critical resources.

pip install apache-airflow==2.3.3
pip install apache-airflow-providers-google
gcloud composer environments run --location=us-central1 --project=aesthetic-way-485114-u2 sur-com-env3 dags list
gsutil cp -r C:\Users\macha\Downloads\pyws\deog\data gs://sur_test_buk/

