================>> composer
>> Cloud Composer: Apache airflow version in GCP - for workflow orchestration and defined by DAG
>> Create composer environment: runs on GKE cluster:   region, image version, SA, env resources (scheduler, web server, worker), N/W
>> Architexture of composer: Dags dir in gcs - each dag associated with schedulr & scheduler will trigger the dags based on scheduled time 
                           - dags runs on worker nodes - dags & dag runs info stored in metadata db - Web server (for airflow UI)

>> create dag file: multiple DAGs are defined in py script, same dag can be in multiple py scipts.
   1. import modules & operators    2. contruct dag object    3. define tasks    4. set task depencies
   
>>
Git Repo
   â”œâ”€â”€ dags/
   â”œâ”€â”€ requirements.txt
   â””â”€â”€ ci-cd-pipeline.yaml

>> commands
pip install apache-airflow==2.3.3
pip install apache-airflow-providers-google
gcloud composer environments run --location=us-central1 --project=aesthetic-way-485114-u2 sur-com-env11 dags list
gsutil cp -r C:\Users\macha\Downloads\pyws\deog\data gs://sur_test_buk/

>> Production Recommendation for package managment
  Install package
    # update env variable, install packages
    gcloud composer environments list
    gcloud composer environments update sur-com-env11 --location us-central1 --update-env-variables=NEXUS_USER=myuser,NEXUS_PASS=mypass,NEXUS_HOST=nexus.hsbc.net
    or create artifact repo in gcp, sync nexus repo to artifact repo, grant access to composer sa on artifact repo, 
    gcloud composer environments update sur-com-env11 --location us-central1 --update-pypi-packages-from-file requirements.txt
    gcloud composer environments update sur-com-env11 --location us-central1 --update-pypi-packages="pandas==2.1.4,pyarrow==15.0.0"
    gcloud composer environments describe sur-com-env11 --location us-central1 
    gcloud composer environments restart sur-com-env11 --location us-central1 
  Create verification DAG (pip_list.py)
  Validate version: deploy dag and run the dag view all packages in the the logs 
  Delete verification DAG
  Note:
  If credentials or repo URLs are needed BEFORE Airflow starts (e.g., to install packages) â†’ Use ENV VARS
  If values are needed DURING DAG execution â†’ Use Airflow Variables




>> What max_active_runs=1 -> applies to ONE DAG only ->same DAG cannot have more than one active run at the same time
  Two different DAGs can: Run at the same time, Run in parallel, Use different or even same resources (unless you restrict them) 
  what controls parallel runs 1ï¸âƒ£ Airflow environment limits (Worker count, CPU / memory 2ï¸âƒ£ Pools (IMPORTANT) Used when DAGs share critical resources.




>>
TaskGroup and SubDAG are both used to organize complex DAGs, but they solve very different problems.
SubDAG creates a separate DAG inside a parent DAG. It has its own scheduler, start date, and execution context. often caused scheduler delays, deadlocks, and performance issues.
TaskGroup - logically group related tasks within the same DAG, helps to create parallel processing of etl.
ðŸ‘‰ Today, TaskGroup is the recommended approach, and SubDAG is considered anti-pattern / deprecated in practice.

>>
Variables - store configuration values outside your DAG code so you donâ€™t hard-code environment-specific or sensitive values.
Same DAG â†’ works in dev / qa / prod ðŸŽ¯
set variable: UI: Airflow UI â†’ Admin â†’ Variables â†’ + (stores in metadata DB). 
access: var1 = Variable.get("project_id", default_var="sur_project")  # default

>>
   Xcom: review runtime variables for dag run using xcom
