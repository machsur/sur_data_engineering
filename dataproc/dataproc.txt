
==================================================================DataProc======================================================================================
hh
>> 
execution of spark application? (see in dataproc11 doc):
once saprk app reuest submitted - it goes to Yarn - yarn creates application master (AM) container on any one of worker node - AM starts app driver nothing but JVM process and its job is to execute main() of spark app - driver programe request resources from yarn - yarn create executors and handover details back driver - before driver schedules tasks on executors, driver creates query plan and gives final pysical plan and creates dag by dag scheduler using pysical plan - driver schedule tasks - executors execute task and return results back to driver
NOTE: Spark core written in SCALA --> on top Java wrapper --> python wrapper
	  if UDF/libraries written that are not natively available in spark, each executor may spinup python process too
	
components and architexture of spark? (see in dataproc11 doc)
	Driver Programme: runs on MN of cluster, responsible for converts user code into tasks, distributing tasks to executors, collect results from executors
	cluster manager: allocate resources 
	Executors: runs on worker node, responsible for executing tasks assigned by driver, cashing data
	task: smallest unit of work and executed by executors
	Job: high level action triggers job, stage: job divided into stages based on shuffle boundaries, task: each stage divided into tasks, one for each partition

Query paln:

DAG: logical representation of sequence of computtations

Spark executor memory management
	on-heap memory managed by JVM (10 gb) [spark.executor.memory ] - when on-heap memory is full, operations paused and do gcc then resume operation - perfor reduce
		reserved memory 300 mb --> fixed reserve for spark engine
		unified memory [spark.memory.fraction] (0.6 default, u can change)
			executor memory pool --> joins, shuffle, groupBy, 
			storage memory pool [spark.memory.storageFraction], (0.5 default, u can change) --> RDD, DF cashing, broadcast var
		user memory --> used for spark internal metada, udf, variables, objects
	off-heap mwmory: managed by OS [spark.memory.offHeap.enabled/size - default 0, 10to20% of on-heap memory]
		useful when spark handle large datasets - frequent garbage collection happening - stops programme and does clean to resume progamme - costly operation
		Off-heap memory can reduce garbage collection overhead, leading to better performance and lower latency
	overhead memory [spark.executor.memoryOverhead--> 10% of spark executor memory 0r 384mb max one] --> for container processes, n/w transfer, read shuffle, python worker 
 
Spark deployment modes: Local, Client (app driver runs on the client machine) and cluster mode (app driver runs any one of node in the cluster)

executor tuning: rule of thumb - leave 1 core & some memory for system processes, 3-5 cores/executor, leave 1Exe for app master container at cluster level
	cluster capacity : 10 nodes (16 cores 64gb ram) 
	reserved : 1 c 8gb per node for system processes - ramaining: 15c 56gb ram
    executors per node = 15c/5 = 3  total executors = 10 nodes * 3 = 30
	memory per executor = 56gb / 3 = 18gb

>> Spark optimization techniques.
==> Use columnar file formats (parquet or orc) for better compression and faster reads than csv 

==> partitioning: distibute data into folders based on column values - best for low to medium cardinality columns & select only required partitions (filter or where) 
	df.write.mode('overwrite').partitionBy("listen_hour", "listen_date").parquet('/con/') 
	static partition pruning: 
   	dynamic partition pruning: get requ partions to scan during run time - spark uses results to scan only required partitions from other dataset.
	config("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")

==>	bucketing: distribute data into fixed no of files based on hash of column - best for high cordinality columns and frequent joins - avoid shuffelling during the join. 
	optimal buckets = size of dataset / 128-200mb
	config('spark.sql.warehouse.dir', 'gs://gcs_bucket/warehouse') 
   	df.write.mode('overwrite').partitionBy("listen_date").bucketBy(4, 'product_id').sortBy("product_id").saveAsTable('buckted_table')  
   	df.write.mode('overwrite').bucketBy(4, 'product_id').sortBy("product_id").saveAsTable('buckted_table')  
	df = spark.sql(query) # select * from t1 join t2 on t1.product_id = t2.product_id where date = date_col

==> Enable Dynamic Resource allocation for varying workloads to use resource efficiently (rec to set at cluster level)
	gcloud dataproc clusters create my-cluster --region=us-central1 --properties=spark:spark.dynamicAllocation.enabled=true, spark:spark.shuffle.service.enabled=true,
	spark:spark.dynamicAllocation.minExecutors=2, spark:spark.dynamicAllocation.maxExecutors=50, spark:spark.dynamicAllocation.initialExecutors=5
==> Column pruning (select req columns) + filter pushdown (Reads only required row groups)
	config('spark.sql.parquet.filterPushDown', 'true')
	df = spark.read.parquet(input_path).select("event_id", "user_id", "device_type", "event_type", "event_ts").filter(col("event_type") == "CLICK")
==> Used caching for repeted data access:  df.cache() / df.persist(StorageLevel.MEMORY_AND_DISK_DESER_2) / df.unpersist() /
	SER (ser format & lesser in size) is CPU intensive, memory saving as data is compact while DESER (memory as jvm objects) is CPU efficient, memory intensive

==> tune shuffle partitions: any wide transformation (groupby, join) involves shuffeling
	under partitioning: SP is low - large amount of data - skewed workloads and OOM errors
	over partitioning: SP is high -  excessive task overhead and slow completion time
	Manual Tuning: if data is small to medium - well understood
	based on data size (SP=100GB/128MB = 800) based on cluster cores (min SP = 160 cores *2-3 = 320) ==> set for 800
	AQE: Enable AQE to dynamically adjust no of shuffle partitions - if data size is large and behaviour is unpredictable
	set initial shuffle partitions: spark.sql.shuffle.partitions		
	SP is small- coalsece with 64mb - (spark.sql.adaptive.coalescePartitions.enabled", "true") - spark.sql.adaptive.shuffle.targetShufffleFileInputSize, "64MB"
	SP is large - AQE splits into smaller ones - ("spark.sql.files.maxPartitionBytes", "128mb")
	Note: else, repartition the df based key column and then perform wide transformation

==> optimize joins
	broad cast join if one table is small --> reratition larger table based on join key.
	partition pruning: filter data before join to reduce shuffle overhead
	skewed joins: if one of key in join is highly skewed then implement slating
	when both are large tables --> repartition and sort based on the join keys to reduce shuffle overhead
		large_df = large_df.repartition(10, 'id').sortWithinPartitions('id')
	SSMJ: shuffle - sort - slower performance
	SMJ: requires explicit repartition and sorting - after no shuffle and sort when join - faster performanance

==>> Data Skew: uneven data distribution during shuffle operations like groupBy, join (job taking time, uneven resource utilization, OOM/data spill)
	slating in Join:
	SALT_NUMBER = int(spark.conf.get("spark.sql.shuffle.partitions"))
	df_skew = df_skew.withColumn("salt", floor(rand() * SALT_NUMBER))
	df_uniform = df_uniform.withColumn("salt", explode(col("salt_array")))  # salt_array = array([lit(i) for i in range(SALT_NUMBER)])
	df_joined = df_skew.join(df_uniform, ["value", "salt"], 'inner').drop('salt')
	salting in groupBy:
    df_skew.withColumn("salt", floor(rand() * SALT_NUMBER)).groupBy("value", "salt").agg(count("value").alias("count")).groupBy("value").agg(sum("count").alias("count"))
	AQE(>3.0) - uses runtime statistics - choose most efficient query plan
		-> optimize joins - SMJ to BJ - df1.repartiton(4), df2 broadcasted on all executors - spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10*1024*1024/-1) 
		-> optimize Skew joins - spark.conf.set("spark.sql.adaptive.skewedJoin.enabled", "true")
		   with aqe enable, spark detects skewed key - splits into smaller chunks - allows to process skewed key in parallel

==>> Use DataFrame/Dataset over RDD, Avoid User Defined Functions (UDFs)

===================> low-level api Syntax

>> rdd = spark.sparkContext.parallelize(data)  /  rdd = spark.sparkContext.textFile(hdfs_gcs_path) 
>> rdd1 = rdd.filter(lambda row: 'Frieza' in row) / rdd1 = rdd.filter(lambda row : row!=header)  - header = rdd.first()
   rdd1 = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)   ---> word count programme
   rdd1 = rdd.map(lambda row:row.split(',')).map(lambda row:(row[2],1)).groupByKey().map(lambda row:(row[0],len(row[1]))).collect()
   rdd1 = rdd.map(parse_row) / def parse_row(row):
>> rdd.first() / rdd.collect() / rdd.take(3) / rdd1.countByValue()
   
===================>> high-level api syntax

>>
df.rdd.getNumPartitions()
spark.sparkContext.defaultMinPartitions
spark.sparkContext.defaultParallelism
spark.sparkContext.applicationId, spark.stop(), spark application -list, spark application -kill app_id
spark_new_app_session = spark.newSession()

.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores

>> dataproc cluster: stagging bucket --> cluster metainfo, notebooks	temp bucket --> MR/spark job history, YARN logs
>> validate pyspark exit() / spark-sql exit; / 
>> execute gcs script: master node: spark-submit gs://buk/pysp1.py   >> submit pyspark job using UI >> gcloud dataproc jobs submit pyspark pysp1.py --cluster --region

>>
spark = SparkSession.builder.appName('surapp11')
.master("yarn")
.config("spark.submit.deployMode", "cluster")
.config("spark.driver.memory", "4g")
.config("spark.executor.memory", "8g")
.config("spark.executor.cores", "4")
.config("spark.executor.instances", "10")
.config('spark.sql.files.maxPartitionBytes', '3mb').
getOrCreate() 

>> custom_schema = StructType([StructField("id", IntegerType(), True), ....]) or ''' ID Integer, Name String '''
   df = spark.read.option("header", True).option("inferSchema", True)/schema(cust_schema).json("file:///path_to_file/data_files/")
   df = spark.read.format("csv").option("header", True).option("inferSchema", True)/schema(cust_schema).load("file:///path_to_file/data_files/")
   option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record") / option("mode", "DROPMALFORMED") / option("mode", "FAILFAST") 
   df.write.format('parquet').mode('overwrite').save('gs://surdatabuk/data/olist_op/')
   df.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket', 'gcs_path').option('table', 'p.d.t').save()
>> join_df = df1.join(df2, df1.id == df2.id / () & () / on='id'/ on=["id", "country"] / &, how='inner/left/right/outer/left_semi/left_anti') 
    df1.join(broadcast(df2), "id", "inner") /  df1.crossjoin(df2)  
>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  (& AND | OR) +  ~ NOT /
>> df.select("customer_id", col("Name"), df.email, df["city"])    /    df.selectExpr("id*2 as nid", "name as nname").show()   /  df.select('*').show() / 
	df.select([c for c in df.columns if c not in ls]).show()	/ ls = ['product','region']
>> sorted_df = df.orderBy(asc('department')) / col("country).desc())  / asc_nulls_last('value') /
>> df.show(n=3, truncate=25, vertical=True) / df.collect() / df.count() / df.printSchema() / df.columns / df.limit(10) / 

>> union tables: union/subtract/intersect/   -->> / col name, position, schema match /
   union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> handle col inorder / missing col handle with null values /
>> 	add new columns: df.withColumn("col1", lit(100)).withColumn("col2", col("id") * 10) 
	column type change: df.withColumn("amount", col("amount").cast("double"))   # Change 'amount' from string to double
	columns name change: df.withColumnRenamed("old_name", "new_name")
	drop columns: df.drop("col1", "col2", "col3")
>> df.dropna(how="all/any", subset=["age"]).show()
   df.fillna({'name': 'suresh', 'age': 40}).show()   /   avg_value = df.select(avg("age")).collect()[0][0]
   df.replace(to_replace=['Alice', 'Bob'], value=['suresh', 'unknown']).show()
>> when&otherwise -->> df = df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult"))
>> drop duplicates: df.distinct() / df.orderBy('Age').dropDuplicates(['Name']) / row_number()  / 
>> agg 	----> df.select(sum('CN')).show()  /   ---> note: ignore null values
   sum / min / max / avg / count / sum_distinct / count_distinct / first_value / last_value / collect_list / collect_set
   grouping and agg----> df.groupBy('CN').agg(sum('CN')).show()   	---> note: ignore null values 	
   sum / min / max / avg / count / sum_distinct / count_distinct / first_value / last_value / collect_list / collect_set
   window functions----> win_spe=Window.partitionBy("category").orderBy(asc("timestamp")).rowsBetween(Window.unboundedPreceding, ..) or (-2, 3) /    
   df.withColumn("row_number", row_number().over(window_spec)) 
   sum / min / max / avg / count / collect_list / collect_set / first_value / last_value / nth_value / 
   row_number / rank / dense_rank / lag / lead / ntile / cume_dist() / percent_rank()
>> splitng column, array_length (size), word in array (array_contains), explode/explode_outer, indexing: [0] or .getItem(0)
   df_split = df.withColumn("name_parts", size/explode(split(col("full_name"), " ")))
   df_split = df.withColumn("name_parts", array_contains(split(col("full_name"), " "), 'Alice'))
   df_split = df.withColumn("name_parts", split(col("full_name"), " ").getItem(1)/[0])
>> Pivot: df.groupBy('Month').pivot('product').sum('sales').show()
   unpivot: unpivot_exp = "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (Product, Sales)"  ------> df1.selectExpr("Month", unpivot_exp).show()

>>  
extract part 	- 	substring(col, pos, len) 
substring_index(col, delim, count) â€“ before/after delimiter
locate(substr, col[, pos]) / instr("text", "World")
regexp_replace(col("first"), 'Ali', '') / translate(col("first"), 'Al', '') / 
element size	-	length("text").alias("len")
remove spaces  - 	trim("text").alias("trim")  / ltrim / rtrim / 
capitalize 	-	upper("first").alias("first_upper")	/ lower / initcap /
merge words	-	concat(col("first"), lit(" "), col("last")).alias("full_name_raw") / concat_ws(" ", "first", "last").alias("full_name_ws")
fill with some char 	-	lpad(col("first"), 8, "0").alias("lpad_first")  / rpad
3 times	-	repeat(col("first"), 3).alias("first_x3")

>> 
to_date("date")
year(col("dt/ts"))  -  month / day / hour / minute / second 
date_trunc("month", "dt/ts") 
date_diff("end_date", "start_date") / round((unix_timestamp("end_ts") - unix_timestamp("start_ts")) / 3600, 2)
date_sub("start_date", 10) / date_add("start_date", 10) / expr("ts_col + interval -2 year")

==================> spark-sql 

>> spark temp table (exists only for SS) / global temp table (exists all SS within same app)
   spark.sql('show databases') --> spark.sql('use db_name') --> spark.sql('show tables') --> spark.sql('show tables in global_temp')
   df.createOrReplaceTempView('customers') -->	df.createOrReplaceGlobalTempView('gcustomers')
   spark.sql('select * from customers limit 5') or spark.table('customers') --> spark.sql('describe extended customers') --> spark.sql('drop table customers') 		
>> Spark Persistant: Managed table (spark owns both metadata and data)
   spark.sql('''CREATE TABLE custm (customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) USING CSV''')
   df.write.mode('overwrite').saveAsTable('default.customers_sur')
   data: hdfs://sur-dp-clu11-m/user/hive/warehouse/customers 
   metadata: vi etc/hive/conf/hive-site.xml -> connect mysql -u hive -p  -> Pw1+fgC9gJI+ -> show databases; use metastore; show tables; select * from TBLS; exit
>> Spark Persistant: external table (spark owns metadata and gcs/hdfs owns data)
   spark.sql(''' create external table custe (customer_id STRING, ...) using csv location 'gs://surdatabuk/data/olist/olist_customers_dataset.csv' ''')

=================>>  HiveQL: 

>> sql like interface (HQL) --> translator ---> MR/spark/tej code (is abstraction for java or MR programe) >> cli: set hive.execution.engine=tez/spark/mr;
   table --> data (hdfs, s3, gcs) + metadata (hive stores metastore in it). 
>>  Hive external table (data file in GCS)
   create external table custe (customer_id string, ...) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location 'gs://surdatabuk/data/olist/cust_exe/'
>> Hive managed table (data in hdfs --> hdfs dfs -ls /user/hive/warehouse/  --> set hive.metastore.warehouse.dir; )
   create table custm (	order_id string, ....) STORED AS TEXTFILE;
>> connect: 1.hive & 2.beeline : prod rec --> !connect jdbc:hive2://<hostname>:<port>/<database> / !connect jdbc:hive2://localhost:10000/default (hive & Pw1+fgC9gJI+)

-- hive is database like mysql?		its not a database. but, its DWH tool built on top of Hadoop to query large datasets from HDFS storage
-- hive is replacement for Hadoop? 	No, buit on top of hadoop core components (hdfs, yarn, mr/spark/tej)
-- hive queries like normal sql queries in db?  	HQL queries --> translatior --> mr/spark/tej code which runs on distributed system
-- hive can perform row-level transaction like mysql? 	yes, but, hive is not designed for it (if u change one row- it will recreate entire partition file)
-- hive can works with only HDFS?  			no. HDFS/s3/gcs/data lake
-- hive tables work like noraml database tables? 	hive is metastore and not storing any tables

=================================>> CONCEPT interview QA

>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
	SparkContext: entry point for RDD operations/functionality. SparkSession: Provides a unified entry point for DataFrame and Dataset APIs, SQL queries, and more. 
	key-benfits of high level api compared to low-level api : schema awareness, ease of use, optimized, various file supports 
>> reading data in spark is action or transformation? :  header=true & inferschema=false A |  header=true & inferschema=true A | explicit schema T |
 	writing data is action or transformation? : Action
	how spark reads the data? (already distributed data or read data and partition based on config)
    transformation: operation which creates new rdd from existing one - lazily evaluated -  why trans are lazy & how Lazy Evaluation work in PySpark? 
	action: operation that executes series of transformations and return results.
	narrow transformations: data is not shuffled across the partitions - faster perf - filter
	wide transformations: data is shufflled acrros partitions, requiring network i/o  - slower perf - groupby
	spark jobs (no of actions), stages (no of wide trans + 1), tasks (no of partitions used)
>> .ðœð¨ð¥ð¥ðžðœð­() work?  --> gathers all partitions of RDD and brings them to the driver node. - data is no longer distributed, instead stored in the memory of the driver program
>> UDF: custom Python logic when no built-in Spark SQL functions. maskfun_udf = udf(maskfun, StringType()) 	 /	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))
>> df.repartition(10): random data distribution - to ensure parallelisum
	df.repartition(10, column): data distributed based hash of key column - prepare data for groupby, join operations
>> extract filename: element_at(split(input_file_name(), "/"), -1)

>> sc.parallelize: function to create a RDD from a local in-memory collection on the driver (e.g., a Python list). Best for small test data.
   sc.textFile: function to read data from a text file from HDFS/S3/local FS. Best for large datasets.
>> RDD - reduceByKey: local aggregation - less data for shuffling / DF - df.groupBy("dept").agg(sum("salary").as("total_salary"))
	RDD- groupByKey: no local aggragation - more data for shuffling / DF - df.groupBy("dept").agg(collect_list("salary").as("salaries"))
>> map() returns a single value for each input element / flatMap() return multiple values for each input element
>> Difference between sortby and orderby? - sortBy is used with RDDs and orderBy is used with DataFrames.
	
==========================> coding QA
>> joins
	data1 = [(1,), (1,), (None,), (0,), (3,)]
	data2 = [(1,), (None,), (0,), (5,), (1,)]
	'inner' : 5
	'outer', 'full', 'fullouter', 'full_outer' : 9
	'leftouter', 'left', 'left_outer': 7
	'rightouter', 'right', 'right_outer': 7  
	'leftsemi', 'left_semi', 'semi': 3
	'leftanti', 'left_anti', 'anti' : 2
	'crossJoin': 25
>> display only last 4 characters of card number. : lpad(substring(col("card_no").cast("string"), -4, 4),16, '#')
>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> What are the different ways to remove duplicate records ---> distinct() / dropDuplicates(["name", 'id']) / row_number() / groupBy /
>> find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 
>> split the data into two columns (Even, Odd) 
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> Identify employees whose salary is greater than the average salary of their department.
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> Replace null values in a column with the last non-null value in that partition. -----> last_value('cn', ignorenulls=True).over(window_spec)
>> Categorize employees based on years of experience into Junior, Mid, and Senior levels.
>> Fill the mean salary value in Null. ---> avg_sal = df.select(mean(col('sal'))).collect()[0][0] --> df.fillna({'salary': avg_salary}).show() / avg_sal and cross join
>> ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]
	Replace all NULL values in the Quantity column with 0 / Price column with the average price of the existing data / Fill missing Sales_Date with '2025-01-01'. / 
    Drop rows where the Product column is NULL / Drop rows where all columns are NULL.
>> find max id excluding duplicates | 2, 6, 5, 6, 9, 9, 8: df1=df.groupBy('id').agg(count('id').alias('count')).filter(col('count')==1)
>> emp earning more than avg sal in their department: 
>> Handling NULL values using the average salary | emp_id, salary: IFNULL(cn, rep_val) COALESCE(cn, rep_val1, rep_val2, ...) 
>> Extract the Domain from the Email column 
>> Word count program in pyspark rdd & DF
>> count the number of null values in each column of df. 	df.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()
>> column Country: India Australia Pakistan | Output: India vs Australia India vs Pakistan Australia vs Pakistan
	df.alias("a").crossJoin(df.alias("b")).filter(col("a.Country") < col("b.Country")).selectExpr("concat(a.Country, ' vs ', b.Country) as Matchup")
>> Get the first and last transaction date for each customer. | row_number()
>> Find all employees whose names contain the letters "a" exactly twice:  length("name_lc") - length(regexp_replace("name_lc", "a", ""))
>> find customers who made transactions in every month of the year | cust_id, date, purchage_amount
>> ORDER_DAY, ORDER_ID, PRODUCT_ID, QUANTITY, PRICE | Get me all products that got sold both the days and the number of times the product is sold.  
>> Write a query to count how many employees share the same salary.
>> employees who hired in last 90 days: 
>> Retrieve employees who joined in the last 6 months.

>> why parquet?
- Columnar format: Only reads the region column to filter â€” better compression better than rows..
- Metadata: contains Min/Max values for each column in every data block - help skip blocks -  SELECT * FROM sales WHERE region = 'Asia'
- Schema evolution: You can add new columns later without breaking old data.
- Spark optimization: Spark understands Parquet deeply and uses its metadata smartly.

>> You have a DataFrame:
data = [
    (1, "A", "2024-01-01"),
    (1, "A", "2024-01-02"),
    (1, "B", "2024-01-01"),
    (2, "A", "2024-01-01"),
]
Columns: user_id, product, date
Find duplicate rows per user where product repeats across different dates, but count only unique product-user pairs.

>>
distinct() removes complete row duplicates
dropDuplicates() removes duplicates based on specified columns. 
window functions to retain the latest or most relevant record per business key

>>
âœ… Data Quality (DQ) â†’ checks basic correctness
Example:
 invoice_id not null
 amount > 0
 currency allowed
 tax >= 0

âœ… Data Validation (DV) â†’ checks business logic
Example:
 invoice must be unique
 tax code must exist in master
 tax must match calculation

Key Takeaway
 DQ ensures data is clean
 DV ensures data is correct

>>
!apt-get install openjdk-11-jdk -y
!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar xf spark-3.4.1-bin-hadoop3.tgz
!pip install -q findspark
