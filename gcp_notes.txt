===============>> cloud scheduler

================>> composer
>> Cloud Composer: Apache airflow version in GCP - for workflow (series of task) orchestration - defined by DAG
>> Architexture of composer: Dags dir in gcs - scheduler will trigger the dags - runs dag on worker nodes - metadata db stores inf0 - Web server (for airflow UI)
>> Advantage of cloud composer:
   | DB -- via python --> datalake (gcs) | --> File coneverter csv to parquet --> tarnsform --> load into BQ |
   | py app                             |       dataproc workflow                                            |
   |              cloud composer                                                                              |

===============>> Cloud Run Functions
>> Cloud run functions: 1st gen >> 2nd gen runs on cloud run >> renaned to cloud run functions (aligned with cloud run UI)
>> trigger function on event basis

================>> Data Fusion
>> Data Fusion: Code free, Drag and drop tool - 150+ preconfigured connectors 
>> enable api - create data fusion instance (apox 20 min) - click on view instance - opens web ui in new tab - click on wrangler - select source (here csv files on GCS) - 
   apply transformations - click on create pipeline - select bigquery target using sink - click on validate  - name your pipeline - deploy pipeline - configure pipeline -      schedule or run on demand or airflow orch - 
   while running pipeline it will create dataproc cluster (provising - starting - running - failed or Succeeded - review logs  ) - pipeline failed - review logs - click on     actions - duplicate pipeline - click on wranger properties - remove hash algorithum trans - validate - deploy pipeline - run it again
>> use data fusion pipeline RunID to filter logs in logs explorer / use data fusion console to check logs.

=================>> Databricks
>> Databricks: big data processing 
>> spin up databrick clusters on GCP  >> copy datasets from local dir to dbfs using cli >> process data >> save in target
>> overview of databricks workflows
    task1(py) --> task2(databricks app) --> task3a/3b(databricks app)--> task4(db app) --> task5(py app)
               |                    databricks workflow                                 |
    |                               external orchestration (airflow)                                     |

====================================================

>> Service Account Impersonation: Instead of giving a user or SA direct access to a resource, you allow them to impersonate a SA that has the required permissions. 
   use: logging and SA opeartes the commands on behalf of user
   Deployer Service Account: deployer-sa@my-project.iam.gserviceaccount.com Has roles/storage.admin.
   Impersonator Identity (SA or user): user/SA Has roles/iam.serviceAccountTokenCreator & serviceAccountUser on the deployer SA.
   gcloud storage buckets create gs://my-unique-bucket-name \
     --project=my-project \
     --impersonate-service-account=deployer-sa@my-project.iam.gserviceaccount.com \
     --location=us-central1
===========================================
ACID:Imagine a transaction where Suresh transfers ₹1000 from Account A to Account B.
Atomicity – All parts of a transaction must succeed or none at all.
Consistency – The total money in the system remains ₹8000. No data corruption.
Isolation – Transactions must not interfere with each other.
Durability – Once a transaction is committed, it remains even in case of a system failure

========================
>> Vertex AI: provides tools to implement ML workflow
   ingest, analyse, transform: using managed datasets, label & annotate data
   create and train the model: AutoML- for image, video, text, tablar data it works great & custom - for frameworks, architexture
   evaluate model for efficiency & opti: explainable AI (understand about model like which has more impact)
   deploy the model : scalable H/W
   predictions: get predections via cli, sdk, ui, api

>> 

============================

serverless vs fully-managed vs hybrid:
>> "pay as you use". No traffic, you pay nothing. 
   Ex: Cloud Run, Cloud Function, AppEngine standard, firestore, datastore, dataproc, dataflow, ai-platform 
>> managed but not serverless. 
   You always have a minimal number of VM/node up and you pay for these, traffic or not.           
   However, you have nothing to worry about: patching, updates, networking, backups, HA, redundancy(...) are managed for       you.  
   Ex:like Cloud SQL, BigTable or Spanner, AppEngine flex
>> hybrid product: 
   you pay as you use the processing (BigQuery) or the traffic (Cloud Storage), 
   storage is always billed if you have no traffic.
   ex: Cloud Storage or BigQuery:

================================
>> Managed services in GCP: IaaS, PaaS, CaaS, FaaS, serverless - no infra visibility, zero request zero cost, pay for requests not servers. 
   compute engine - IaaS
   App Engine - PaaS, CaaS (simple), serverless(1+2)
   cloud run - CaaS (simple), serverless
   cloud function - FaaS, serverless(1+2+3)
   GKE - CaaS (needs cluster)

>> Why containers: microservices (flexibility in diff lang) - but deployment complex - thats where container 
   docker is popular tool - create docker image with runtime, app code, dependcies - run image to create containers - adv: light weight, isolated, cloud nuetral
   container orchestration sol (k8s) - offer features: autoscaling, service discovery, load balance, self healing, zero downtime deployments
>> Cluster --> Node Pool --> node
   App --> Container --> Pod, deployment, service, security (iam), logging & Monitoring
>> 
1. gcloud container clusters create
2. login into cloud shell
3. connect cluster: gcloud container clusters get-credentials my-cluster --zone us-central1-c --project my-kubernetes-project-304910
4. deploy microservice to k8s:  
   kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE (docker image pushed to docker hub)
   kubectl get deployment
   expose deploy: 
   kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
   kubectl get services
   kubectl get services --watch
   curl 35.184.204.214:8080/hello-world
5. increase no of instances of microservice: kubectl scale deployment hello-world-rest-api --replicas=3
6. increase no of nodes of k8s cluster: gcloud container clusters resize my-cluster --node-pool default-pool --num-nodes=2 --zone=us-central1-c
7. Autoscaling for microservice
   kubectl autoscale deployment hello-world-rest-api --max=4 --cpu-percent=70
   kubectl get hpa
8. Autoscaling cluster
   gcloud container clusters update my-cluster --enable-autoscaling --min-nodes=1 --max-nodes=10
9. create config map - app to talk db
   kubectl create configmap hello-world-config --from-literal=RDS_DB_NAME=todos
   kubectl get configmap
   kubectl describe configmap hello-world-config
10. k8s secreats instead of config map
    kubectl create secret generic hello-world-secrets-1 --from-literal=RDS_PASSWORD=dummytodos
    kubectl get secret
    kubectl describe secret hello-world-secrets-1
   kubectl apply -f deployment.yaml
11. deploy new ms with gcpu
    gcloud container node-pools list --zone=us-central1-c --cluster=my-cluster
    kubectl get pods -o wide
12. delete microservice 
     kubectl delete service hello-world-rest-api
     kubectl delete deployment hello-world-rest-api
13. delete cluster
    gcloud container clusters delete my-cluster --zone us-central1-c

===============================

>> Secreat Manager: to store, manage and access screats.  use case- passwords, api keys, tls cert, etc..

===============================
pubsub - messaging system for stream analytics (kafca)
use cases - real time data ingestion, iot devices / parallel processing / replicate data among db 

publisher(IOT, App) --> sends message --> | Topic (here msg storage system) --> Subscription | -->
message --> subscriber(BQ)
subscriber ack message from subscription -  pull / push
one to many / many to many / many to one

IOT (publisher) --> pubsub (stream pipeline) --> dataflow (etl / subscriber) --> BQ--> Looker 
                        gcs (batch pipeline) -->

=================================

ways to authenticate GCP:
>> Application Default Credentials (ADC) in GCP: Default authentication in GCP-managed environments
   Local: Use gcloud auth application-default login for local development.
   Cloud: Deploy the code to GCP, where ADC will use the default service account.
   gcloud auth application-default set-quota-project powerful-layout-445408-p5 
   gcloud auth application-default login    (Quota project "powerful-layout-445408-p5" was added to ADC which can be used by Google client libraries for billing and quota.)
   gcloud auth application-default print-access-token (to verofy credintial)
>> Service Account Key file: Local scripts accessing GCP resources.  / used for on-prim app, other cloud, CI/CD pipelines
   in Bash:   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your-service-account-key.json"
   In Python: os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/path/to/service-account-key.json"
>> OAuth 2.0: For web apps requiring user interaction / You are building a web app that allows users to access their Google Drive.
>> Using User Credentials: Easy for local development
   gcloud auth login / gcloud auth application-default login
>> Workload Identity Federation: Your app is running on AWS Lambda, and you need it to access GCP Storage.

================================
Ways to interact with GCP:
UI / CLI - Cloud shell (preconfigured cloud SDK) & Terminal/cmd (install cloud sdk) / Client libraries / Rest api

================================

IAM
Identity: user, groups, SA, domain
Resource
Roles – primitive, predefined, custom
Policy – binding identity with role
SA- default, user-maintained, google-maintained

App on vm – accessing GCS bucket
On-prim - accessing GCS bucket (long-lived)
On-prim – GCP API (short-lived)
Static website - accessing GCS bucket (public access)

Bucket level accesses – IAM (uniform)
Object level accesses – ACL (fine-grained)
Signed url – user (gcp account not required) can read object from GCS using signurl for specified period.

==============================

GKE:
https://www.okteto.com/blog/kubernetes-basics/

==============================

data base concepts:
Fact tables - contain numerical data, 
dimension tables - provide context and background information.

Star schema contains a fact table surrounded by dimension tables.
Snowflake schema is surrounded by dimension table which are in turn surrounded by dimension table

===============================

Dataprep for data preparation (cleaning, wrangling) for analysis and ML 
- built by trifacta 3rd party tool – need to share data with trifacta (dis adv)
- serverless
- automatically detects schema and anomalies
 
Raw – prepare data using dataprep – run job on dataflow/trifacta(for small dataset) – dataflow job created – 

===============================

Could build – builds container image deployed to cloud run – cloudrun see metrics, versions, traffic
Pre-requisites:
Cloudfunctions api / cloud build api (its ci-cd tool to deploy function)/cloud logging api/ cloud pubsub api/ eventarc api

Project IAM admin role – 

===============================

Cloud composer:
Manages apache airflow env on GCP

===============================

Cloudrun:
Container to production in sec

Automate Python script execution on GCP

 
https://github.com/rafaello9472/c4ds/tree/main/Automate%20Python%20script%20execution%20on%20GCP%20

===============================





