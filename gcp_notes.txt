===================
Data fusion / DataProc / Composer / Databricks / cloud sql (postgresql) /

=====================

Data Fusion:
>> Fully managed , cloud native solution to quickly building data pipelines
>> Code free, Drag n drop tool
>> 150+ preconfigured connectors & transformations
>> Built with Open source CDAP
>> 3 Edition are available: Developer / Basic / Enterprise

>> enable api - create data fusion instance (apox 20 min) - click on view instance - opens web ui in new tab - 
click on wrangler - select source (here csv files on GCS) - 
apply transformations (split body column into multiple columns using parse as csv - remove body column - filter remove rows
if value is empty - change data type - find and replace - mask data - hash data using algorithum ) 
- click on create pipeline - choose batch / stream here gcs supports batch pipeline - select bigquery target using sink - 
configure bq properties (poject, dataset, table, review columns) - click on validate  - name your pipeline - deploy pipeline - configure pipeline - schedule or run on demand or airflow orch - 
while running pipeline it will create dataproc cluster (provising - starting - failed or success - review logs  ) - 
pipeline failed - review logs - click on actions - duplicate pipeline - click on wranger properties - remove hash algorithum trans - validate - 
deploy pipeline - run it again

=======================
>> Apache Spark - multi-language engine for executing data engineering, data science, and machine learning on single-node       machines or clusters.

>> Apache Spark is the core distributed data processing engine, suitable for large-scale data workloads, and can be             deployed on-premises or in the cloud.
   Google Cloud Dataproc is a managed service that runs Apache Spark (and other frameworks), making it easier to manage,       scale, and integrate with Google Cloud services.

Dataproc: big data processing
1. enable api 
2. create dataproc cluster on vm/gke.
   cluster type: single node 1m0w, standard for dev team 1m Nw, high availability for prod 3m Nw
   configure nodes
   network

gcloud dataproc clusters create dataprocclu1 --enable-component-gateway --region us-central1 --subnet default --no-address --zone us-central1-a --single-node --master-machine-type n1-standard-4 --master-boot-disk-size 500 --image-version 2.2-debian12 --optional-components JUPYTER --scopes 'https://www.googleapis.com/auth/cloud-platform' --project possible-dream-433814-b4

3. validate ssh connectivity to master node vm of dataproc cluster: gcloud ssh cammand helps to copy key on vm and able to       ssh vm (external ip is emphemeral)
   >> establish an SSH connection to a remote server VM instance:
      gcloud compute ssh --zone "us-central1-a" "dataprocclu1-m" --project "possible-dream-433814-b4"
      or
      ssh -i C:\Users\surmacha\.ssh\google_compute_engine surmacha@35.184.234.232

3.1. allocate static ip to it.

4. setup vs code remote window for dataproc vm of master node.
5. copy local files/ gcs blobs into hdfs on dataproc.
6. validate pyspark / spark-shell (spark scala) / spark-sql CLI in dataproc cluster.
7. submit dataproc job using spark sql in UI
8. create dataproc workflow using jobs
   cleanup --> convert json file to parquet for orders/order_items --> compute daily product revenue and save back to gcs.
   >> review the scripts.
   >> apply unit tests/validation in local vs code environment configured with dataproc cluster master node.
   >> use scripts in gcs rather then hdfs (temproray based on cluster)
   >> copy spark-sql scripts to gcs location.
   >> run and validate spark-sql scripts placed in gcs using vs code configured with dataproc master node
   >> limitations of running spark sql scripts using dataproc jobs
         submtting jobs by defining query file in gcs via ui is not working (gcloud cli is working).
         submit jobs using gcloud from external cli but not from vs code dataproc master node
   >> submit spark sql jobs: list spark-sql scripts in gcs and review content then submit job using dataproc gcloud command.
   >> dataproc workflow templates: orchestrated pipeline within gcp dataproc
         UI- create template using jobs defined by query text (query file not working in job) - not use in practical
         gcloud commands (more flexibility than ui):
               1. create dataproc workflow template (gcloud dataproc workflow-templates create ----)
               2. attach workflow template with existing cluster (gcloud dataproc workflow-templates add-cluster-selector---)
               3. add jobs to workflow template (gcloud dataproc workflow-templates add-job ----)
               4. instantiate workflow template (gcloud dataproc workflow-templates instantiate ----)
   >> stop / delete cluster using ui, gcloud

==================

===========================
>> 
Apache airflow operators guide: https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/index.html

-- data pipeline: consists of several tasks or actions
-- airflow: workflows - made of dags - automation - scheduling
-- Cloud composer: full managed apache airflow version in GCP for workflow orchestration.

-- create composer environment: runs on GKE
    enable api
    location
    image version (cloud composer and airflow)
    Cloud Composer v2 API Service Agent Extension role to the service account
    enivronment (schedular, triggerrer, web server, worker)
    network

-- setup local vs code env and install required dependcies
    use gcloud commands to intract with cloud composer env
    deploy airflow dag into composer env by placing into gcs dags folder


-- Architexture of composer: Web server (UI) - Dags dir in gcs - each dag associated schedule with it -    
    based on schedule, the Executor/scheduler will trigger the dags - runs on worker 
    - metadata db stores executor, web server info

-- dag constructor/class - with in dag id, default args, keyword args, tasks are created by instntiating operators - it helps to create dag object
    note: multiple dags in one py script / one dag can spans multiple py scripts

| DB -- via python --> datalake (gcs) | --> File coneverter csv to parquet --> tarnsform --> load into BQ |
| py app                             |       dataproc workflow                                            |
|              cloud composer                                                                              |

>> templating in Airflow : use templating to inject dynamic values (like dates and parameters) into your bash commands in Airflow.
  
>> use variables
   
>> Calling bash/py script in different folder / machine by using connections

>> XCom (Cross-communication) - for passing small pieces of data between tasks - making workflows more dynamic and flexible

>> branching -  allows you to control the execution path of a workflow based on certain conditions - BranchPythonOperator 

>> Subdag : consists group of parellel tasks.

>> 
SLA (Service Level Agreement) - 
   - is a time-based contract specifying how long a task or DAG should take to run. 
   - If a task exceeds this time limit, Airflow will trigger an alert to notify that the SLA has been missed.

Apache Airflow with Kubernetes allows for a scalable and flexible infrastructure, integrating the dynamic orchestration power of Airflow with the container management capabilities of Kubernetes. This integration provides benefits like dynamic resource allocation, auto-scaling, and isolated task execution in containers. There are two primary ways Kubernetes can be integrated with Airflow:

   Kubernetes Executor: Airflow tasks are dynamically launched in Kubernetes pods.
   KubernetesPodOperator: Individual tasks are run inside Kubernetes pods, but Airflow is otherwise deployed with a             different executor.

Sensors continuously check (or "poke") to see if a condition has been met. 
   Once the condition is satisfied, the sensor task succeeds, allowing the next task in the workflow to execute. 
   If the condition is not met within the defined timeout period, the sensor fails.

   Types of Sensors
   FileSensor: Waits for a file to appear in a directory.
   ExternalTaskSensor: Waits for another task in a different DAG to complete.
   HttpSensor: Waits for an HTTP endpoint to return a certain response.
   S3/GCSKeySensor: Waits for a file to appear in an S3 bucket.
   TimeSensor: Waits until a specific time of day.

>> Apache Airflow vs beam vs spark
   Airflow: schedule and monitor workflows
   Beam: data processing pipeline (google has cloud dataflow)
   Spark: fast and general processing engine compatible with hadoop eco system. (Dataproc in Google)


=====================
Databricks: big data processing
1. signup for databricks from marketplace to use on GCP. then page redirected to workspaces.
2. create databricks workspace on GCP
   data bricks run on cloud platform and utilizes some resorces from underlying gcp project.
3. open workspace UI and spin up databrick clusters on GCP.(cluster mode, node type, runtime version-scala,spark)
4. getting started with databrick notebook on cluster. (do developement using python code and use magic commands to interact with databricks fs)
5. setup databricks cli/sdk or restApi for clients/servers on winodws.
6. manage databricks clusters/fs/jobs/secreats using cli
7. copy datasets from local dir to dbfs using cli
8. process data in dbfs using spark sql in notebooks
9. spark sql to find daily product revenue from the dbfs files
10. convert csv to parquet with schema using pyspark.

11. overview of databricks workflows
    task1(py) --> task2(databricks app) --> task3a/3b(databricks app)--> task4(db app) --> task5(py app)
               |                    databricks workflow                                 |
    |                               external orchestration (airflow)                                     |
12. Pass arguments to databricks python notebooks/ sql notebooks
13. click on dataflow workflow - create job - under job define several tasks and dependency, also utlize cluster type based 
     task load 
    --> create and run first databricks job.
    --> pass arg at task level / job level.
    --> schedule the db workflow job in Job ui (but usally schedule these jobs using external orchestration i.e. airflow)
14. real time databricks job
    --> cleanup database and datasets. Task1
    --> convert csv to parquet by applying schema. Task2
    --> create spark sql table on top of parquet files. Task3
    --> claucate daily product revenue from sql  tables and write result to target in parquet format. Task4
    --> validata app (tasks) for ELT pipeline using databricks.
    --> build ELT pipeline using databricks job in workflows.
    --> run and review execution elt pipeline using databricks job.

==============
cloud sql (postgresql)
Managing PostgreSQL database via UI(pgAdmin), CLI, Python client library

on-prim data files ---> process using python pandas --> Load into PostgreSQL DB --> validate data

read data from PostgreSQL DB --> process using python pandas --> GCS in parquet file --> published into BQ table.

====================



=======
Secreat Manager: to store, manage and access screats.
use case- passwords, api keys, tls cert, etc..
access DB passwords from the secreat manager by python scripts.

================
GCP cloud logging: store logs in JSON format
logs - contains timestamp, resorce, payload, activity (admin/dataAccess/systenEvent/policyDenied)
export logs to gcs/bq/pubsub topic using sinks (has sa and grant req roles)

=================
pubsub - messaging system for stream analytics (kafca)
use cases - real time data ingestion, iot devices / parallel processing / replicate data among db 

publisher(IOT, App) --> sends message --> | Topic (here msg storage system) --> Subscription | -->
message --> subscriber(BQ)
subscriber ack message from subscription -  pull / push
one to many / many to many / many to one

IOT (publisher) --> pubsub (stream pipeline) --> dataflow (etl / subscriber) --> BQ--> Looker 
                        gcs (batch pipeline) -->

==================

gcloud auth application-default login    
Quota project "sur-cloud-fun" was added to ADC which can be used by Google client libraries for billing and quota.

=================

Ways to interact with GCP:
UI / CLI - Cloud shell (preconfigured cloud SDK) & Terminal/cmd (install cloud sdk) / Client libraries / Rest api

=========

serverless means "pay as you use". No traffic, you pay nothing, 
(like Cloud Run, Cloud Function, AppEngine standard, firestore, datastore, dataproc, dataflow, ai-platform).

managed but not serverless - always have a minimal number of VM/node up and you pay for these traffic or not - 
However, you have nothing to worry about: patching, updates, networking, backups, HA, redundancy(...) are managed for you.
(like Cloud SQL, BigTable or Spanner or AppEngine flex belong to this category).

hybrid product - like Cloud Storage or BigQuery: 
you pay as you use the processing (BigQuery) or the traffic (Cloud Storage)
but the storage is always billed if you have no traffic.

This is for GCP. If you look for other cloud provider, the definition is not the same. 

====================

IAM
Identity: user, groups, SA, domain
Resource
Roles – primitive, predefined, custom
Policy – binding identity with role
SA- default, user-maintained, google-maintained

App on vm – accessing GCS bucket
On-prim - accessing GCS bucket (long-lived)
On-prim – GCP API (short-lived)
Static website - accessing GCS bucket (public access)

Bucket level accesses – IAM (uniform)
Object level accesses – ACL (fine-grained)
Signed url – user (gcp account not required) can read object from GCS using signurl for specified period.

================

GKE:
https://www.okteto.com/blog/kubernetes-basics/

===============

===============

data base concepts:
Fact tables - contain numerical data, 
dimension tables - provide context and background information.

Star schema contains a fact table surrounded by dimension tables.
Snowflake schema is surrounded by dimension table which are in turn surrounded by dimension table

==================

Dataprep for data preparation (cleaning, wrangling) for analysis and ML 
- built by trifacta 3rd party tool – need to share data with trifacta (dis adv)
- serverless
- automatically detects schema and anomalies
 
Raw – prepare data using dataprep – run job on dataflow/trifacta(for small dataset) – dataflow job created – 

================



==================


Could build – builds container image deployed to cloud run – cloudrun see metrics, versions, traffic
Pre-requisites:
Cloudfunctions api / cloud build api (its ci-cd tool to deploy function)/cloud logging api/ cloud pubsub api/ eventarc api

Project IAM admin role – 


================

Cloud composer:
Manages apache airflow env on GCP

===============

Cloudrun:
Container to production in sec

Automate Python script execution on GCP

 
https://github.com/rafaello9472/c4ds/tree/main/Automate%20Python%20script%20execution%20on%20GCP%20


===================


======================

Hoogle:

IAM
>> Create Service account via automation 
>> Create AD group/user account via HSBC active directory request using SNOW portal - through sync process AD group/user s recognized on GCP via single sign-on. (managed at org level).
Diff AD vs google cloud 
>> Assign roles to AD group/SA via automation
>> creating service account file
The service account file is used for non-Google environments that need to to communicate with Google Cloud Projects.
gcloud iam service-accounts add-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountKeyAdmin
gcloud iam service-accounts keys create [FILENAME].json --iam-account=[SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com 
gcloud iam service-accounts keys list --iam-account=[SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud iam service-accounts keys delete [KEY_ID] --iam-account=[SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud iam service-accounts remove-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountKeyAdmin
>> service account impersonation
Benfits: no floting keys, enhance security, short lived tokens
User/SA (SATokenCreator role)----impersonate-------> SA ------has permissions --------> gcp resources
gcloud iam service-accounts add-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountTokenCreator
gcloud [COMMAND] --impersonate-service-account [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud config set auth/impersonate_service_account [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud config unset auth/impersonate_service_account
gcloud iam service-accounts remove-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountTokenCreator

===================

BIGQUERY: 

>> Time travel concept: Query the state of the table as it was at a specific timestamp

>> Creation of view/materialized view and auth view: efficiently manage and secure your data access in BigQuery
   Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data       access / 
   Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
   Authorized views and authorized materialized views: let you share query results with particular users and groups without    giving them access to the underlying source data.

>> 
DDL(Data Definition Language) – define structure/schema of database (Create/alter/drop/truncate)
DML(Data Manipulation Language) – manipulate data (Insert/update/delete/merge)
DQL(Data Query Language) – select 
TCL(Transaction Control Language) – commit transaction and role back in case of any errors
DCL(Data Control Language) – for security and access control 

>> Create table with metadata from existing table – use where 1 = 2 or false

>> except / distinct / limit / Round /
concat / left / right / substring / upper / lower / replace / ltrim / rtrim/ trim / contains_substr() / length() / starts_with() / ends_with() / strpos()

>> Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

>>   Permanent table: save query results on BQ storage
   Temporary table/cache – Query results saved on cache and valid for 24 hrs.
   Internal/native table/managed tables: table in BQ storage
   External table: BQ querying data from bigtable, gcs, google drive

>> Partitioning and clustering:
   Improved Query Performance: By scanning only relevant partitions and clustered blocks, queries run faster.
   Cost Efficiency: Reduced data scanned means lower costs, as BigQuery charges based on the amount of data processed.

>> Joins: -  combine rows from two or more tables based on a related column between them.
   INNER JOIN / left join / right join / full join / cross join / self join /
   Best Practices for Joins in BigQuery: Use Appropriate Join Types: / Filter Early / Avoid Cross Joins When Possible / 
   Use Partitioned and Clustered Tables: If joining large tables / Monitor Query Performance /

>> With statement: also known as Common Table Expressions (CTEs), allows you to define temporary result sets.
   Adv: code more readable, resusebility - referenced in several places within the main SQL query. 

>> conversion functions:
   The CAST function performs a conversion between compatible data types. 
   If the conversion fails due to data type incompatibility or data loss, the query will fail
   The SAFE_CAST function also performs a conversion between compatible data types, 
   but it handles conversion errors differently. Instead of failing the query, it returns NULL when the conversion cannot    be performed.

>> commenting: -- or # or /* LINES */

>> Conditions:
If condition
then
sql statement;
end if;

for var in list
do 
sql statement;
end for;

/ labels /repeat / while /leave/ break/ continue/ iterate 

>> Pivoting and unpivoting: help in reshaping your data for analysis or reporting
   Pivot: row into columns
   unpivot: columns into rows

>> Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
   De Normalization – redundant data (high storage cost) – better Query performance 
   Why array & structs? – for better query per and lesser storage costs
   
   Arrays – list of items having same data type
   SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element;

   Struct – record with nested fields having different data types
   SELECT person.id, person.name FROM mydataset.mytable;
 
>> Window functions: performing advanced calculations over a set of rows, go beyond simple aggregates or filters
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /row_number() / rank() / dense_rank() 
   SUM(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rt
   lag(cn) / lead(cn): Accesses data from a previous or subsequent row in the same result set without use of a self-join.
   LAG(column2, 1) OVER (PARTITION BY column1 ORDER BY column2) AS previous_value,
   first_value(cn) / last_value(cn) / nth_value(cn, n) 
   FIRST_VALUE(column2) OVER (PARTITION BY column1 ORDER BY column2) AS first_value,
   ntile(n) – n groups 
   NTILE(4) OVER (PARTITION BY column1 ORDER BY column2) AS quartile
   cum_dist() / percent_rank() 
   Rows consider current row even if duplicates / range considers bottom row of duplicates.

>> Regular expressions: used for pattern matching and string manipulation
   REGEXP_CONTAINS(column_name, r'pattern')
   REGEXP_EXTRACT_ALL(column_name, r'pattern')
   REGEXP_REPLACE(column_name, r'pattern', 'replacement')
   SPLIT(column_name, r'pattern')

>> Sql query execution order: From / where / group by / having / select / order by / limit

>> BQ 
>> Big Query cost optimization
   -- compute optimization: on-demaned analysis, bq editions - decide based on your workloads.
   -- BQ data storage – (Billing model: logical- data in uncompressed format/ physical- data in compressed format) - based          on compression ratio
   -- Query optimization 
   Use Partitioned Tables and Clustered Tables
   select columns instead of *
   Use WHERE Clauses for Filtering
   Optimize JOIN Operations
   Avoid Nondeterministic Functions: CURRENT_TIMESTAMP()) in SELECT statements; 
   Aggregate Data at the Source: Use Approximate Aggregations:  
   Optimize Window Functions
   Monitor and Tune Query Performance
   
   Cost Controls: Set budget alerts and quotas
   Schedule queries during off-peak hours to take advantage of low on-demand pricing typically at mid-night/early morning

==================






