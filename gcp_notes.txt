===================
Data fusion / DataProc / Composer / Databricks / cloud sql (postgresql) /

=====================

Data Fusion:
>> Fully managed , cloud native solution to quickly building data pipelines
>> Code free, Drag n drop tool
>> 150+ preconfigured connectors & transformations
>> Built with Open source CDAP
>> 3 Edition are available: Developer / Basic / Enterprise

>> enable api - create data fusion instance (apox 20 min) - click on view instance - opens web ui in new tab - 
click on wrangler - select source (here csv files on GCS) - 
apply transformations (split body column into multiple columns using parse as csv - remove body column - filter remove rows
if value is empty - change data type - find and replace - mask data - hash data using algorithum ) 
- click on create pipeline - choose batch / stream here gcs supports batch pipeline - select bigquery target using sink - 
configure bq properties (poject, dataset, table, review columns) - click on validate  - name your pipeline - deploy pipeline - configure pipeline - schedule or run on demand or airflow orch - 
while running pipeline it will create dataproc cluster (provising - starting - failed or success - review logs  ) - 
pipeline failed - review logs - click on actions - duplicate pipeline - click on wranger properties - remove hash algorithum trans - validate - 
deploy pipeline - run it again

=======================
Dataproc: big data processing
1. enable api 
2. create dataproc cluster on vm/gke.
   cluster type: single node 1m0w, standard for dev team 1m Nw, high availability for prod 3m Nw
   configure nodes
   network

gcloud dataproc clusters create dataprocclu1 --enable-component-gateway --region us-central1 --subnet default --no-address --zone us-central1-a --single-node --master-machine-type n1-standard-4 --master-boot-disk-size 500 --image-version 2.2-debian12 --optional-components JUPYTER --scopes 'https://www.googleapis.com/auth/cloud-platform' --project possible-dream-433814-b4

3. validate ssh connectivity to master node vm of dataproc cluster: gcloud ssh cammand helps to copy key on vm and able to       ssh vm (external ip is emphemeral)
   >> establish an SSH connection to a remote server VM instance:
      gcloud compute ssh --zone "us-central1-a" "dataprocclu1-m" --project "possible-dream-433814-b4"
      or
      ssh -i C:\Users\surmacha\.ssh\google_compute_engine surmacha@35.184.234.232

3.1. allocate static ip to it.

4. setup vs code remote window for dataproc vm of master node.
5. copy local files/ gcs blobs into hdfs on dataproc.
6. validate pyspark / spark-shell (spark scala) / spark-sql CLI in dataproc cluster.
7. submit dataproc job using spark sql in UI
8. create dataproc workflow using jobs
   cleanup --> convert json file to parquet for orders/order_items --> compute daily product revenue and save back to gcs.
   >> review the scripts.
   >> apply unit tests/validation in local vs code environment configured with dataproc cluster master node.
   >> use scripts in gcs rather then hdfs (temproray based on cluster)
   >> copy spark-sql scripts to gcs location.
   >> run and validate spark-sql scripts placed in gcs using vs code configured with dataproc master node
   >> limitations of running spark sql scripts using dataproc jobs
         submtting jobs by defining query file in gcs via ui is not working (gcloud cli is working).
         submit jobs using gcloud from external cli but not from vs code dataproc master node
   >> submit spark sql jobs: list spark-sql scripts in gcs and review content then submit job using dataproc gcloud command.
   >> dataproc workflow templates: orchestrated pipeline within gcp dataproc
         UI- create template using jobs defined by query text (query file not working in job) - not use in practical
         gcloud commands (more flexibility than ui):
               1. create dataproc workflow template (gcloud dataproc workflow-templates create ----)
               2. attach workflow template with existing cluster (gcloud dataproc workflow-templates add-cluster-selector---)
               3. add jobs to workflow template (gcloud dataproc workflow-templates add-job ----)
               4. instantiate workflow template (gcloud dataproc workflow-templates instantiate ----)
   >> stop / delete cluster using ui, gcloud

==================

===========================
-- Cloud composer: full managed apache airflow version in GCP for workflow orchestration.

-- create composer environment: runs on GKE
    enable api
    location
    image version (cloud composer and airflow)
    Cloud Composer v2 API Service Agent Extension role to the service account
    enivronment (schedular, triggerrer, web server, worker)
    network

-- setup local vs code env and install required dependcies
    use gcloud commands to intract with cloud composer env
    deploy airflow dag into composer env by placing into gcs dags folder


-- Architexture of composer: Web server (UI) - Dags dir in gcs - each dag associated schedule with it -    
    based on schedule, the Executor/scheduler will trigger the dags - runs on worker 
    - metadata db stores executor, web server info

-- dag constructor/class - with in dag id, default args, keyword args, tasks are created by instntiating operators - it helps to create dag object
    note: multiple dags in one py script / one dag can spans multiple py scripts

| DB -- via python --> datalake (gcs) | --> File coneverter csv to parquet --> tarnsform --> load into BQ |
| py app                             |       dataproc workflow                                            |
|              cloud composer                                                                              |


=====================
Databricks: big data processing
1. signup for databricks from marketplace to use on GCP. then page redirected to workspaces.
2. create databricks workspace on GCP
   data bricks run on cloud platform and utilizes some resorces from underlying gcp project.
3. open workspace UI and spin up databrick clusters on GCP.(cluster mode, node type, runtime version-scala,spark)
4. getting started with databrick notebook on cluster. (do developement using python code and use magic commands to interact with databricks fs)
5. setup databricks cli/sdk or restApi for clients/servers on winodws.
6. manage databricks clusters/fs/jobs/secreats using cli
7. copy datasets from local dir to dbfs using cli
8. process data in dbfs using spark sql in notebooks
9. spark sql to find daily product revenue from the dbfs files
10. convert csv to parquet with schema using pyspark.

11. overview of databricks workflows
    task1(py) --> task2(databricks app) --> task3a/3b(databricks app)--> task4(db app) --> task5(py app)
               |                    databricks workflow                                 |
    |                               external orchestration (airflow)                                     |
12. Pass arguments to databricks python notebooks/ sql notebooks
13. click on dataflow workflow - create job - under job define several tasks and dependency, also utlize cluster type based 
     task load 
    --> create and run first databricks job.
    --> pass arg at task level / job level.
    --> schedule the db workflow job in Job ui (but usally schedule these jobs using external orchestration i.e. airflow)
14. real time databricks job
    --> cleanup database and datasets. Task1
    --> convert csv to parquet by applying schema. Task2
    --> create spark sql table on top of parquet files. Task3
    --> claucate daily product revenue from sql  tables and write result to target in parquet format. Task4
    --> validata app (tasks) for ELT pipeline using databricks.
    --> build ELT pipeline using databricks job in workflows.
    --> run and review execution elt pipeline using databricks job.

==============
cloud sql (postgresql)
Managing PostgreSQL database via UI(pgAdmin), CLI, Python client library

on-prim data files ---> process using python pandas --> Load into PostgreSQL DB --> validate data

read data from PostgreSQL DB --> process using python pandas --> GCS in parquet file --> published into BQ table.

====================



=======
Secreat Manager: to store, manage and access screats.
use case- passwords, api keys, tls cert, etc..
access DB passwords from the secreat manager by python scripts.

================
GCP cloud logging: store logs in JSON format
logs - contains timestamp, resorce, payload, activity (admin/dataAccess/systenEvent/policyDenied)
export logs to gcs/bq/pubsub topic using sinks (has sa and grant req roles)

=================
pubsub - messaging system for stream analytics (kafca)
use cases - real time data ingestion, iot devices / parallel processing / replicate data among db 

publisher(IOT, App) --> sends message --> | Topic (here msg storage system) --> Subscription | -->
message --> subscriber(BQ)
subscriber ack message from subscription -  pull / push
one to many / many to many / many to one

IOT (publisher) --> pubsub (stream pipeline) --> dataflow (etl / subscriber) --> BQ--> Looker 
                        gcs (batch pipeline) -->

==================

gcloud auth application-default login    
Quota project "sur-cloud-fun" was added to ADC which can be used by Google client libraries for billing and quota.

=================

Ways to interact with GCP:
UI / CLI - Cloud shell (preconfigured cloud SDK) & Terminal/cmd (install cloud sdk) / Client libraries / Rest api

=========

serverless means "pay as you use". No traffic, you pay nothing, 
(like Cloud Run, Cloud Function, AppEngine standard, firestore, datastore, dataproc, dataflow, ai-platform).

managed but not serverless - always have a minimal number of VM/node up and you pay for these traffic or not - 
However, you have nothing to worry about: patching, updates, networking, backups, HA, redundancy(...) are managed for you.
(like Cloud SQL, BigTable or Spanner or AppEngine flex belong to this category).

hybrid product - like Cloud Storage or BigQuery: 
you pay as you use the processing (BigQuery) or the traffic (Cloud Storage)
but the storage is always billed if you have no traffic.

This is for GCP. If you look for other cloud provider, the definition is not the same. 

====================

IAM
Identity: user, groups, SA, domain
Resource
Roles – primitive, predefined, custom
Policy – binding identity with role
SA- default, user-maintained, google-maintained

App on vm – accessing GCS bucket
On-prim - accessing GCS bucket (long-lived)
On-prim – GCP API (short-lived)
Static website - accessing GCS bucket (public access)

Bucket level accesses – IAM (uniform)
Object level accesses – ACL (fine-grained)
Signed url – user (gcp account not required) can read object from GCS using signurl for specified period.

================

GKE:
https://www.okteto.com/blog/kubernetes-basics/

===============

===============

data base concepts:
Fact tables - contain numerical data, 
dimension tables - provide context and background information.

Star schema contains a fact table surrounded by dimension tables.
Snowflake schema is surrounded by dimension table which are in turn surrounded by dimension table

==================

Dataprep for data preparation (cleaning, wrangling) for analysis and ML 
- built by trifacta 3rd party tool – need to share data with trifacta (dis adv)
- serverless
- automatically detects schema and anomalies
 
Raw – prepare data using dataprep – run job on dataflow/trifacta(for small dataset) – dataflow job created – 

================



==================


Could build – builds container image deployed to cloud run – cloudrun see metrics, versions, traffic
Pre-requisites:
Cloudfunctions api / cloud build api (its ci-cd tool to deploy function)/cloud logging api/ cloud pubsub api/ eventarc api

Project IAM admin role – 


================

Cloud composer:
Manages apache airflow env on GCP

===============

Cloudrun:
Container to production in sec

Automate Python script execution on GCP

 
https://github.com/rafaello9472/c4ds/tree/main/Automate%20Python%20script%20execution%20on%20GCP%20


===================


======================
IAM
>> Create Service account via automation 
>> Create AD group/user account via HSBC active directory request using SNOW portal - through sync process AD group/user s recognized on GCP via single sign-on. (managed at org level).
Diff AD vs google cloud 
>> Assign roles to AD group/SA via automation
>> creating service account file
The service account file is used for non-Google environments that need to to communicate with Google Cloud Projects.
gcloud iam service-accounts add-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountKeyAdmin
gcloud iam service-accounts keys create [FILENAME].json --iam-account=[SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com 
gcloud iam service-accounts keys list --iam-account=[SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud iam service-accounts keys delete [KEY_ID] --iam-account=[SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud iam service-accounts remove-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountKeyAdmin
>> service account impersonation
Benfits: no floting keys, enhance security, short lived tokens
User/SA (SATokenCreator role)----impersonate-------> SA ------has permissions --------> gcp resources
gcloud iam service-accounts add-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountTokenCreator
gcloud [COMMAND] --impersonate-service-account [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud config set auth/impersonate_service_account [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com
gcloud config unset auth/impersonate_service_account
gcloud iam service-accounts remove-iam-policy-binding [SERVICE_ACCOUNT]@[PROJECT_ID].iam.gserviceaccount.com \
--member group:gcp.[PROJECT_ID].devops-team@hsbc.com \
--role roles/iam.serviceAccountTokenCreator

===================









