===================
DataProc / Composer /

=====================
Dataproc: big data processing
1. enable api 
2. create dataproc cluster on vm/gke.
3. validate ssh connectivity to master node vm of dataproc cluster and allocate static ip to it.
4. setup vs code remote window for dataproc vm of master node.
5. copy local files/ gcs blobs into hdfs on dataproc.
6. validate pyspark / spark-shell (spark scala) / spark-sql CLI in dataproc cluster.
7. submit dataproc job using spark sql in UI
8. create dataproc workflow using jobs
   cleanup --> convert json file to parquet for orders/order_items --> compute daily product revenue and save back to gcs.
   >> review the scripts.
   >> apply unit tests/validation in local vs code environment configured with dataproc cluster master node.
   >> use scripts in gcs rather then hdfs (temproray based on cluster)
   >> copy spark-sql scripts to gcs location.
   >> run and validate spark-sql scripts placed in gcs using vs code configured with dataproc master node
   >> limitations of running spark sql scripts using dataproc jobs
         submtting jobs by defining query file in gcs via ui is not working (gcloud cli is working).
         submit jobs using gcloud from external cli but not from vs code dataproc master node
   >> submit spark sql jobs: list spark-sql scripts in gcs and review content then submit job using dataproc gcloud command.
   >> dataproc workflow templates: orchestrated pipeline within gcp dataproc
         UI- create template using jobs defined by query text (query file not working in job) - not use in practical
         gcloud commands (more flexibility than ui):
               1. create dataproc workflow template (gcloud dataproc workflow-templates create ----)
               2. attach workflow template with existing cluster (gcloud dataproc workflow-templates add-cluster-selector---)
               3. add jobs to workflow template (gcloud dataproc workflow-templates add-job ----)
               4. instantiate workflow template (gcloud dataproc workflow-templates instantiate ----)
   >> stop / delete cluster using ui, gcloud


=========
Databricks: big data processing
1. signup for databricks from marketplace to use on GCP. then page redirected to workspaces.
2. create databricks workspace on GCP
   data bricks run on cloud platform and utilizes some resorces from underlying gcp project.
3. open workspace UI and spin up databrick clusters on GCP.(cluster mode, node type, runtime version-scala,spark)
4. getting started with databrick notebook on cluster. (do developement using python code and use magic commands to interact with databricks fs)
5. setup databricks cli/sdk or restApi for clients/servers on winodws.
6. manage databricks clusters/fs/jobs/secreats using cli
7. copy datasets from local dir to dbfs using cli
8. process data in dbfs using spark sql in notebooks
9. spark sql to find daily product revenue from the dbfs files
10. convert csv to parquet with schema using pyspark.

11. overview of databricks workflows
    task1(py) --> task2(databricks app) --> task3a/3b(databricks app)--> task4(db app) --> task5(py app)
               |                    databricks workflow                                 |
    |                               external orchestration (airflow)                                     |
12. Pass arguments to databricks python notebooks/ sql notebooks
13. click on dataflow workflow - create job - under job define several tasks and dependency, also utlize cluster type based 
     task load 
    --> create and run first databricks job.
    --> pass arg at task level / job level.
    --> schedule the db workflow job in Job ui (but usally schedule these jobs using external orchestration i.e. airflow)
14. real time databricks job
    --> cleanup database and datasets. Task1
    --> convert csv to parquet by applying schema. Task2
    --> create spark sql table on top of parquet files. Task3
    --> claucate daily product revenue from sql  tables and write result to target in parquet format. Task4
    --> validata app (tasks) for ELT pipeline using databricks.
    --> build ELT pipeline using databricks job in workflows.
    --> run and review execution elt pipeline using databricks job.


=======
Secreat Manager: to store, manage and access screats.
use case- passwords, api keys, tls cert, etc..
access DB passwords from the secreat manager by python scripts.

================
GCP cloud logging: store logs in JSON format
logs - contains timestamp, resorce, payload, activity (admin/dataAccess/systenEvent/policyDenied)
export logs to gcs/bq/pubsub topic using sinks (has sa and grant req roles)

=================
pubsub - messaging system for stream analytics (kafca)
use cases - real time data ingestion, iot devices / parallel processing / replicate data among db 

publisher(IOT, App) --> sends message --> | Topic (here msg storage system) --> Subscription | -->
message --> subscriber(BQ)
subscriber ack message from subscription -  pull / push
one to many / many to many / many to one

IOT (publisher) --> pubsub (stream pipeline) --> dataflow (etl / subscriber) --> BQ--> Looker 
                        gcs (batch pipeline) -->

==================

gcloud auth application-default login    
Quota project "sur-cloud-fun" was added to ADC which can be used by Google client libraries for billing and quota.

=================

Ways to interact with GCP:
UI / CLI - Cloud shell (preconfigured cloud SDK) & Terminal/cmd (install cloud sdk) / Client libraries / Rest api

=========

serverless means "pay as you use". No traffic, you pay nothing, 
(like Cloud Run, Cloud Function, AppEngine standard, firestore, datastore, dataproc, dataflow, ai-platform).

managed but not serverless - always have a minimal number of VM/node up and you pay for these traffic or not - 
However, you have nothing to worry about: patching, updates, networking, backups, HA, redundancy(...) are managed for you.
(like Cloud SQL, BigTable or Spanner or AppEngine flex belong to this category).

hybrid product - like Cloud Storage or BigQuery: 
you pay as you use the processing (BigQuery) or the traffic (Cloud Storage)
but the storage is always billed if you have no traffic.

This is for GCP. If you look for other cloud provider, the definition is not the same. 

====================

IAM
Identity: user, groups, SA, domain
Resource
Roles – primitive, predefined, custom
Policy – binding identity with role
SA- default, user-maintained, google-maintained

App on vm – accessing GCS bucket
On-prim - accessing GCS bucket (long-lived)
On-prim – GCP API (short-lived)
Static website - accessing GCS bucket (public access)

Bucket level accesses – IAM (uniform)
Object level accesses – ACL (fine-grained)
Signed url – user (gcp account not required) can read object from GCS using signurl for specified period.

================

GKE:
https://www.okteto.com/blog/kubernetes-basics/

===============

===============

data base concepts:
Fact tables - contain numerical data, 
dimension tables - provide context and background information.

Star schema contains a fact table surrounded by dimension tables.
Snowflake schema is surrounded by dimension table which are in turn surrounded by dimension table

==================

Dataprep for data preparation (cleaning, wrangling) for analysis and ML 
- built by trifacta 3rd party tool – need to share data with trifacta (dis adv)
- serverless
- automatically detects schema and anomalies
 
Raw – prepare data using dataprep – run job on dataflow/trifacta(for small dataset) – dataflow job created – 

================



==================


Could build – builds container image deployed to cloud run – cloudrun see metrics, versions, traffic
Pre-requisites:
Cloudfunctions api / cloud build api (its ci-cd tool to deploy function)/cloud logging api/ cloud pubsub api/ eventarc api

Project IAM admin role – 


================

Cloud composer:
Manages apache airflow env on GCP

===============

Cloudrun:
Container to production in sec

Automate Python script execution on GCP

 
https://github.com/rafaello9472/c4ds/tree/main/Automate%20Python%20script%20execution%20on%20GCP%20











