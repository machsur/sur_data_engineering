========
- problem: big data processing - cant handle by excel , sql db, etc.
- Sol : parallel processing - spark 
        https://spark.apache.org/
- Spark : 
        > dirver node (spark contest) - cluster/resource manager - worker nodes (executor, task, cache).
        > ram in worker nodes has divided into partitions - these partitions can be inside specific data structure RDD (write code in py) / DF (strucre databases).
        > RDD - read only - immutable - fault tolerant using DAG concept - 1. transformation (new rdd - not human readble) 2. action (to read)

- Platforms for pyspark:
        > https://colab.research.google.com/
        > VM vare on desktop version
        > databricks community edition / on cloud
        > dataproc on GCP


why PySpark not python for bigdata? :
python - take sample of data and apply statistics to develop histograms
pyrhon - can process large datasets with multithreads 
problem - when data is big and stays in memory / we cant take entire data on compute and use python on it.
        - data cant stay in one computer and it can be distributed.
=========
Apache spark:
- opensource distributed processing system for big data.
- in-memory caching
- uses optimized query execution for fast queries
- provides api in java, scala, python, r
- supports batch processing, inetractive queries, real-time analytics, ml and graph processing

