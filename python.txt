===================
Python:

>> Interpreter: The programme that runs your code.
    Virtual Environment: A separate "workspace" that uses a Python interpreter but has its own packages and dependencies.

>> setup python, pip
    configure environment variable path - so that execute programme from terminal

>>  
    python -m venv myenv                   --- Create a Virtual Environment
    or
    pip install virtualenv 
    virtualenv myenv                     ----- Create a Virtual Environment
    myenv\Scripts\activate               ---- Activate the Virtual Environment
    deactivate                            ------ Deactivate the Virtual Environment
    pip install pandas
    pip freeze > requirements.txt             ---- download req file from venv
    pip install -r requirements.txt           ---- navigate to file loc and install into venv

>> logging: avoid print - avoid garbage values in prod - helps in debugging, disable/enable logging easily
    DEBUG (10): Example: Variable values during execution.
    INFO (20): Example: Confirmation that a function has started or completed.
    WARNING (30):Example: Deprecated features, nearing disk space limit.
    ERROR (40):Example: Failed database connections.
    CRITICAL (50):Example: System crashes or unavailable resources.
 
>> unique identifiers (UUIDs)

>> requests module

>> try: / except ZeroDivisionError as e: # Handling specific exception (division by zero) /  except Exception as e:
    # Handling any other exceptions / else: # Code to execute if no exception occurred / finally:

>> Types of Config Files 
.ini or .cfg or .properties files - often used with configparser.
JSON files: Lightweight and hierarchical, ideal for structured data.
YAML files: Human-readable and popular for complex configurations.
Environment files (.env): For environment variables, often used in web development for sensitive settings.

>> Modules: re-use, no impact on other modules
datetime module: used to work with dates
math module: to utilize built-in functions
user input: we can ask user to provide input
string format method

>> Handlling errors: 
    compile time error: Syntatical - missing :
    Logical error: wrong output 2+3 = 7
    run time error: not running for specific time (mistake use done by user: 5/0) - as a devloper u have to understand user     prospective 

>>  break / continue / pass

>> if, if else, nested if, if elif else / while loop / for loop / for else

>> List, tuple, set, dictionary, array

>> functions: reuse block of code / 
    Formal arguments:
    Actuval arguments:
    positional / keyworded / variable length: def sum(a, *b): / keyword variable length: def person(name, **data):  /
    Global variable (outside fun) / local variable (inside fun)
    recursion: function calling it self ex: factorial
    anonymous function: x = lambda a: a * a / print(x(5))
    filter: list(filter(lambda a: a%2 == 0, lt))
    map: list(map(lambda a: a * 2, lt))
    reduce: from functools import reduce / sum = reduce(lambda a, b: a + b, lt) /

=================================
NumPY and Pandas:

>>  Array: same data type (1-D) - Simple, basic collections (like lists) for small collections.
    NumPy: N-D - numerical calculations for large datasets.
    Pandas: Data analysis library for handling and manipulating structured data, great for dataframes (tables), series.

>> Data scientist or data analytics - Answer the quetions on large data points

>> why Python Pandas
    Excel - Can not handle large sets of data
    Python - Need to write code to find answers
    Pandas dataframe: its python modules - makes data scientist life easier and effective

>> Series is very similar to a NumPy array. 
    Series can have axis labels. 
    it can hold any arbitrary Python Object.
    Dataframe : combination of series.


Create dataframe:
labels = ['a','b','c']
my_list = [10,20,30]
arr = np.array([10,20,30])
d = {'a':10,'b':20,'c':30}

df = pd.read_csv('Salar2.csv')
df1 = client.query(sql).to_dataframe(create_bqstorage_client=False)
df2 = pd.DataFrame(data = [['sure', 20], ['ses', 25], ['siv', 30]], index = ['a', 'b', 'd'], columns = ['name', 'age']) or
from numpy.random import randn
df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split())

----------

df.head(2)
df.set_index("Id", inplace=True)
df = df.reindex(new_index)
df.drop_duplicates(subset=["EmployeeName"], inplace=True)

df['W'] / df[['W', 'Y']] / df['new'] = df['W'] + df['Y']
df.loc['C'] / df.loc[['C', 'D']] / df.loc['C', 'X'] / df.loc[['C', 'E'], ['X', 'Z']]
type(df) / type(df['BasePay'])
df.drop('new',axis=1,inplace=True)
df[df['W']>0][['Y','X']]
df[(df['W']>0) &| (df['Y'] > 1)]

df.groupby('Company').mean()
df.groupby('Company').describe().transpose()

pd.concat([df1,df2,df3],axis=1)
pd.merge(left, right, how='outer/inner/left/right', on=['key1', 'key2']) - based on keys
left.join(right, how='outer') - based on index

df['col2'].unique()
df['col2'].nunique()
df['col2'].value_counts()
df['col1'].apply(lambda x: x * 2 (or) times2 function)
df['col3'].apply(len)
del df['col1']
df.columns
df.index
df.sort_values(by='col2') #inplace=False by default
df.isnull()
df.pivot_table(values='D',index=['A', 'B'],columns=['C'])

--------------

Spiting:

df[['fn', 'ln']] = df['EmployeeName'].str.split(' ', expand = True)
or
df[['ffn', 'lln']] = df['EmployeeName'].apply(lambda x: pd.Series(str(x).split(' ')))

df.head(2)


---------------

Cleaning Missing values:

df.dropna(axis=0, how='any', thresh=2, subset=['col1', 'col2'], inplace=False)
df.fillna(value=2 or df['col1'].mean() or {'col1': 2, 'col2': 4}, method='ffill', axis=0, inplace=False, limit=2)
df.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=None)

--------------
data type casting:

df["Date"] = pd.to_datetime(df["Date"], format='%d/%m/%Y') or df["Date"] = df["Date"].astype('datetime64[ns]')
df["Id"] = df["Id"].astype(str)
df["Id"] = df["Id"].astype(float)
df["Id"] = df["Id"].astype(int)

----------------

cleaning wrong data:

df.loc[5, 'BasePay'] = 110000

for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.drop(i, inplace = True)
        
for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.loc[i, 'BasePay'] = 10000
        

=======================================

df.duplicated()
df.drop_duplicates(inplace = True)

=======================================

df2.corr()

============================

import matplotlib.pyplot as plt
df2.plot(kind='scatter', x = 'Year', y = 'BasePay')
plt.show()

===========================
data structures (containers storing data in memory) + code instructions = software apply
selecting right data structure for a given problem - makes efficient programme

bigo - measure running time or space rwuirements for programme as input size grows

time complexity (o(1) // log(n) //  o(n) // o(n2) //

================
Python / pyspark /
===============
https://github.com/itversity
https://github.com/dgadiraju?tab=repositories
https://spark.apache.org/

=============


       
=====================
PySpark:

>> read data and process using python - not recommend these low level apis (use for reading schemas/properties)
>> read data and process using python pandas -  recommend for small/medium datasets

>>  Bigdata and datalakes:

-- databases / use cases / tchnologies / volume: 
    RDBMS - order management system, point of sales, transaction involve b/w two parties - oracle, sql server, mysql,       postgres , IBM DB2  - small to medium
    Data lake - low-cost storage (data from all relavant sources)- GCS, S3, ADLS - high
    data warehouse - for reporting purpose - snowflake, teradata, databricks (these 3 available on all cloud), oracle exadata, BQ, Amazon redshift, Azure Synapse - medium (typical) to high
    no sql - operational stroes (chats, products catalog in Amazon, endorsements, recommendations) - Bigtable, Amazon Dynamodb, Azusre(multiple), casandra, mangodb - medium (typical) to high
    graph - network -ne04j - small to medium
    seach -search quickly - elastic search, solr, lucene - medium

    small - few hundred of gb
    medium - under 1 tb
    high - few gundred of tb / pb

-- Big data: 
    characteristics - volume, varity, velocity
    for google search engine, introduced 3 tech GFS, GMR, big table - popular and published white paper in 2000 
    based on white papers, hadoop and then spark came into picture.

-- Google white papers------> Haddop ecosystem------------> Spark

-- HES: tech - HDFS based on GFS, HMR based on GMR, HBASE based on GBT, later introduced Sqoop/dead, flume/dead, hive, oozie - doest use memory efficiently and cloud -so, spark came into picture

    HDFS - large datasets distribute on multiple machines - replicate them for fault tolerance
    map reduce - batch jobs break down - process them across cluster - combine them / HBASE - nosql for real-time access to data / both will provide compute capabilities on top of hdfs
    Sqoop - import and export data from/to structured databases into haddop - underhood it uses map reduce
    Hive - querying and manage large datasets using sql like lang HiveQL - underhood it uses map reduce
    Flume - move large vol of web server log, event data to HDFS or Hbase
    Oozie - workflow scheduling tool for Hadoop jobs - underhood it uses map reduce

    challemges - steep learining curve / setup time and cost / maintanance of cluster / application development life cycle / ineeficient usage of cluster / slow performance during data processing
    
    These challenges in HES (datalake using HES) are overcome by datalake using cloud

//////////////////////////////////////

20. Overview of spark and its architexture

-- data processing: source (files,db, rest payloads) -----> process (sql, libraries/pandas/dask, frameworks/spark) ------->
    target(files,db, rest payloads)

-- data processing libraries: python pandas/limitations whth large datasets - introduces python dask 
(uses server efficiently and multi nodes)  - pyspark works well spark clusers with multiple nodes when compared to dask 

-- distributed computung: ex: hadoop and spark
    single server (etl) - problem is sla in prod and capcity issues even if you use efficiently with multithreading
    cluster (have multiple server)

-- databricks have spark based runtime - AWS (EMR & GLUE), both spark embedded in them - in gcp dataproc, we got hadoop and spark - Azure synaps, spark embedded it - 
    snow flake most popular DWH they introduced snowpark based on spark

-- spark: multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
    key features: batch/streaming data - sql analytics  - datascience at scale - machine learning
    architexture: https://spark.apache.org/docs/latest/cluster-overview.html
    cluster capacity, spark cluster: cluster(D,W) nodes/OS/Spark engine/executors(JavaVM) in worker, vcpu or slots unused capacity, tasks used capacity/network/


=====================
- problem: big data processing - cant handle by excel , sql db, etc.
- Sol : parallel processing - spark 
        https://spark.apache.org/
- Spark : 
        > dirver node (spark contest) - cluster/resource manager - worker nodes (executor, task, cache).
        > ram in worker nodes has divided into partitions - these partitions can be inside specific data structure RDD (write code in py) / DF (strucre databases).
        > RDD - read only - immutable - fault tolerant using DAG concept - 1. transformation (new rdd - not human readble) 2. action (to read)

- Platforms for pyspark:
        > https://colab.research.google.com/
        > VM vare on desktop version
        > databricks community edition / on cloud
        > dataproc on GCP


why PySpark not python for bigdata? :
python - take sample of data and apply statistics to develop histograms
pyrhon - can process large datasets with multithreads 
problem - when data is big and stays in memory / we cant take entire data on compute and use python on it.
        - data cant stay in one computer and it can be distributed.
=========
Apache spark:
- opensource distributed processing system for big data.
- in-memory caching
- uses optimized query execution for fast queries
- provides api in java, scala, python, r
- supports batch processing, inetractive queries, real-time analytics, ml and graph processing
