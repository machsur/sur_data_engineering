================
Python / pyspark /
===============
https://github.com/itversity
https://github.com/dgadiraju?tab=repositories
https://spark.apache.org/

=============

Python:

>> logging: avoid print - avoid garbage values in prod - helps in debugging, disable/enable logging easily
>> configparser: handling configuration files, similar to .ini files. 
>> unique identifiers (UUIDs)
>> requests module



=====================
PySpark:

>> read data and process using python - not recommend these low level apis (use for reading schemas/properties)
>> read data and process using python pandas -  recommend for small/medium datasets

>>  Bigdata and datalakes:

-- databases / use cases / tchnologies / volume: 
    RDBMS - order management system, point of sales, transaction involve b/w two parties - oracle, sql server, mysql,       postgres , IBM DB2  - small to medium
    Data lake - low-cost storage (data from all relavant sources)- GCS, S3, ADLS - high
    data warehouse - for reporting purpose - snowflake, teradata, databricks (these 3 available on all cloud), oracle exadata, BQ, Amazon redshift, Azure Synapse - medium (typical) to high
    no sql - operational stroes (chats, products catalog in Amazon, endorsements, recommendations) - Bigtable, Amazon Dynamodb, Azusre(multiple), casandra, mangodb - medium (typical) to high
    graph - network -ne04j - small to medium
    seach -search quickly - elastic search, solr, lucene - medium

    small - few hundred of gb
    medium - under 1 tb
    high - few gundred of tb / pb

-- Big data: 
    characteristics - volume, varity, velocity
    for google search engine, introduced 3 tech GFS, GMR, big table - popular and published white paper in 2000 
    based on white papers, hadoop and then spark came into picture.

-- Google white papers------> Haddop ecosystem------------> Spark

-- HES: tech - HDFS based on GFS, HMR based on GMR, HBASE based on GBT, later introduced Sqoop/dead, flume/dead, hive, oozie - doest use memory efficiently and cloud -so, spark came into picture

    HDFS - large datasets distribute on multiple machines - replicate them for fault tolerance
    map reduce - batch jobs break down - process them across cluster - combine them / HBASE - nosql for real-time access to data / both will provide compute capabilities on top of hdfs
    Sqoop - import and export data from/to structured databases into haddop - underhood it uses map reduce
    Hive - querying and manage large datasets using sql like lang HiveQL - underhood it uses map reduce
    Flume - move large vol of web server log, event data to HDFS or Hbase
    Oozie - workflow scheduling tool for Hadoop jobs - underhood it uses map reduce

    challemges - steep learining curve / setup time and cost / maintanance of cluster / application development life cycle / ineeficient usage of cluster / slow performance during data processing
    
    These challenges in HES (datalake using HES) are overcome by datalake using cloud

//////////////////////////////////////

20. Overview of spark and its architexture

-- data processing: source (files,db, rest payloads) -----> process (sql, libraries/pandas/dask, frameworks/spark) ------->
    target(files,db, rest payloads)

-- data processing libraries: python pandas/limitations whth large datasets - introduces python dask 
(uses server efficiently and multi nodes)  - pyspark works well spark clusers with multiple nodes when compared to dask 

-- distributed computung: ex: hadoop and spark
    single server (etl) - problem is sla in prod and capcity issues even if you use efficiently with multithreading
    cluster (have multiple server)

-- databricks have spark based runtime - AWS (EMR & GLUE), both spark embedded in them - in gcp dataproc, we got hadoop and spark - Azure synaps, spark embedded it - 
    snow flake most popular DWH they introduced snowpark based on spark

-- spark: multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
    key features: batch/streaming data - sql analytics  - datascience at scale - machine learning
    architexture: https://spark.apache.org/docs/latest/cluster-overview.html
    cluster capacity, spark cluster: cluster(D,W) nodes/OS/Spark engine/executors(JavaVM) in worker, vcpu or slots unused capacity, tasks used capacity/network/


=====================
- problem: big data processing - cant handle by excel , sql db, etc.
- Sol : parallel processing - spark 
        https://spark.apache.org/
- Spark : 
        > dirver node (spark contest) - cluster/resource manager - worker nodes (executor, task, cache).
        > ram in worker nodes has divided into partitions - these partitions can be inside specific data structure RDD (write code in py) / DF (strucre databases).
        > RDD - read only - immutable - fault tolerant using DAG concept - 1. transformation (new rdd - not human readble) 2. action (to read)

- Platforms for pyspark:
        > https://colab.research.google.com/
        > VM vare on desktop version
        > databricks community edition / on cloud
        > dataproc on GCP


why PySpark not python for bigdata? :
python - take sample of data and apply statistics to develop histograms
pyrhon - can process large datasets with multithreads 
problem - when data is big and stays in memory / we cant take entire data on compute and use python on it.
        - data cant stay in one computer and it can be distributed.
=========
Apache spark:
- opensource distributed processing system for big data.
- in-memory caching
- uses optimized query execution for fast queries
- provides api in java, scala, python, r
- supports batch processing, inetractive queries, real-time analytics, ml and graph processing
