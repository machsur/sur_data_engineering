===============
https://github.com/itversity
https://github.com/dgadiraju?tab=repositories
https://spark.apache.org/

=============

>> dealing with files: open file and read (low level api not recommended, use for properties/schemas), pandas, pyspark
>> lambda, list comprehension, filter, map/set, sorted

>> get column names(metadata) from schemas.json

>> read data and process using python - not recommend these low level apis

>> read data and process using python pandas -  recommend for small/medium datasets

>> To improve the efficiency of our data engineering pipelines, we need to convert these CSV files into JSON files, since JSON is better to use in downstream applications than CSV files. The scope of this project involves converting CSV files into JSON files.
read files using glob - get column names from schemas - read csv files into df - generate file paths for json target files -- write into json files from df 

=======================
19. Bigdata and datalakes:

-- databases / use cases / tchnologies / volume: 
    RDBMS - order management system, point of sales, transaction involve b/w two parties - oracle, sql server, mysql,       postgres , IBM DB2  - small to medium
    Data lake - low-cost storage (data from all relavant sources)- GCS, S3, ADLS - high
    data warehouse - for reporting purpose - snowflake, teradata, databricks (these 3 available on all cloud), oracle exadata, BQ, Amazon redshift, Azure Synapse - medium (typical) to high
    no sql - operational stroes (chats, products catalog in Amazon, endorsements, recommendations) - Bigtable, Amazon Dynamodb, Azusre(multiple), casandra, mangodb - medium (typical) to high
    graph - network -ne04j - small to medium
    seach -search quickly - elastic search, solr, lucene - medium

    small - few hundred of gb
    medium - under 1 tb
    high - few gundred of tb / pb

-- Big data: 
    characteristics - volume, varity, velocity
    for google search engine, introduced 3 tech GFS, GMR, big table - popular and published white paper in 2000 
    based on white papers, hadoop and then spark came into picture.

-- Google white papers------> Haddop ecosystem------------> Spark

-- HES: tech - HDFS based on GFS, HMR based on GMR, HBASE based on GBT, later introduced Sqoop/dead, flume/dead, hive, oozie - doest use memory efficiently and cloud -so, spark came into picture

    HDFS - large datasets distribute on multiple machines - replicate them for fault tolerance
    map reduce - batch jobs break down - process them across cluster - combine them / HBASE - nosql for real-time access to data / both will provide compute capabilities on top of hdfs
    Sqoop - import and export data from/to structured databases into haddop - underhood it uses map reduce
    Hive - querying and manage large datasets using sql like lang HiveQL - underhood it uses map reduce
    Flume - move large vol of web server log, event data to HDFS or Hbase
    Oozie - workflow scheduling tool for Hadoop jobs - underhood it uses map reduce

    challemges - steep learining curve / setup time and cost / maintanance of cluster / application development life cycle / ineeficient usage of cluster / slow performance during data processing
    
    These challenges in HES (datalake using HES) are overcome by datalake using cloud

//////////////////////////////////////

20. Overview of spark and its architexture

-- data processing: source (files,db, rest payloads) -----> process (sql, libraries/pandas/dask, frameworks/spark) ------->
    target(files,db, rest payloads)

-- data processing libraries: python pandas/limitations whth large datasets - introduces python dask 
(uses server efficiently and multi nodes)  - pyspark works well spark clusers with multiple nodes when compared to dask 

-- distributed computung: ex: hadoop and spark
    single server (etl) - problem is sla in prod and capcity issues even if you use efficiently with multithreading
    cluster (have multiple server)

-- databricks have spark based runtime - AWS (EMR & GLUE), both spark embedded in them - in gcp dataproc, we got hadoop and spark - Azure synaps, spark embedded it - 
    snow flake most popular DWH they introduced snowpark based on spark

-- spark: multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
    key features: batch/streaming data - sql analytics  - datascience at scale - machine learning
    architexture: https://spark.apache.org/docs/latest/cluster-overview.html
    cluster capacity, spark cluster: cluster(D,W) nodes/OS/Spark engine/executors/network/

-- 





===========================

class: is design - has attributes, methods
object: instance from class
self: referring to the objects
init - special method - to initialize variables - called init mtd automatically when you instantiate object (we can say construcor)

========================

variables: instance variable (within init mtd), class variable
mtd: instance mtd, class mtd, static mtd

class Student:
    college = 'narayana'  # class variable
    def __init__(self, m1, m2): # instance variable
        self.m1 = m1
        self.m2 = m2
    def avg(self):   # instance method
        return (self.m1 + self.m2)/2
    @classmethod   
    def get_school(cls):   # class method
        return cls.college
    @staticmethod    # static method
    def fact():
        print('this returns factorial of number')


s1 = Student(20, 40)
print(s1.college)
print(Student.college)
print(s1.m1)
print(s1.avg())
print(s1.get_school())
s1.fact()

===================

logging: avoid print - its just for testing - not recommended in prod to display these garbage values
helps in debugging, disable/enable logging easily

import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
f = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
fh = logging.FileHandler('debug.log')
fh.setFormatter(f)
logger.addHandler(fh)

# logging.basicConfig(filename='debug.log', filemode='w', level=logging.INFO, 
# format='%(asctime)s - %(levelname)s - %(message)s')

# logging.disable()
# levels - DEBUG, INFO, WARNING, ERROR, CRITICAL

logger.debug('this is debug')
logger.info('this is info')
logger.warning('this is warning')
logger.error('this is error')
logger.critical('this is critical')

# logging.error('this name is not valid')

=======================

>>  configparser is a Python module that provides functionality for handling configuration files, similar to .ini files. 

>> These files typically contain sections, each with key-value pairs.
[Section1]
key1 = value1
key2 = value2

>> 
import configparser
# Create the config parser object
config = configparser.ConfigParser()
# Read from the config file
config.read(path+'config.ini')
# Access a value
config.get('Section1', 'key2')

=====================

In Python, the uuid module provides functions to generate universally unique identifiers (UUIDs). 

=======================


