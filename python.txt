=============

>> dealing with files: open file and read (low level api not recommended, use for properties/schemas), pandas, pyspark
>> lambda, list comprehension, filter, map/set, sorted

>> get column names(metadata) from schemas.json

>> read data and process using python - not recommend these low level apis

>> read data and process using python pandas -  recommend for small/medium datasets

>> To improve the efficiency of our data engineering pipelines, we need to convert these CSV files into JSON files, since JSON is better to use in downstream applications than CSV files. The scope of this project involves converting CSV files into JSON files.
read files using glob - get column names from schemas - read csv files into df - generate file paths for json target files -- write into json files from df 

=======================
Bigdata and datalakes:

>> databases / use cases / tchnologies / volume: 
RDBMS - order management system, point of sales, transaction involve b/w two parties - oracle, sql server, mysql, postgres, IBM DB2  - small to medium
Data lake - low-cost storage (data from all relavant sources)- GCS, S3, ADLS - high
data warehouse - for reporting purpose - snowflake, teradata, databricks (these 3 available on all cloud), oracle exadata, BQ, Amazon redshift, Azure Synapse - medium (typical) to high
no sql - operational stroes (chats, products catalog in Amazon, endorsements, recommendations) - Bigtable, Amazon Dynamodb, Azusre(multiple), casandra, mangodb - medium (typical) to high
graph - network -ne04j - small to medium
seach -search quickly - elastic search, solr, lucene - medium

small - few hundred of gb
medium - under 1 tb
high - few gundred of tb / pb

>> Big data: characteristics - volume, varity, velocity - for google search engine, introduced 3 tech GFS, GMR, big table - popular and published white paper in 2000 - then big data tech evolved - based on white papers, hadoop and then spark came into picture.
Google white papers------> Haddop ecosystem------------> Spark
HES: tech - HDFS based on GFS, HMR based on GMR, HBASE based on GBT, later introduced Sqoop/dead, flume/dead, hive, oozie - doest use memory efficiently and cloud -so, spark came into picture

>> HDFS - large datasets distribute on multiple machines - replicate them for fault tolerance
-- map reduce - batch jobs break down - process them across cluster - combine them / HBASE - nosql for real-time access to data / both will provide compute capabilities on top of hdfs
-- Sqoop - import and export data from/to structured databases into haddop - underhood it uses map reduce
-- Hive - querying and manage large datasets using sql like lang HiveQL - underhood it uses map reduce
-- Flume - move large vol of web server log, event data to HDFS or Hbase
-- Oozie - workflow scheduling tool for Hadoop jobs - underhood it uses map reduce

-- challemges - steep learining curve / setup time and cost / maintanance of cluster / application development life cycle / ineeficient usage of cluster / 
slow performance during data processing

>> challenges in HES (datalake using HES) are overcome by datalake using cloud



===========================

class: is design - has attributes, methods
object: instance from class
self: referring to the objects
init - special method - to initialize variables - called init mtd automatically when you instantiate object (we can say construcor)

========================

variables: instance variable (within init mtd), class variable
mtd: instance mtd, class mtd, static mtd

class Student:
    college = 'narayana'  # class variable
    def __init__(self, m1, m2): # instance variable
        self.m1 = m1
        self.m2 = m2
    def avg(self):   # instance method
        return (self.m1 + self.m2)/2
    @classmethod   
    def get_school(cls):   # class method
        return cls.college
    @staticmethod    # static method
    def fact():
        print('this returns factorial of number')


s1 = Student(20, 40)
print(s1.college)
print(Student.college)
print(s1.m1)
print(s1.avg())
print(s1.get_school())
s1.fact()

===================

logging: avoid print - its just for testing - not recommended in prod to display these garbage values
helps in debugging, disable/enable logging easily

import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
f = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
fh = logging.FileHandler('debug.log')
fh.setFormatter(f)
logger.addHandler(fh)

# logging.basicConfig(filename='debug.log', filemode='w', level=logging.INFO, 
# format='%(asctime)s - %(levelname)s - %(message)s')

# logging.disable()
# levels - DEBUG, INFO, WARNING, ERROR, CRITICAL

logger.debug('this is debug')
logger.info('this is info')
logger.warning('this is warning')
logger.error('this is error')
logger.critical('this is critical')

# logging.error('this name is not valid')

=======================

>>  configparser is a Python module that provides functionality for handling configuration files, similar to .ini files. 

>> These files typically contain sections, each with key-value pairs.
[Section1]
key1 = value1
key2 = value2

>> 
import configparser
# Create the config parser object
config = configparser.ConfigParser()
# Read from the config file
config.read(path+'config.ini')
# Access a value
config.get('Section1', 'key2')

=====================

In Python, the uuid module provides functions to generate universally unique identifiers (UUIDs). 

=======================


