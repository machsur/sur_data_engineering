=============================================================================================================================================

>> Bigquery arch: storage and compute - decoupled/isolated (cost efficient)
   colossus - distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus;    borg - scheduling jobs like run queries
>> Fact Table: Contains numeric values (quantitative data - sales, revenue, profit, etc), foreign keys, Grows rapidly
   Dimension Table: contains textual or categorical data, smaller and changes less frequently.
>> Star Schema: Multiple Dimension Tables directly connected to the fact table
   Snowflake Schema: Dimension tables are split into sub-dimensions, reducing redundancy, more normalized form. 
>> Change Data Capture -->> identify & capture changes made to data in DB --> log based - very efficient, query based on timestamp, triggers load changes into new tables
>> SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
   SCD1 - Overwrite - Updates the existing record with new data - No history is preserved.
   SCD2 - Add New Row  -- Keeps full history by adding a new row with versioning or effective dates.
   SCD3 - Add New Column -- Keeps limited history by adding a new column for the previous value | SCD4 - scd1+scd2 - Current Table & History Table
>> to get expensive queries, metadata about datasets:  SELECT * FROM `project_id`.`region-europe-west2`.INFORMATION_SCHEMA..JOBS/SCHEMATA;
   to get metadata about tables/columns: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.TABLES/.COLUMNS;
>> difference b/w CAST, SAFE_CAST : If the conversion fails, SAFE_CAST returns NULL instead of throwing an error, allowing the query to continue.
>> Union all / Union distinct  / Except distinct / Intersect distinct / Table wildcards: test_*` where _Table_Suffix > 5
>> Temporary table - intermediate data proce and caching | Internal table- freq accessed and processed data | External table - for occasional queries on large datasets.
>> Time travel concept: SELECT * FROM `your_dataset.your_table`	FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)
>> view, materialized view and auth view (share results without giving access to source data): efficiently manage and secure your data access in BigQuery
>> Partitioning (single column, 4000 max) and clustering (sorts parti data in storage blocks, four col): Improved Query Performance, Cost Efficiency
>> Joins: - INNER JOIN / left join / right join / full join / cross join /
>> Pivot: row into columns 	- product| month| sales  	- select * from table pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar')); --> or use case statement
   unpivot: columns into rows 	- product | Jan | Feb | Mar	- select product, month, sales from table unpivot(sales for month in (Jan, Feb, Mar));
>>  Normalization – avoid redundant data, poor query performance |  De Normalization – redundant data (high storage cost) – better Query performance 
    Why array & structs? – for better query per and lesser storage costs
    Arrays – list of items having same data type  -  SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element; 
    Struct – record with nested fields having different data types  -   SELECT person.id, person.name FROM mydataset.mytable;
>> bq slots: units of computational capacity -CPU and memory, Reservations: allow you to purchase dedicated slots (called commitments) 
>> Big Query cost optimization
   bq compute optimization: on-demaned pricing, Capacity Pricing (slots reserve)
   BQ data storage: Billing model: logical/physical, Use Table Expiration, Use Long-Term Storage Pricing (> 90 days)
   query Opti: Agg at Source,  use avro, dataset at cust loc, Parti & Clust, preview, select col, use trun, Join, where, late agg, Caching, mat view for freq queries
>> Cost Controls: budget alerts, Set query usage limits, export Billing reports, Schedule queries during off-peak hours
 
=============================================================================================================================================
>>
DATE(DC in iso) / PARSE_DATE(format, DC) / SAFE.PARSE_DATE(format, DC)	-	Y, y / m, B, b / d, A, a / 
FORMAT_DATE('%B %d, %Y', sale_date)
extract(year from date)      						-     	year / month / date / hour / minute / second / microsecond /
DATE_TRUNC(order_date, MONTH) 
DATE_DIFF(order_date, previous_date, day) 
Date_add(order_date, INTERVAL 5 day) / Date_sub(order_date, INTERVAL 5 day)

>> FROM / WHERE / Filters / GROUP BY / HAVING / WINDOW functions like ROW_NUMBER() / SELECT / DISTINCT / ORDER BY / LIMIT or OFFSET

>> Window functions: performing advanced calculations over a set of rows, go beyond simple aggregates or filters
   SELECT *, SUM(sales) OVER (PARTITION BY id ORDER BY updated_at ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rolling_sum FROM my_table
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /  first_value(cn) / last_value(cn) / nth_value(cn, n) /
   row_number() / rank() / dense_rank() 
   lag(cn) / lead(cn) / 
   ntile(n) – n groups 
   CUME_DIST() = Number of rows with values less than or equal to the current row / Total number of rows /   PERCENT_RANK() = Rank of current row−1 / Total rows−1
>> SPLIT(REPLACE(name, '-', ' '), ' ')[OFFSET(0)] 
>> CASE WHEN MOD(number, 2) = 0 THEN number ELSE NULL END AS Even,
>> SELECT customer_id, SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales, .... FROM orders GROUP BY customer_id;
   select * from (select customer_id, region, sales_amount from orders) pivot(sum(sales_amount) for region in ('north', 'south', 'east', 'west'));
>> WITH CustomOrder AS (SELECT emp_name, emp_dep, CASE WHEN emp_dep = 'it' THEN 1 WHEN emp_dep = 'hr' THEN 2 WHEN emp_dep = 'sales' THEN 3 end as emp_dept_sort,     
   ROW_NUMBER() OVER(PARTITION BY emp_dep ORDER BY emp_name) AS row_num  FROM table) | SELECT * FROM CustomOrder ORDER BY emp_dept_sort, row_num;
>> Src, beforeTgt, tgt after src ing: MERGE INTO target_table AS T  USING source_table AS S  ON T.id = S.id	
   WHEN MATCHED THEN UPDATE SET T.name = S.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);













