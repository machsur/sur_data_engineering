===================
>> Gcs bucket: name, region, storgae class, access control (public, uniform, fine-grained), data protection (object versining, retantion) , data encryption
BIGQUERY:

>> Bigquery arch
   storage and compute - decoupled/isolated (cost efficient)
   layers:
   colossus - is google distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus
   borg - scheduling jobs like run queries

>> Sql query execution order: From / where / group by / having / select / distinct / order by / limit
select 5 /distict 6 / from table 1 / where 2 / group by 3 / having 4/ order by 7 / limit 8

>> conversion functions:
   The CAST function performs a conversion between compatible data types. 
   If the conversion fails due to data type incompatibility or data loss, the query will fail
   The SAFE_CAST function also performs a conversion between compatible data types, 
   but it handles conversion errors differently. Instead of failing the query, it returns NULL when the conversion cannot    be performed.

>> commenting: -- or # or /* LINES */

>> Create table with metadata from existing table – use where 1 = 2 or false

>> .
   SELECT * EXCEPT(column1, column2, ...) FROM table_name; 
   SELECT DISTINCT department FROM employees limit 10;
   SELECT ROUND(56.14159, -2/-1); / SELECT ROUND(56.14159, 2); /

>> String functions:
concat, length, repeat, reverse, replace, upper, lower, ltrim, rtrim, trim, left, right, substr, starts_with, ends_with, contains_substr, strpos, instr
CONCAT('#', LTRIM('   apple   '), '#') / LENGTH('suresh') / SPLIT('apple,banana,orange', ',') / REPEAT('abc', 3) / REVERSE('abc') / REPLACE ('desert pie', 'pie', 'cobbler') 
LEFT('banana', 3) / RIGHT('apple', 3) / SUBSTR('apple', 2, 2) / SUBSTRING alias for SUBSTR /
LOWER('FOO BAR BAZ') / UPPER('foo')
LTRIM('   apple   ') / RTRIM('***apple***', '*') / TRIM( '   apple   ') /
STARTS_WITH('bar', 'b') / ENDS_WITH('apple', 'e') / CONTAINS_SUBSTR('the blue house', CONCAT('Blue ', 'house')) / 
INSTR('banana', 'an', 1, 1) or INSTR(string, substring, start_position, occurrence) / STRPOS('foo@example.com', '@') 

>> Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

>>   Permanent table: save query results on BQ storage
   Temporary table/cache – Query results saved on cache and valid for 24 hrs.
   Internal/native table/managed tables: table in BQ storage
   External table: BQ querying data from bigtable, gcs, google drive

>> Time travel concept: Query the state of the table as it was at a specific timestamp
      SELECT * FROM `your_dataset.your_table`
      FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)

>> Creation of view/materialized view and auth view: efficiently manage and secure your data access in BigQuery
   Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data       access / 
   Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
   Authorized views and authorized materialized views: let you share query results with particular users and groups without    giving them access to the underlying source data.
         CREATE VIEW `your_dataset.your_view` AS
         SELECT
           customer_id,
           order_date,
           total_amount
         FROM
           `your_dataset.orders`
         WHERE
           order_date >= '2025-01-01'

            CREATE MATERIALIZED VIEW `your_dataset.your_materialized_view` AS
            SELECT
              customer_id,
              order_date,
              SUM(total_amount) AS total_spent
            FROM
              `your_dataset.orders`
            GROUP BY
              customer_id, order_date

>> Partitioning and clustering:
   Improved Query Performance: By scanning only relevant partitions and clustered blocks, queries run faster.
   Cost Efficiency: Reduced data scanned means lower costs, as BigQuery charges based on the amount of data processed.
      CREATE TABLE `your_dataset.your_table`
      PARTITION BY DATE(order_date)
      CLUSTER BY customer_id, product_id
      AS
      SELECT * FROM `source_dataset.source_table`

>> Joins: -  combine rows from two or more tables based on a related column between them.
   INNER JOIN / left join / right join / full join / cross join / self join /
   Best Practices for Joins in BigQuery: Use Appropriate Join Types: / Filter Early / Avoid Cross Joins When Possible / 
   Use Partitioned and Clustered Tables: If joining large tables / Monitor Query Performance /

>> With statement: also known as Common Table Expressions (CTEs), allows you to define temporary result sets.
   Adv: code more readable, resusebility - referenced in several places within the main SQL query. 

>> Conditions:
If condition
then
sql statement;
end if;

for var in list
do 
sql statement;
end for;

/ labels /repeat / while /leave/ break/ continue/ iterate 

>> Pivoting and unpivoting: help in reshaping your data for analysis or reporting
   Pivot: row into columns
   unpivot: columns into rows

>> Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
   De Normalization – redundant data (high storage cost) – better Query performance 
   Why array & structs? – for better query per and lesser storage costs
   
   Arrays – list of items having same data type
   SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element;

   Struct – record with nested fields having different data types
   SELECT person.id, person.name FROM mydataset.mytable;
 
>> Window functions: performing advanced calculations over a set of rows, go beyond simple aggregates or filters
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /row_number() / rank() / dense_rank() 
   SUM(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rt
   lag(cn) / lead(cn): Accesses data from a previous or subsequent row in the same result set without use of a self-join.
   LAG(column2, 1) OVER (PARTITION BY column1 ORDER BY column2) AS previous_value,
   first_value(cn) / last_value(cn) / nth_value(cn, n) 
   FIRST_VALUE(column2) OVER (PARTITION BY column1 ORDER BY column2) AS first_value,
   ntile(n) – n groups 
   NTILE(4) OVER (PARTITION BY column1 ORDER BY column2) AS quartile
   cum_dist() / percent_rank() 
   CUME_DIST= Number of rows with values less than or equal to the current row / Total number of rows
​   PERCENT_RANK= Rank of current row−1 / Total rows−1
​   Rows consider current row even if duplicates / range considers bottom row of duplicates.

>> Regular expressions: used for pattern matching and string manipulation
   REGEXP_CONTAINS(column_name, r'pattern')
   REGEXP_EXTRACT_ALL(column_name, r'pattern')
   REGEXP_REPLACE(column_name, r'pattern', 'replacement')
   SPLIT(column_name, r'pattern') --> select  SPLIT("mach suresh", ' ')[OFFSET(0)] AS first_name,

>> Big Query cost optimization
   -- bq compute optimization: on-demaned analysis, bq editions - decide based on your workloads.
   
   -- BQ data storage – 
   Billing model: logical- data in uncompressed format/ physical- data in compressed format) 
         # For logical bytes billing
         bq update --dataset --storage_billing_model=LOGICAL my_dataset
         # For physical bytes billing
         bq update --dataset --storage_billing_model=PHYSICAL my_dataset
   Use Table Expiration Settings 
   Optimize Data Storage Format (cloumner format - paqruet. ORC)
   Use Long-Term Storage Pricing (> 90 days)

   -- Query optimization 
   create dataset in region wehere customer operates
   Use Partitioned Tables and Clustered Tables
   use avro format to bigquery data load - google rec compressed avro for quick data load
   use preview option for bq native table
   select columns instead of *
   use truncate instead of delete
   nest repeated data, Denormalize Data Where Appropriate (Use ARRAY Functions for Multi-Value Fields)
   Join pattern larger table join smaller tables, Optimize JOIN Operations, avoid cross join
   where class first condition in such way that eliminates most data
   late aggregation
   Use WITH Clauses for Subquery Caching (Avoid Repeated Scalar Subqueries)
   Take Advantage of Query Caching
   Aggregate Data at the Source: Use Approximate Aggregations: 
   Create a materialized view based on frequently run queries.
   Optimize Window Functions
   Monitor and Tune Query Performance
  
   -- Cost Controls: 
   Set quotas and budget alerts
   set query limits
   Analyze Cost Breakdown Using Cloud Billing Reports
   Schedule queries during off-peak hours to take advantage of low on-demand pricing typically at mid-night/early morning

====================================================================================================================================================

SQL QA:

>> To find the department names where no employees are found, you can use the following SQL query:
   emp -->> employee_id	employee_name	dept_id
   Department Table -->> dept_id	dept_name
   SELECT d.dept_name FROM dept d LEFT JOIN employee e ON d.dept_id = e.dept_id WHERE e.dept_id IS NULL;

>> Given two tables, find the count of records for Left Outer Join and Inner Join:
Table A: Table B:
1 1
1 1
1 1 
1 
Ans. 12 & 12

>> Give the output for DENSE_RANK() and RANK() functions for the below dataset:
Nums 
85 
85 
80 
75 
75 
70

>> Given a table with column 'Country', select data in the below sequence:
Table: Matches
Country 
India 
Australia 
Pakistan 
Output:
India vs Australia 
India vs Pakistan 
Australia vs Pakistan
Ans. 
    SELECT a.countryc || ' vs ' || b.countryc as country
    FROM
      `powerful-layout-445408-p5.sur_test_ds.countryt` a
    join
      `powerful-layout-445408-p5.sur_test_ds.countryt` b
    on a.countryc > b.countryc;  -----> or <

>> Given two tables, output the result of INNER, LEFT, RIGHT, FULL JOINS.
Table1:
col1 
---- 
1 
1 
Table2:
---- 
b 
a 
1 

>> Find the 777th highest salary from a table.
      SELECT salary
      FROM (
        SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num
        FROM `your_dataset.your_table`
      )
      WHERE row_num = 777

>> Identify customers who placed orders in consecutive months.
         WITH orders AS (
           SELECT
             customer_id,
             DATE_TRUNC(order_date, MONTH) AS order_month
           FROM
             `your_dataset.your_table`
         ),
         consecutive_orders AS (
           SELECT
             customer_id,
             order_month,
             LEAD(order_month) OVER (PARTITION BY customer_id ORDER BY order_month) AS next_order_month
           FROM
             orders
         )
         SELECT
           customer_id,
           order_month,
           next_order_month
         FROM
           consecutive_orders
         WHERE
           DATE_DIFF(next_order_month, order_month, MONTH) = 1

>> Query to get the total number of patients per doctor, including unassigned patients.
         SELECT
           d.doctor_id,
           COUNT(p.patient_id) AS total_patients
         FROM
           `your_dataset.doctors` d
         LEFT JOIN
           `your_dataset.patients` p
         ON
           d.doctor_id = p.doctor_id
         GROUP BY
           d.doctor_id

>> Handling NULL values in employee salary using the average salary.
    IFNULL(expression, replacement_value) -->> It takes two arguments. If the first argument is NULL, it returns the second argument.
    COALESCE(expression1, expression2, ..., expressionN) -->> It can take multiple arguments. It returns the first non-NULL argument from the list.
         WITH avg_salary AS (
           SELECT AVG(salary) AS average_salary
           FROM `your_dataset.your_table`
         )
         SELECT
           employee_id,
           COALESCE(salary, (SELECT average_salary FROM avg_salary)) AS adjusted_salary
         FROM
           `your_dataset.your_table`

>> Difference between Subquery and Materialized Views.
    Performance: Materialized views can improve query performance by storing precomputed results, while subqueries are recalculated each time the main query runs.
    Storage: Materialized views consume storage space to store the precomputed results, whereas subqueries do not.
    Maintenance: Materialized views need to be refreshed to stay up-to-date with the underlying data, while subqueries always reflect the current state of the data.

>> CTE vs Subquery in SQL and their performance impact.
    Readability and Maintainability:
    Optimization: Subquery Treated similarly to CTEs in terms of optimization. The query optimizer will attempt to optimize the entire query, including the subqueries.
    Reusability:

>> Steps to debug a slow SQL query.

>> Change Data Capture (CDC) -->> identify and capture changes made to data in a database --> It helps keep track of inserts, updates, and deletes in real-time.
      Benefits:
      Real-time updates: Ensures data consistency across systems.
      Efficiency: Reduces the need for full data scans.
      Accuracy: Minimizes errors by capturing changes as they happen.
      how to implement:
      log based --> very efficient, no impact on source system
      query based on timestamp
      triggers load changes into new tables

>>
Src					
id	name			
1	a								
2	b1							
4	d								
					
beforeTgt					
id	name
1	a
2	b
3	c

after source ingestion Tgt	
id	name
1	a
2	b1
3	c
4	d


MERGE INTO target_table AS T
USING source_table AS S
ON T.id = S.id
WHEN MATCHED THEN
    UPDATE SET T.name = S.name
WHEN NOT MATCHED THEN
    INSERT (id, name) VALUES (S.id, S.name);

>> 
ORDER_DAY         ORDER_ID            PRODUCT_ID   QUANTITY     PRICE
------------------ -------------------- ---------- ---------- ----------
01-JUL-11         O1                  P1                 5          5
01-JUL-11         O2                  P2                 2         10
01-JUL-11         O3                  P3                10         25
01-JUL-11         O4                  P1                20          5
02-JUL-11         O5                  P3                 5         25
02-JUL-11         O6                  P4                 6         20
02-JUL-11         O7                  P1                 2          5
02-JUL-11         O8                  P5                 1         50
02-JUL-11         O9                  P6                 2         50
02-JUL-11         O10                 P2                 4         10
02-JUL-11         O11                 P6                 2         50

Get me all products that got sold both the days and the number of times the product is sold.  


Desired output
PRODUCT_ID  COUNT
P1            3
P2            2
P3            2

SELECT PRODUCT_ID, COUNT(*) AS COUNT
FROM orders
WHERE ORDER_DAY IN ('01-JUL-11', '02-JUL-11')
GROUP BY PRODUCT_ID
HAVING COUNT(DISTINCT ORDER_DAY) = 2;

>> 
INPUT
order_id	customer_id	region	order_date	sales_amount
1	101	North	1/1/2022	100
4	101	North	1/15/2022	400
8	101	EAST	2/5/2022	700
2	102	South	1/5/2022	200
6	102	South	1/25/2022	600
10	102	WEST	2/15/2022	1000
3	103	East	1/10/2022	300
7	103	East	2/1/2022	200
5	104	West	1/20/2022	500
9	104	South	2/10/2022	200
11	104	South	2/10/2022	100


OUTPUT
customer_id	north_sales	south_sales	East sales	West Sales
101	500	0	700	0
102	0	800	0	1000
103	0	0	500	0
104	0	300	0	500


SELECT 
  customer_id,
  SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales,
  SUM(CASE WHEN region = 'South' THEN sales_amount ELSE 0 END) AS south_sales,
  SUM(CASE WHEN region = 'East' THEN sales_amount ELSE 0 END) AS east_sales,
  SUM(CASE WHEN region = 'West' THEN sales_amount ELSE 0 END) AS west_sales
FROM orders
GROUP BY customer_id;

>> Null doesnt include matching, empty string includes matching
table1
1
2
3
null
""

table2
2
2
3
null
null
""

inner join  - 2 2 / 2 2 / 3 3 / empty empty / 
left join - 1 null / 2 2 / 2 2/ 3 3/ empty empty / null null /

>> Find the second highest salary without using LIMIT, OFFSET, or TOP.
	WITH RankedSalaries AS (
	  SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank FROM (SELECT DISTINCT salary FROM employees))
	SELECT salary FROM RankedSalaries WHERE rank = 2
	
	SELECT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 1;

>> Given a table of orders, write a query to find the running total (cumulative sum) of revenue for each day.
	SELECT  order_date, revenue, SUM(revenue) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM  orders

>> Write an SQL query to identify employees who earn more than their managers.
	SELECT  e.employee_id,  e.name AS employee_name,  e.salary AS employee_salary,  m.name AS manager_name,  m.salary AS manager_salary 
	FROM  employees e JOIN  employees m ON e.manager_id = m.employee_id WHERE  e.salary > m.salary
	ORDER BY  e.salary DESC;

>> Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount.
	use rank here

>> Identify consecutive login streaks for users where they logged in for at least three consecutive days.
	WITH login_data AS (
	  SELECT user_id, login_date,
	         LAG(login_date, 1) OVER (PARTITION BY user_id ORDER BY login_date) AS prev_login_date,
	         LAG(login_date, 2) OVER (PARTITION BY user_id ORDER BY login_date) AS prev_prev_login_date
	  FROM `your_dataset.your_table`
	),
	streaks AS (
	  SELECT user_id, login_date,
	         CASE
	           WHEN DATE_DIFF(login_date, prev_login_date, DAY) = 1 AND DATE_DIFF(prev_login_date, prev_prev_login_date, DAY) = 1 THEN 1
	           ELSE 0
	         END AS is_streak
	  FROM login_data
	)
	SELECT user_id, login_date
	FROM streaks
	WHERE is_streak = 1
	ORDER BY user_id, login_date;

>> Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.
	WITH ranked_records AS (
	  SELECT *,
	         ROW_NUMBER() OVER (PARTITION BY column1, column2, column3 ORDER BY column1) AS row_num
	  FROM `your_dataset.your_table`
	)
	DELETE FROM `your_dataset.your_table`
	WHERE row_num > 1;

>> You have a table with millions of records. How would you optimize a slow-performing query that involves multiple joins and aggregations?

>> Retrieve the first order for each customer, ensuring that ties (customers with multiple first orders on the same date) are handled correctly.
	WITH ranked_orders AS (
	  SELECT customer_id, order_id, order_date,
	         ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date, order_id) AS row_num
	  FROM `your_dataset.orders`
	)
	SELECT customer_id, order_id, order_date
	FROM ranked_orders
	WHERE row_num = 1;

>> Find products that were never purchased by any customer.
	SELECT p.product_id, p.product_name
	FROM `your_dataset.products` p
	LEFT JOIN `your_dataset.orders` o
	ON p.product_id = o.product_id
	WHERE o.product_id IS NULL;

>> Given a table with customer transactions, find customers who made transactions in every month of the year.
	WITH monthly_transactions AS (
	  SELECT customer_id, EXTRACT(MONTH FROM transaction_date) AS month
	  FROM `your_dataset.transactions`
	  GROUP BY customer_id, month
	),
	customer_months AS (
	  SELECT customer_id, COUNT(month) AS months_count
	  FROM monthly_transactions
	  GROUP BY customer_id
	)
	SELECT customer_id
	FROM customer_months
	WHERE months_count = 12;

>> Find employees who have the same salary as another employee in the same department.
	WITH EmployeeSalaries AS (
	  SELECT   department_id, salary, COUNT(*) AS employee_count  FROM employees GROUP BY department_id, salary HAVING COUNT(*) > 1)
	SELECT
	  e.employee_id,
	  e.department_id,
	  e.salary
	FROM
	  employees e
	JOIN
	  EmployeeSalaries es
	ON
	  e.department_id = es.department_id
	  AND e.salary = es.salary
	ORDER BY
	  e.department_id, e.salary;
12/ Write an SQL query to retrieve the department with the highest total salary paid to employees.
13/ Use a window function to rank orders based on order value for each customer, and return only the top 3 orders per customer.
14/ Find the median salary of employees using SQL (without using built-in median functions).
15/ Write a query to pivot a table where each row represents a sales transaction and transform it into a summary format where each column represents a month.
16/ Find the moving average of sales for the last 7 days for each product in a sales table.
17/ Given an events table, find the first and last occurrence of each event per user.
>> Identify users who have placed an order in two consecutive months but not in the third month.

>> Find the most frequently purchased product category by each user over the past year.
	WITH user_category_sales AS (
	  SELECT user_id,  product_category, COUNT(*) AS purchase_count FROM sales_table
	  WHERE sale_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) GROUP BY user_id, product_category),
	ranked_categories AS (
	  SELECT  user_id, product_category, purchase_count, RANK() OVER (PARTITION BY user_id ORDER BY purchase_count DESC) AS rank FROM user_category_sales)
	SELECT user_id, product_category, purchase_count FROM ranked_categories WHERE rank = 1;

>> Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year.
	SELECT year, product_id, total_sales, RANK() OVER (PARTITION BY year ORDER BY total_sales DESC) AS rank
	FROM (
	  SELECT EXTRACT(YEAR FROM sale_date) AS year, product_id, SUM(sales_amount) AS total_sales FROM sales_table
	  GROUP BY year,  product_id) AS yearly_sales
	ORDER BY  year, rank;


