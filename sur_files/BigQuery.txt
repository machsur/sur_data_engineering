================================================================================================================================================
DATE(string in iso) / PARSE_DAT(format, string) / SAFE.PARSE_DATE(format, invalid date)

DATE_DIFF() / TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) AS minutes_diff
extract(month from date)
DATE_TRUNC(order_date, MONTH)
FORMAT_DATE('%B %d, %Y', sale_date)  - Y, y / m, B, b / d, A, a /

=============================================================================================================================================

>> SCD: dimensions that change over time
	SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
	SCD1 - Overwrite -Updates the existing record with new data - No history is preserved.
	SCD2 - Add New Row  -- Keeps full history by adding a new row with versioning or effective dates.
	SCD3 - Add New Column -- Keeps limited history by adding a new column for the previous value.
	SCD4 - scd1+scd2 - Current Table: Stores only the latest data. & History Table: Stores all historical changes with timestamps or versioning.
	scd6 - 1+2+3 - 

>> "write a query to how will you get a list of the top ten most expensive queries that have been executed in your project over the past 30 days:"
	SELECT project_id, job_id, user_email, creation_time, total_slot_ms, total_bytes_billed, total_bytes_processed, query FROM `region-eu`.INFORMATION_SCHEMA.JOBS
	WHERE creation_time between TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND CURRENT_TIMESTAMP()
	      AND job_type = "QUERY"
	      AND total_bytes_processed > 0
	ORDER BY total_bytes_billed DESC LIMIT 10;

>> difference b/w CAST, SAFE_CAST 
	If the conversion fails, SAFE_CAST returns NULL instead of throwing an error, allowing the query to continue.

>> Create table with metadata from existing table – use where 1 = 2 or false

>> Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

>> 
	Temporary table/cache: exist only for the specified period. - useful for intermediate data processing and caching query results.
	Internal tables: stored in bq - take adv of opti like partitioning & clustering - Ideal for frequently accessed and processed data.
	External table: Data remains in its original location (e.g., Cloud Storage) - avoid storage costs - Suitable for occasional queries on large datasets.

>> Time travel concept: Query the state of the table as it was at a specific timestamp
	create table `your_dataset.your_table11` as
	SELECT * FROM `your_dataset.your_table`
	FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)

	bq cp pr_id:mydataset.mytable@7200000 pr_id:mydataset.mytable_restored (millisecounds)


>> Creation of view/materialized view and auth view: efficiently manage and secure your data access in BigQuery
   Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data access / 
   Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
   Authorized views and authorized materialized views: let you share query results with particular users and groups withoutgiving them access to the underlying source data.
         CREATE VIEW `your_dataset.your_view` AS
         SELECT customer_id, order_date, total_amount FROM `your_dataset.orders` WHERE order_date >= '2025-01-01'

        CREATE MATERIALIZED VIEW `your_dataset.your_materialized_view` AS
        SELECT customer_id, order_date, SUM(total_amount) AS total_spent FROM `your_dataset.orders` GROUP BY customer_id, order_date

>> Partitioning and clustering:
   Partitioning: dividing a table into smaller segments. - single column (time-based & inter range), 4000 max
   clustering: sorts the partition data in storage blocks. - four columns, 
   Improved Query Performance, Cost Efficiency
      
	CREATE TABLE `your_dataset.your_table`
      	PARTITION BY DATE(order_date) / _PARTITIONTIME / RANGE_BUCKET(product_id, GENERATE_ARRAY(1, 1000, 100))
      	CLUSTER BY customer_id, product_id
      	AS
      	SELECT * FROM `source_dataset.source_table`

	CREATE TABLE `project_id.dataset_id.table_name`
	(
	  id INT64,
	  name STRING,
	  created_at DATE,
	  category STRING,
	  region STRING
	)
	PARTITION BY DATE(created_at)
	CLUSTER BY category, region;


>> Joins: -  combine rows from two or more tables based on a related column between them.
    INNER JOIN / left join / right join / full join / cross join /
 	SELECT c.CustomerID, c.CustomerName, o.OrderID, o.OrderDate, oi.OrderItemID,
	FROM `your_dataset.customers` AS c 
	JOIN `your_dataset.orders` AS o ON c.CustomerID = o.CustomerID
	JOIN `your_dataset.order_items` AS oi ON o.OrderID = oi.OrderID;

	SELECT * FROM  `project.dataset.table1` AS a CROSS JOIN `project.dataset.table2` AS b;

>> table1 --> 1, 2, 2, 3 table2 --> 1, 1, 2, 3, 3, 3
	SELECT * FROM  `project.dataset.table1` a JOIN `project.dataset.table2` AS b 
	on a.id = b.id;  1 1 / 1 1 / 2 2 / 2 2 / 3 3 / 3 3 /3 3 /
	on a.id > b.id;  2 1 / 2 1 / 2 1 / 2 1 / 3 1 / 3 1 / 3 2/
	on a.id < b.id;  1 2 / 1 3 / 1 3 / 1 3 / 2 3 / 2 3 / 2 3/ 2 3 / 2 3 / 2 3 /

>> With statement: also known as Common Table Expressions (CTEs)
   Adv: code more readable, resusebility 
	WITH orders as (select statement)
	select * from orders

11. Pivoting and unpivoting: help in reshaping your data for analysis or reporting
   Pivot: row into columns
	product	| month	| sales
	select product, IFNULL(Jan, 0) as Jan, IFNULL(Feb, 0) as Feb, IFNULL(Mar, 0) as Mar from table 
	pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar'));	
	or use case statement
	select product, 
	sum(case when month = 'Jan' then sales else 0 end) as Jan,
	sum(case when month = 'Feb' then sales else 0 end) as Feb,
	sum(case when month = 'Mar' then sales else 0 end) as Mar,
	from table group by product;

   unpivot: columns into rows
	product | Jan | Feb | Mar
	select product, month, sales from table 
	unpivot(sales for month in (Jan, Feb, Mar));

12. Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
   De Normalization – redundant data (high storage cost) – better Query performance 
   Why array & structs? – for better query per and lesser storage costs
   
   Arrays – list of items having same data type
   SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element;

   Struct – record with nested fields having different data types
   SELECT person.id, person.name FROM mydataset.mytable;
 
13. Window functions: performing advanced calculations over a set of rows, go beyond simple aggregates or filters
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /row_number() / rank() / dense_rank() 
   SUM(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rt
   lag(cn) / lead(cn): Accesses data from a previous or subsequent row in the same result set without use of a self-join.
   LAG(column2, 1) OVER (PARTITION BY column1 ORDER BY column2) AS previous_value,
   first_value(cn) / last_value(cn) / nth_value(cn, n) 
   FIRST_VALUE(column2) OVER (PARTITION BY column1 ORDER BY column2) AS first_value,
   ntile(n) – n groups 
   NTILE(4) OVER (PARTITION BY column1 ORDER BY column2) AS quartile
   cum_dist() / percent_rank() 
   CUME_DIST= Number of rows with values less than or equal to the current row / Total number of rows
​   PERCENT_RANK= Rank of current row−1 / Total rows−1
​   Rows consider current row even if duplicates / range considers bottom row of duplicates.

14. Regular expressions: used for pattern matching and string manipulation
	   REGEXP_EXTRACT(column_name, r'pattern') / select regexp_extract('file_name_20250430123056.csv', r'^(.*)_\d{14}\.csv$')
	   REGEXP_EXTRACT_ALL(column_name, r'pattern') / select regexp_extract_all('abc123def456ghi789', r'\d+')
	   REGEXP_REPLACE(column_name, r'pattern', 'replacement') / REGEXP_REPLACE(file_path, r'_\d{14}\.csv$', '_TIMESTAMP.csv') 
	   REGEXP_CONTAINS('abc123def456ghi', r'\d') AS contains_digits
	   REGEXP_SUBSTR('abc123def456ghi', r'\d+', 5, 2) AS first_digits

15. BigQuery slots vs reservations? 
	Slots are units of computational capacity in BigQuery. (CPU and memory)
	Reservations allow you to purchase dedicated slots (called commitments) and allocate them to specific workloads or projects.

15.1. Big Query cost optimization
	   -- bq compute optimization: 
		on-demaned pricing - cost per tb, good for unpredectable workloads
		Capacity Pricing - purchase dedicated slots, charged per slot-hour (different editions like Enterprise and Enterprise Plus, which offer options for one- or 		three-year commitments at discounted rates)- decide based on your workloads.
	   -- BQ data storage – 
		Billing model: logical- data in uncompressed format/ physical- data in compressed format) 
	         # For logical bytes billing:   bq update --dataset --storage_billing_model=LOGICAL my_dataset
	         # For physical bytes billing:  bq update --dataset --storage_billing_model=PHYSICAL my_dataset
	   	Use Table Expiration Settings, Optimize Data Storage Format (cloumner format - paqruet. ORC), Use Long-Term Storage Pricing (> 90 days)
	   -- bq query optimization

16. Optimization of query performance in BigQuery?
	   create dataset in region wehere customer operates
	   Use Partitioned Tables and Clustered Tables
	   use avro format to bigquery data load - google rec compressed avro for quick data load
	   preview option, 
	   select columns instead of *, use truncate instead of delete
	   Join pattern larger table join smaller tables, avoid cross join
	   where class first condition in such way that eliminates most data
	   late aggregation
	   Use WITH Clauses for Subquery Caching (Avoid Repeated Scalar Subqueries)
	   Take Advantage of Query Caching
	   Aggregate Data at the Source: Use Approximate Aggregations: 
	   Create a materialized view based on frequently run queries.
	   Optimize Window Functions
	   Monitor and Tune Query Performance
	   nest repeated data, Denormalize Data Where Appropriate (Use ARRAY Functions for Multi-Value Fields)
  
>> Cost Controls: budget alerts, Set query usage limits in IAM quotas, Billing reports, Schedule queries during off-peak hours

>> Explain about data modelling concepts( Star and snowflake schema)? 
	Star Schema: Faster query performance due to fewer joins.
	Snowflake Schema: More normalized and reduces data redundancy. May require more joins, potentially impacting query performance.

>> How to get metadata of datasets and tables in BigQuery? 	
	to get metadata about datasets:  SELECT * FROM `project_id`.`region-europe-west2`.INFORMATION_SCHEMA.SCHEMATA;
	to get metadata about tables: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.TABLES;
	to get metadata about columns: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.COLUMNS;

>> Database, DWH, DataLake
	Database: OLTP system - day to day operational data - mysql
	DWH: Stores current and historical data from multiple sources - BI, analysis and reporting. -  bq
	DataLake: Stores large volumes of raw data in its native format (Structured, semi-structured, and unstructured data)-  machine learning - gcs, hdfs

>> ETL VS ELT

>> Subquery, cte
   
>> Change Data Capture (CDC) -->> identify and capture changes made to data in a database 
      log based --> very efficient, no impact on source system
      query based on timestamp
      triggers load changes into new tables


=============================================================================================================================================

>> To find the department names where no employees are found, you can use the following SQL query:
   emp -->> employee_id	employee_name	dept_id
   Department Table -->> dept_id	dept_name
   SELECT d.dept_name FROM dept d LEFT JOIN employee e ON d.dept_id = e.dept_id WHERE e.dept_id IS NULL;

>> Given two tables, find the count of records for Left Outer Join and Inner Join:
Table A: Table B:
1 1
1 1
1 1 
1 
Ans. 12 & 12

>> Give the output for DENSE_RANK() and RANK() functions for the below dataset:
Nums 
85 
85 
80 
75 
75 
70

>> Given a table with column 'Country', select data in the below sequence:
Table: Matches
Country 
India 
Australia 
Pakistan 
Output:
India vs Australia 
India vs Pakistan 
Australia vs Pakistan
Ans. 
    SELECT a.countryc || ' vs ' || b.countryc as country
    FROM
      `powerful-layout-445408-p5.sur_test_ds.countryt` a
    join
      `powerful-layout-445408-p5.sur_test_ds.countryt` b
    on a.countryc > b.countryc;  -----> or <

>> Given two tables, output the result of INNER, LEFT, RIGHT, FULL JOINS.
Table1:
col1 
---- 
1 
1 
Table2:
---- 
b 
a 
1 

>> Find the 777th highest salary from a table.
      SELECT salary FROM 
		(SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num FROM `your_dataset.your_table`) WHERE row_num = 777

>> Bigquery arch
   storage and compute - decoupled/isolated (cost efficient)
   layers:
   colossus - is google distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus
   borg - scheduling jobs like run queries

>> Identify customers who placed orders in consecutive months.
	cust, order_date, order

         WITH orders AS (
           SELECT customer_id, DATE_TRUNC(order_date, MONTH) AS order_month FROM `your_dataset.your_table`),
         consecutive_orders AS (
           SELECT customer_id, order_month, LEAD(order_month) OVER (PARTITION BY customer_id ORDER BY order_month) AS next_order_month FROM orders)
         SELECT customer_id, order_month, next_order_month FROM consecutive_orders WHERE DATE_DIFF(next_order_month, order_month, MONTH) = 1

>> Query to get the total number of patients per doctor, including unassigned patients.
	docter_id | pationt_id, doctor_id

         SELECT d.doctor_id, COUNT(p.patient_id) AS total_patients FROM `your_dataset.doctors` d  LEFT JOIN `your_dataset.patients` p
         ON  d.doctor_id = p.doctor_id GROUP BY d.doctor_id

>> Handling NULL values in employee salary using the average salary.
	emp_id, salary

         IFNULL(expression, replacement_value) -->> It takes two arguments. If the first argument is NULL, it returns the second argument.
         COALESCE(expression1, expression2, ..., expressionN) -->> It can take multiple arguments. It returns the first non-NULL argument from the list.
         WITH avg_salary AS (
           SELECT AVG(salary) AS average_salary  FROM `your_dataset.your_table`)
         SELECT employee_id,  COALESCE(salary, (SELECT average_salary FROM avg_salary)) AS adjusted_salary  FROM `your_dataset.your_table`

>>
	Src					
	id	name											
	2	b1							
	4	d								
						
	beforeTgt					
	id	name
	1	a
	2	b
	3	c
	
	after source ingestion Tgt	
	id	name
	1	a
	2	b1
	3	c
	4	d

	MERGE INTO target_table AS T   USING source_table AS S
	ON T.id = S.id
	WHEN MATCHED THEN UPDATE SET T.name = S.name
	WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);

>>
ORDER_DAY         ORDER_ID            PRODUCT_ID   QUANTITY     PRICE
------------------ -------------------- ---------- ---------- ----------
01-JUL-11         O1                  P1                 5          5
02-JUL-11         O8                  P5                 1         50
Get me all products that got sold both the days and the number of times the product is sold.  
	
	PRODUCT_ID  COUNT
	P1            3
	P2            2
	P3            2

	SELECT PRODUCT_ID, COUNT(*) AS COUNT FROM orders WHERE ORDER_DAY IN ('01-JUL-11', '02-JUL-11') GROUP BY PRODUCT_ID HAVING COUNT(DISTINCT ORDER_DAY) = 2;

>>
INPUT
order_id	customer_id	region	   order_date	  sales_amount
1	        101	        North	   1/1/2022	  100

OUTPUT
customer_id	north_sales	south_sales	East sales	West Sales
101	500	0	700	0
102	0	800	0	1000
103	0	0	500	0
104	0	300	0	500

	SELECT customer_id,
	  SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales,
	  SUM(CASE WHEN region = 'South' THEN sales_amount ELSE 0 END) AS south_sales,
	  SUM(CASE WHEN region = 'East' THEN sales_amount ELSE 0 END) AS east_sales,
	  SUM(CASE WHEN region = 'West' THEN sales_amount ELSE 0 END) AS west_sales
	FROM orders GROUP BY customer_id;

	select * from (select customer_id, region, sales_amount from orders) pivot(sum(sales_amount) for region in ('north', 'south', 'east', 'west'));

>> Null doesnt include matching, empty string includes matching
table1
1
2
3
null
""

table2
2
2
3
null
null
""

	inner join  - 2 2 / 2 2 / 3 3 / empty empty / 
	left join - 1 null / 2 2 / 2 2/ 3 3/ empty empty / null null /

>> Find the second highest salary without using LIMIT, OFFSET, or TOP.
    emp_id, salary

	WITH RankedSalaries AS (SELECT salary, dense_rank() OVER(ORDER BY salary DESC) AS rank FROM employees)
	SELECT salary FROM RankedSalaries WHERE rank = 2;
	
	SELECT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 1;

>> Given a table of orders, write a query to find the running total (cumulative sum) of revenue for each day.
    order_date, revenue

	SELECT  order_date, revenue, SUM(revenue) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM  orders

>> Write an SQL query to identify employees who earn more than their managers.
	EMPID, EMPNAME, MANAGERID, SLARY
	SELECT  e.employee_id,  e.name AS employee_name,  e.salary AS employee_salary,  m.name AS manager_name,  m.salary AS manager_salary 
	FROM  employees e JOIN  employees m ON e.manager_id = m.employee_id WHERE  e.salary > m.salary	ORDER BY  e.salary DESC;

>> Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount.
    CustomerID | PurchaseAmount
	WITH RankedPurchases AS (
    		SELECT CustomerID, PurchaseAmount, DENSE_RANK() OVER (ORDER BY PurchaseAmount DESC) AS Rank FROM `your_dataset.your_table`)
	SELECT CustomerID, PurchaseAmount FROM RankedPurchases WHERE Rank <= N;

>> Identify consecutive login streaks for users where they logged in for at least three consecutive days.
    user_id | login_date
	WITH login_data AS (
	  SELECT user_id, login_date,
	         LAG(login_date, 1) OVER (PARTITION BY user_id ORDER BY login_date) AS prev_login_date,
	         LAG(login_date, 2) OVER (PARTITION BY user_id ORDER BY login_date) AS prev_prev_login_date
	  FROM `your_dataset.your_table`
	),
	streaks AS (
	  SELECT user_id, login_date,
	         CASE  WHEN DATE_DIFF(login_date, prev_login_date, DAY) = 1 AND DATE_DIFF(prev_login_date, prev_prev_login_date, DAY) = 1 THEN 1  ELSE 0 END AS is_streak
	  FROM login_data)
	SELECT user_id, login_date FROM streaks	WHERE is_streak = 1 ORDER BY user_id, login_date;

>> Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.
	WITH ranked_records AS (
	  SELECT *, ROW_NUMBER() OVER (PARTITION BY column1, column2, column3 ORDER BY column1) AS row_num FROM `your_dataset.your_table`)
	DELETE FROM ranked_records WHERE row_num > 1;

	-- Create a temp table to store unique records, Delete all records from the original table, Insert unique records back into the original table, Drop the temp table

>> Retrieve the first order for each customer, ensuring that ties (customers with multiple first orders on the same date) are handled correctly.
    customer_id | order_id | order_date
	
	WITH ranked_orders AS (
	  SELECT customer_id, order_id, order_date, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date, order_id) AS row_num
	  FROM `your_dataset.orders`)
	SELECT customer_id, order_id, order_date FROM ranked_orders WHERE row_num = 1;

>> Find products that were never purchased by any customer.
    t1: product_id, product_name;  t2: product_id, order_id

	SELECT p.product_id, p.product_name FROM `your_dataset.products` p LEFT JOIN `your_dataset.orders` o
	ON p.product_id = o.product_id	WHERE o.product_id IS NULL;

>> Given a table with customer transactions, find customers who made transactions in every month of the year.
    cust_id, date, purchage_amount

	WITH monthly_transactions AS (
	  SELECT customer_id, EXTRACT(MONTH FROM transaction_date) AS month FROM `your_dataset.transactions`  GROUP BY customer_id, month),
	customer_months AS (
	  SELECT customer_id, COUNT(month) AS months_count FROM monthly_transactions GROUP BY customer_id	)
	SELECT customer_id FROM customer_months WHERE months_count = 12;

>> Find employees who have the same salary as another employee in the same department.
    dept_id, emp_name, salary
	WITH EmployeeSalaries AS (
	  SELECT   department_id, salary, COUNT(*) AS employee_count  FROM employees GROUP BY department_id, salary HAVING COUNT(*) > 1)
	SELECT e.employee_id, e.department_id, e.salary	FROM employees e JOIN EmployeeSalaries es
	ON  e.department_id = es.department_id AND e.salary = es.salary
	ORDER BY e.department_id, e.salary;

>> Write an SQL query to retrieve the department with the highest total salary paid to employees.
    dept_od, salary 
	SELECT DepartmentID, SUM(Salary) AS TotalSalary FROM `your_dataset.your_table` GROUP BY DepartmentID ORDER BY TotalSalary DESC LIMIT 1;

>> Use a window function to rank orders based on order value for each customer, and return only the top 3 orders per customer.
    CustomerID, OrderID, OrderValue
	WITH RankedOrders AS (
	    SELECT CustomerID, OrderID, OrderValue, RANK() OVER (PARTITION BY CustomerID ORDER BY OrderValue DESC) AS Rank FROM `your_dataset.your_table`)
	SELECT CustomerID, OrderID, OrderValue FROM RankedOrders WHERE Rank <= 3;

>> Write a query to pivot a table where each row represents a sales transaction and transform it into a summary format where each column represents a month.
	transaction_id	sale_date	amount
	1	        2025-01-15	100
	year	Jan	Feb	Mar
	2025	250	450	300

	WITH sales_data AS (
	  SELECT EXTRACT(YEAR FROM sale_date) AS year, FORMAT_DATE('%b', sale_date) AS month, amount  FROM `project.dataset.sales`)
	SELECT * FROM sales_data	PIVOT (SUM(amount) FOR month IN ('Jan', 'Feb', 'Mar'))	ORDER BY year;

>> Find the moving average of sales for the last 7 days for each product in a sales table.
	product_id, sale_amount, sale_date

	WITH SalesWithMovingAverage AS (
	    SELECT ProductID, SaleDate, SlesAmount, AVG(SalesAmount) OVER (PARTITION BY ProductID ORDER BY SaleDate ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS 		    MovingAverage FROM `your_dataset.sales`)
	SELECT ProductID, SaleDate, SalesAmount, MovingAverage FROM SalesWithMovingAverage ORDER BY ProductID, SaleDate;

>> Given an events table, find the first and last occurrence of each event per user.
        UserID,   EventID,  EventDate,

	WITH ranked_events AS (
	  SELECT
	    user_id,
	    event_type,
	    event_time,
	    ROW_NUMBER() OVER (PARTITION BY user_id, event_type ORDER BY event_time ASC) AS rn_first,
	    ROW_NUMBER() OVER (PARTITION BY user_id, event_type ORDER BY event_time DESC) AS rn_last
	  FROM
	    events
	)
	
	SELECT
	  user_id,
	  event_type,
	  MIN(CASE WHEN rn_first = 1 THEN event_time END) AS first_occurrence,
	  MIN(CASE WHEN rn_last = 1 THEN event_time END) AS last_occurrence
	FROM
	  ranked_events
	GROUP BY
	  user_id, event_type
	ORDER BY
	  user_id, event_type;

        
>> Identify users who have placed an order in two consecutive months but not in the third month.
	user_id, order_date

	WITH MonthlyOrders AS (
	    SELECT UserID, FORMAT_TIMESTAMP('%Y-%m', OrderDate) AS OrderMonth FROM `your_dataset.orders` GROUP BY UserID, OrderMonth),
	ConsecutiveMonths AS (
	    SELECT UserID, OrderMonth,
	        LAG(OrderMonth, 1) OVER (PARTITION BY UserID ORDER BY OrderMonth) AS PreviousMonth,
	        LEAD(OrderMonth, 1) OVER (PARTITION BY UserID ORDER BY OrderMonth) AS NextMonth
	    FROM MonthlyOrders),
	FilteredUsers AS (
	    SELECT UserID, OrderMonth FROM ConsecutiveMonths WHERE 
	        PreviousMonth IS NOT NULL AND NextMonth IS NOT NULL
	        AND DATE_SUB(PARSE_TIMESTAMP('%Y-%m', OrderMonth), INTERVAL 1 MONTH) = PARSE_TIMESTAMP('%Y-%m', PreviousMonth)
	        AND DATE_ADD(PARSE_TIMESTAMP('%Y-%m', OrderMonth), INTERVAL 1 MONTH) = PARSE_TIMESTAMP('%Y-%m', NextMonth)
	)
	SELECT UserID	FROM FilteredUsers GROUP BY UserID HAVING COUNT(OrderMonth) = 2;

>> Find the most frequently purchased product category by each user over the past year.
	user, product_category, order_id, sale_date

	WITH user_category_sales AS (
	  SELECT user_id,  product_category, COUNT(*) AS purchase_count FROM sales_table
	  WHERE sale_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) GROUP BY user_id, product_category),
	ranked_categories AS (
	  SELECT  user_id, product_category, purchase_count, RANK() OVER (PARTITION BY user_id ORDER BY purchase_count DESC) AS rank FROM user_category_sales)
	SELECT user_id, product_category, purchase_count FROM ranked_categories WHERE rank = 1;

>> Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year.
	prodct_id, sale_amount, sale_date

	SELECT year, product_id, total_sales, RANK() OVER(PARTITION BY year ORDER BY total_sales DESC) AS rank
	FROM (
	  SELECT EXTRACT(YEAR FROM sale_date) AS year, product_id, SUM(sales_amount) AS total_sales FROM sales_table
	  GROUP BY year,  product_id) AS yearly_sales
	ORDER BY  year, rank;

>>
input table:
emp_name emp_dep
abc   it
def   hr
ghi  sales
uio  dev
pqr   sales
jkl   it
output table:
emp_name emp_dep
abc   it
jkl   it
def   hr
ghi  sales
pqr  sales
uio  dev
the output order same as above. give bigquery sql query with custom logic

	WITH CustomOrder AS 
	  (SELECT emp_name, emp_dep, 
		CASE WHEN emp_dep = 'it' THEN 1 WHEN emp_dep = 'hr' THEN 2 WHEN emp_dep = 'sales' THEN 3 WHEN emp_dep = 'dev'  THEN 4 end as emp_dept_sort,
		ROW_NUMBER() OVER(PARTITION BY emp_dep ORDER BY emp_name) AS row_num  FROM your_table_name)
	SELECT emp_name, emp_dep FROM CustomOrder ORDER BY emp_dept_sort, row_num;

>> Identify users who increased their average order value consistently over the last 3 months.

>> Write a query to detect a sudden drop (>30%) in daily revenue for any restaurant. (restaurant_id, revenue_date, daily_revenue)
	WITH revenue_changes AS (
	  SELECT
	    restaurant_id,
	    revenue_date,
	    daily_revenue,
	    LAG(daily_revenue, 1) OVER (PARTITION BY restaurant_id ORDER BY revenue_date) AS previous_revenue
	  FROM
	    `your_project.your_dataset.restaurant_revenue`
	),
	sudden_drop AS (
	  SELECT
	    restaurant_id,
	    revenue_date,
	    daily_revenue,
	    previous_revenue,
	    (previous_revenue - daily_revenue) / previous_revenue * 100 AS drop_percentage
	  FROM
	    revenue_changes
	  WHERE
	    previous_revenue IS NOT NULL AND
	    (previous_revenue - daily_revenue) / previous_revenue * 100 > 30
	)
	SELECT
	  restaurant_id,
	  revenue_date,
	  daily_revenue,
	  previous_revenue,
	  drop_percentage
	FROM
	  sudden_drop;

>> Find the moving average of daily revenue for each restaurant over a 7-day window.
	restarant | date | revenue
		SELECT
		  restaurant_id,
		  date,
		  revenue,
		  AVG(revenue) OVER (
		    PARTITION BY restaurant_id
		    ORDER BY date
		    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
		  ) AS moving_avg_revenue
		FROM
		  your_table_name
		ORDER BY
		  restaurant_id, date;

>> Calculate month-over-month growth in active users for each city.
	user_id	| city	| activity_date
		WITH monthly_active_users AS (
		  SELECT
		    city,
		    EXTRACT(YEAR FROM activity_date) AS year,
		    EXTRACT(MONTH FROM activity_date) AS month,
		    COUNT(DISTINCT user_id) AS active_users
		  FROM
		    user_activity
		  GROUP BY
		    city, year, month
		),
		growth_calculation AS (
		  SELECT
		    city,
		    year,
		    month,
		    active_users,
		    LAG(active_users) OVER (
		      PARTITION BY city
		      ORDER BY year, month
		    ) AS prev_month_active_users
		  FROM
		    monthly_active_users
		)
		SELECT
		  city,
		  year,
		  month,
		  active_users,
		  prev_month_active_users,
		  (active_users - prev_month_active_users) / prev_month_active_users * 100 AS month_over_month_growth
		FROM
		  growth_calculation
		WHERE
		  prev_month_active_users IS NOT NULL
		ORDER BY
		  city, year, month;

>> Find restaurants whose best-selling dish makes up over 60% of their revenue.
	restaurant_id, dish_id,    dish_name,  quantity, price

	WITH dish_revenue AS (
	  SELECT
	    restaurant_id,
	    dish_id,
	    dish_name,
	    SUM(quantity * price) AS total_revenue
	  FROM
	    `your_dataset.orders`
	  GROUP BY
	    restaurant_id, dish_id, dish_name
	),
	
	restaurant_total_revenue AS (
	  SELECT
	    restaurant_id,
	    SUM(total_revenue) AS restaurant_revenue
	  FROM
	    dish_revenue
	  GROUP BY
	    restaurant_id
	),
	
	best_selling_dish AS (
	  SELECT
	    restaurant_id,
	    dish_name,
	    MAX(total_revenue) AS best_dish_revenue
	  FROM
	    dish_revenue
	  GROUP BY
	    restaurant_id, dish_name
	),
	
	combined AS (
	  SELECT
	    b.restaurant_id,
	    b.dish_name AS best_selling_dish,
	    b.best_dish_revenue,
	    r.restaurant_revenue,
	    b.best_dish_revenue / r.restaurant_revenue AS revenue_share
	  FROM
	    best_selling_dish b
	  JOIN
	    restaurant_total_revenue r
	  ON
	    b.restaurant_id = r.restaurant_id
	)
	
	SELECT
	  restaurant_id,
	  best_selling_dish,
	  best_dish_revenue,
	  restaurant_revenue,
	  revenue_share
	FROM
	  combined
	WHERE
	  revenue_share > 0.6;


>> Rank users based on their total number of orders and total spend, giving more weight to spend. -- Weighted score: 70% weight to spend, 30% to order count
order_id	user_id		order_amount
1		U001		100
2		U001		150

	WITH user_stats AS (
	  SELECT
	    user_id,
	    COUNT(order_id) AS total_orders,
	    SUM(order_amount) AS total_spend
	  FROM
	    `your_dataset.orders`
	  GROUP BY
	    user_id
	),
	scored_users AS (
	  SELECT
	    user_id,
	    total_orders,
	    total_spend,
	    (0.7 * total_spend + 0.3 * total_orders) AS weighted_score
	  FROM
	    user_stats
	),	
	ranked_users AS (
	  SELECT
	    user_id,
	    total_orders,
	    total_spend,
	    weighted_score,
	    RANK() OVER (ORDER BY weighted_score DESC) AS rank
	  FROM
	    scored_users
	)
	SELECT * FROM ranked_users;

>> Identify restaurants with high churn rates: drop in unique monthly customers for 3+ months.
order_id	user_id		restaurant_id	order_date
1		U001		R001		2024-01-15
2		U002		R001		2024-01-20

	WITH monthly_customers AS (
	  SELECT
	    restaurant_id,
	    FORMAT_DATE('%Y-%m', DATE(order_date)) AS month,
	    COUNT(DISTINCT user_id) AS unique_customers
	  FROM
	    `your_dataset.orders`
	  GROUP BY
	    restaurant_id, month
	),
	
	monthly_with_lag AS (
	  SELECT
	    restaurant_id,
	    month,
	    unique_customers,
	    LAG(unique_customers, 1) OVER (PARTITION BY restaurant_id ORDER BY month) AS prev_month_1,
	    LAG(unique_customers, 2) OVER (PARTITION BY restaurant_id ORDER BY month) AS prev_month_2,
	    LAG(unique_customers, 3) OVER (PARTITION BY restaurant_id ORDER BY month) AS prev_month_3
	  FROM
	    monthly_customers
	),
	
	churn_flags AS (
	  SELECT
	    restaurant_id,
	    month,
	    unique_customers,
	    prev_month_1,
	    prev_month_2,
	    prev_month_3,
	    -- Check if customer count dropped for 3 consecutive months
	    CASE
	      WHEN prev_month_1 IS NOT NULL AND prev_month_2 IS NOT NULL AND prev_month_3 IS NOT NULL
	           AND unique_customers < prev_month_1
	           AND prev_month_1 < prev_month_2
	           AND prev_month_2 < prev_month_3
	      THEN 1 ELSE 0
	    END AS churn_flag
	  FROM
	    monthly_with_lag
	)
	
	SELECT DISTINCT restaurant_id
	FROM churn_flags
	WHERE churn_flag = 1;

>> Calculate the percentage of abandoned carts (added to cart but not ordered). user & order tables
user_id		item_id		event_type	event_time
U001		I001		add_to_cart	2024-01-01 10:00:00
user_id		item_id		order_id	order_time
U001		I001		O001		2024-01-01 10:10:00

	WITH added_to_cart AS (
	  SELECT DISTINCT user_id, item_id
	  FROM `your_dataset.cart_events`
	  WHERE event_type = 'add_to_cart'
	),
	ordered_items AS (
	  SELECT DISTINCT user_id, item_id
	  FROM `your_dataset.orders`
	),
	abandoned_carts AS (
	  SELECT
	    a.user_id,
	    a.item_id
	  FROM
	    added_to_cart a
	  LEFT JOIN
	    ordered_items o
	  ON
	    a.user_id = o.user_id AND a.item_id = o.item_id
	  WHERE
	    o.user_id IS NULL
	),
	cart_stats AS (
	  SELECT
	    (SELECT COUNT(*) FROM abandoned_carts) AS abandoned_count,
	    (SELECT COUNT(*) FROM added_to_cart) AS total_added
	)
	
	SELECT
	  abandoned_count,
	  total_added,
	  ROUND(100 * abandoned_count / total_added, 2) AS abandoned_percentage
	FROM
	  cart_stats;

>> Find cities where lunch orders (12–3 PM) have higher revenue than dinner orders (7–10 PM).
order_id	user_id		city	order_time		order_amount
1		U001		Delhi	2024-01-01 12:30:00	200
2		U002		Delhi	2024-01-01 19:30:00	150
	WITH time_filtered_orders AS (
	  SELECT
	    city,
	    order_amount,
	    EXTRACT(HOUR FROM order_time) AS order_hour
	  FROM
	    `your_dataset.orders`
	),
	categorized_orders AS (
	  SELECT
	    city,
	    CASE
	      WHEN order_hour BETWEEN 12 AND 14 THEN 'lunch'
	      WHEN order_hour BETWEEN 19 AND 21 THEN 'dinner'
	      ELSE NULL
	    END AS meal_period,
	    order_amount
	  FROM
	    time_filtered_orders
	  WHERE
	    order_hour BETWEEN 12 AND 14 OR order_hour BETWEEN 19 AND 21
	),
	revenue_by_city_meal AS (
	  SELECT
	    city,
	    meal_period,
	    SUM(order_amount) AS total_revenue
	  FROM
	    categorized_orders
	  GROUP BY
	    city, meal_period
	),
	pivoted_revenue AS (
	  SELECT
	    city,
	    MAX(CASE WHEN meal_period = 'lunch' THEN total_revenue ELSE 0 END) AS lunch_revenue,
	    MAX(CASE WHEN meal_period = 'dinner' THEN total_revenue ELSE 0 END) AS dinner_revenue
	  FROM
	    revenue_by_city_meal
	  GROUP BY
	    city
	)
	SELECT
	  city,
	  lunch_revenue,
	  dinner_revenue
	FROM
	  pivoted_revenue
	WHERE
	  lunch_revenue > dinner_revenue;

>> Detect duplicated orders — same items, user, and restaurant within 5 minutes.
order_id	user_id		restaurant_id	item_id		order_time
1		U001		R001		I001		2024-01-01 10:00:00
2		U001		R001		I001		2024-01-01 10:03:00
3		U002		R002		I002		2024-01-01 11:00:00
	WITH ordered_data AS (
	  SELECT
	    order_id,
	    user_id,
	    restaurant_id,
	    item_id,
	    order_time,
	    LAG(order_time) OVER (
	      PARTITION BY user_id, restaurant_id, item_id
	      ORDER BY order_time
	    ) AS previous_order_time
	  FROM
	    `your_dataset.orders`
	),
	
	duplicates AS (
	  SELECT
	    order_id,
	    user_id,
	    restaurant_id,
	    item_id,
	    order_time,
	    previous_order_time,
	    TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) AS minutes_diff
	  FROM
	    ordered_data
	  WHERE
	    previous_order_time IS NOT NULL
	    AND TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) <= 5
	)
	
	SELECT * FROM duplicates;


>> For each cuisine, identify the most loyal customer (most repeat orders).
order_id	user_id		cuisine		order_date
1		U001		Italian		2024-01-01
	WITH order_counts AS (
	  SELECT   cuisine,    user_id,  COUNT(*) AS order_count FROM `your_dataset.orders` GROUP BY  cuisine, user_id),
	ranked_customers AS (
	  SELECT   cuisine,  user_id, order_count, ROW_NUMBER() OVER (PARTITION BY cuisine ORDER BY order_count DESC) AS rank FROM  order_counts)
	SELECT  cuisine,  user_id AS most_loyal_customer,  order_count	FROM  ranked_customers	WHERE rank = 1;

>> Write a query to return the top 5% of users by revenue contribution (Pareto Principle). (user_id,  revenue)
	
	WITH ranked_users AS (
	  SELECT
	    user_id,
	    revenue,
	    PERCENT_RANK() OVER (ORDER BY revenue DESC) AS revenue_percentile
	  FROM
	    `your_dataset.user_revenue`
	)
	
	SELECT
	  user_id,
	  revenue
	FROM
	  ranked_users
	WHERE
	  revenue_percentile <= 0.05
	ORDER BY
	  revenue DESC;
	
>> Determine which users are likely bots based on their ordering patterns (e.g., 50+ orders/day). (user, order_timestamp, order_id)
	SELECT
	  user_id,
	  DATE(order_timestamp) AS order_day,
	  COUNT(order_id) AS orders_per_day
	FROM
	  `your_project.your_dataset.orders`
	GROUP BY
	  user_id,
	  order_day
	HAVING
	  orders_per_day >= 50
	ORDER BY
	  order_day,
	  orders_per_day DESC;

>> Build a time-series summary table that aggregates revenue per day, per restaurant, for dashboarding.     (restaurant_id, order_day, revenue)
	SELECT  restaurant_id, order_day, SUM(revenue) AS daily_revenue FROM  `your_project.your_dataset.orders`
	GROUP BY  restaurant_id,  order_day ORDER BY  order_day,  restaurant_id;

>> Identify duplicate records in a table based on a specific column. (like email)
	select * from `elegant-circle-454709-h5.surtest11.dupli` where email in
	(SELECT email FROM `elegant-circle-454709-h5.surtest11.dupli` group by email having count(*)>1);
	
>> Find the department with the highest average salary.
	dept, salary

	SELECT
	  department,
	  AVG(salary) AS average_salary
	FROM
	  `your_project.your_dataset.employee_data`
	GROUP BY
	  department
	ORDER BY
	  average_salary DESC
	LIMIT 1;


>> List the top 3 most frequently sold products. (product_name , quantity_sold)

	SELECT
	  product_name,
	  SUM(quantity_sold) AS total_quantity_sold
	FROM
	  `your_project.your_dataset.sales_data`
	GROUP BY
	  product_name
	ORDER BY
	  total_quantity_sold DESC
	LIMIT 3;

>> Fetch employees who were hired in the last 90 days.  (emp_name, hire_date)
	SELECT
	  *
	FROM
	  `your_project.your_dataset.employee_data`
	WHERE
	  hire_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);


>> Question:- Extract the Domain from the Email column in Employee Table.
	substring(email, strpos(email, '@')+1, length-opt) 

