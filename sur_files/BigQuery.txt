================================================================================================================================================
DATE(DC in iso) / PARSE_DATE(format, DC) / SAFE.PARSE_DATE(format, DC)	-	Y, y / m, B, b / d, A, a / 
extract(year from date)      						-     	year / month / date / hour / minute / second / microsecond /
FORMAT_DATE('%B %d, %Y', sale_date)  	
DATE_TRUNC(order_date, MONTH) 		
DATE_DIFF(order_date, previous_date, day)
Date_add(order_date, INTERVAL 5 day)
Date_sub(order_date, INTERVAL 5 day)

to_date("date_str in iso") / to_date("date_str", "dd/MM/yyyy") / same for safe
datediff("end_date", "start_date")   (unix_timestamp("end_ts") - unix_timestamp("start_ts")) / 3600
month("order_date")
trunc("order_date", "MM")
date_format("sale_date", "MMMM dd, yyyy") 	yy, yyyy, M, MM, MMM, MMMM, d, dd, E, EEEE
=============================================================================================================================================

>> Bigquery arch
   storage and compute - decoupled/isolated (cost efficient)
   layers:
   colossus - is google distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus
   borg - scheduling jobs like run queries

>>  
Fact Table
Contains numeric values (quantitative data ) (facts) like sales, revenue, profit, etc.
Has foreign keys referencing dimension tables.
Grows rapidly as new transactions are added.
Date_ID	Product_ID	Store_ID	Sales_Amount	Quantity_Sold
20250601		101		1		5000

Dimension Table
A Dimension Table contains descriptive attributes or textual or categorical data.
Typically smaller and changes less frequently.
Product_ID	Product_Name	Category	Brand
101		Laptop		Electronics	Dell

>>
Star Schema: Multiple Dimension Tables directly connected to the fact table
Snowflake Schema: is a more normalized form of the star schema. Dimension tables are split into sub-dimensions, reducing redundancy.

>> SCD: dimensions that change over time
	SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
	SCD1 - Overwrite -Updates the existing record with new data - No history is preserved.
	SCD2 - Add New Row  -- Keeps full history by adding a new row with versioning or effective dates.
	SCD3 - Add New Column -- Keeps limited history by adding a new column for the previous value.
	SCD4 - scd1+scd2 - Current Table: Stores only the latest data. & History Table: Stores all historical changes with timestamps or versioning.
	scd6 - 1+2+3 - 

>> "write a query to how will you get a list of the top ten most expensive queries that have been executed in your project over the past 30 days:"
	SELECT * FROM `region-eu`.INFORMATION_SCHEMA.JOBS WHERE creation_time between TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND CURRENT_TIMESTAMP();
	to get metadata about datasets:  SELECT * FROM `project_id`.`region-europe-west2`.INFORMATION_SCHEMA.SCHEMATA;
	to get metadata about tables: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.TABLES;
	to get metadata about columns: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.COLUMNS;

>> difference b/w CAST, SAFE_CAST : If the conversion fails, SAFE_CAST returns NULL instead of throwing an error, allowing the query to continue.

>> Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

>> 	Temporary table/cache: exist only for the specified period. - useful for intermediate data processing and caching query results.
	Internal tables: stored in bq - take adv of opti like partitioning & clustering - Ideal for frequently accessed and processed data.
	External table: Data remains in its original location (e.g., Cloud Storage) - avoid storage costs - Suitable for occasional queries on large datasets.

>> Time travel concept: Query the state of the table as it was at a specific timestamp
	create table `your_dataset.your_table11` as
	SELECT * FROM `your_dataset.your_table`	FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)

>> Creation of view/materialized view and auth view: efficiently manage and secure your data access in BigQuery
   Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data access / 
   Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
   Authorized views and authorized materialized views: let you share query results with particular users and groups withoutgiving them access to the underlying source data.

>> Partitioning and clustering:    Improved Query Performance, Cost Efficiency
   Partitioning: dividing a table into smaller segments. - single column (time-based & inter range), 4000 max
   clustering: sorts the partition data in storage blocks. - four columns, 
	CREATE TABLE `your_dataset.your_table`
      	PARTITION BY DATE(order_date) / _PARTITIONTIME / RANGE_BUCKET(product_id, GENERATE_ARRAY(1, 1000, 100))
      	CLUSTER BY customer_id, product_id
      	AS SELECT * FROM `source_dataset.source_table`

>> Joins: -  combine rows from two or more tables based on a related column between them.
    INNER JOIN / left join / right join / full join / cross join /
 
>> Pivoting and unpivoting: help in reshaping your data for analysis or reporting
   Pivot: row into columns 	- product| month| sales  	- select * from table pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar'));	 	- or use case st
   unpivot: columns into rows 	- product | Jan | Feb | Mar	- select product, month, sales from table unpivot(sales for month in (Jan, Feb, Mar));

>>  Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
    De Normalization – redundant data (high storage cost) – better Query performance 
    Why array & structs? – for better query per and lesser storage costs
    Arrays – list of items having same data type  -  SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element; 
    Struct – record with nested fields having different data types  -   SELECT person.id, person.name FROM mydataset.mytable;
 
>> Window functions: performing advanced calculations over a set of rows, go beyond simple aggregates or filters
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /row_number() / rank() / dense_rank() /  lag(cn) / lead(cn) /   first_value(cn) / last_value(cn) / nth_value(cn, n) /
   SUM(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rt
   ntile(n) – n groups 
   CUME_DIST() = Number of rows with values less than or equal to the current row / Total number of rows
​   PERCENT_RANK() = Rank of current row−1 / Total rows−1
​   Rows consider current row even if duplicates / range considers bottom row of duplicates.

>> BigQuery slots vs reservations? 
	Slots are units of computational capacity in BigQuery. (CPU and memory)
	Reservations allow you to purchase dedicated slots (called commitments) and allocate them to specific workloads or projects.

>> Big Query cost optimization
	   -- bq compute optimization: 
		on-demaned pricing - cost per tb, good for unpredectable workloads
		Capacity Pricing - purchase dedicated slots, charged per slot-hour (different editions like Enterprise and Enterprise Plus, which offer options for one- or 		three-year commitments at discounted rates)- decide based on your workloads.
	   -- BQ data storage – 
		Billing model: logical- data in uncompressed format/ physical- data in compressed format) 
	         # For logical bytes billing:   bq update --dataset --storage_billing_model=LOGICAL my_dataset
	         # For physical bytes billing:  bq update --dataset --storage_billing_model=PHYSICAL my_dataset
	   	Use Table Expiration Settings, Optimize Data Storage Format (cloumner format - paqruet. ORC), Use Long-Term Storage Pricing (> 90 days)
	   -- bq query optimization

>> Optimization of query performance in BigQuery?
	Aggregate Data at the Source,  use avro format - for quick data load, 
	dataset in wehere customer operates, 
	Partitioned Tables and Clustered,  preview option, select columns instead of *, use truncate instead of delete
	Join pattern larger table join smaller tables, where class first condition in such way that eliminates most data,   late aggregation, 	   
	Use WITH Clauses for Subquery Caching (Avoid Repeated Scalar Subqueries),  Take Advantage of Query Caching, 
	Create a materialized view based on frequently run queries.
	nest repeated data, Denormalize Data Where Appropriate (Use ARRAY Functions for Multi-Value Fields)
  
>> Cost Controls: budget alerts, Set query usage limits in IAM quotas, Billing reports, Schedule queries during off-peak hours

>> Database, DWH, DataLake

>> ETL VS ELT

>> Subquery, cte
   
>> Change Data Capture (CDC) -->> identify and capture changes made to data in a database 
      log based --> very efficient, no impact on source system
      query based on timestamp
      triggers load changes into new tables


=============================================================================================================================================
>> find max id excluding duplicates: 2, 6, 5, 6, 9, 9, 8

>> To find the department names where no employees are found, you can use the following SQL query:
   emp -->> employee_id	employee_name	dept_id
   Department Table -->> dept_id	dept_name
   SELECT d.dept_name FROM dept d LEFT JOIN employee e ON d.dept_id = e.dept_id WHERE e.dept_id IS NULL;

>> Given two tables, find the count of records for Left Outer Join and Inner Join:
Table A   / Table B:
1 1 1 1  /  1 1 1 
Ans. 12 & 12

>> Give the output for DENSE_RANK() and RANK() functions for the below dataset:
Nums: 85 85 80 75 75 70

>> Given a table with column 'Country', select data in the below sequence:
Country: India Australia Pakistan 
Output: India vs Australia 	India vs Pakistan 	Australia vs Pakistan
    SELECT a.countryc || ' vs ' || b.countryc as country
    FROM `powerful-layout-445408-p5.sur_test_ds.countryt` a  join `powerful-layout-445408-p5.sur_test_ds.countryt` b
    on a.countryc > b.countryc;  -----> or <

>> Given two tables, output the result of INNER, LEFT, RIGHT, FULL JOINS.
Table1-  1 1   /  Table2- b a 1

>> Find the 777th highest salary from a table.
      SELECT salary FROM (SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num FROM `your_dataset.your_table`) WHERE row_num = 777

>> Identify customers who placed orders in consecutive months:  	cust, order_date, order
         WITH orders AS (
           SELECT customer_id, DATE_TRUNC(order_date, MONTH) AS order_month FROM `your_dataset.your_table`),
         consecutive_orders AS (
           SELECT customer_id, order_month, LEAD(order_month) OVER (PARTITION BY customer_id ORDER BY order_month) AS next_order_month FROM orders)
         SELECT customer_id, order_month, next_order_month FROM consecutive_orders WHERE DATE_DIFF(next_order_month, order_month, MONTH) = 1

>> Query to get the total number of patients per doctor, including unassigned patients: 	docter_id | pationt_id, doctor_id
         SELECT d.doctor_id, COUNT(p.patient_id) AS total_patients FROM `your_dataset.doctors` d  LEFT JOIN `your_dataset.patients` p
         ON  d.doctor_id = p.doctor_id GROUP BY d.doctor_id

>> Handling NULL values in employee salary using the average salary:	emp_id, salary
         IFNULL(expression, replacement_value) -->> It takes two arguments. If the first argument is NULL, it returns the second argument.
         COALESCE(expression1, expression2, ..., expressionN) -->> It can take multiple arguments. It returns the first non-NULL argument from the list.
         SELECT employee_id,  COALESCE(salary, (SELECT avg(salary) FROM table)) AS adjusted_salary  FROM table;

>>
	Src					
	id	name											
	2	b1							
	4	d								
						
	beforeTgt					
	id	name
	1	a
	2	b
	3	c
	
	after source ingestion Tgt	
	id	name
	1	a
	2	b1
	3	c
	4	d

	MERGE INTO target_table AS T   USING source_table AS S
	ON T.id = S.id
	WHEN MATCHED THEN UPDATE SET T.name = S.name
	WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);

>>
ORDER_DAY         ORDER_ID            PRODUCT_ID   QUANTITY     PRICE
01-JUL-11         O1                  P1                 5          5
Get me all products that got sold both the days and the number of times the product is sold.  
PRODUCT_ID  COUNT
P1            3
	SELECT PRODUCT_ID, COUNT(*) AS COUNT FROM orders WHERE ORDER_DAY IN ('01-JUL-11', '02-JUL-11') GROUP BY PRODUCT_ID HAVING COUNT(DISTINCT ORDER_DAY) = 2;

>>
order_id	customer_id	region	   order_date	  sales_amount
1	        101	        North	   1/1/2022	  100
customer_id	north_sales	south_sales	East sales	West Sales
101		500		0		700		0
	SELECT customer_id,
	  SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales,
	  SUM(CASE WHEN region = 'South' THEN sales_amount ELSE 0 END) AS south_sales,
	  SUM(CASE WHEN region = 'East' THEN sales_amount ELSE 0 END) AS east_sales,
	  SUM(CASE WHEN region = 'West' THEN sales_amount ELSE 0 END) AS west_sales
	FROM orders GROUP BY customer_id;

	select * from (select customer_id, region, sales_amount from orders) pivot(sum(sales_amount) for region in ('north', 'south', 'east', 'west'));

>> Null doesnt include matching, empty string includes matching
table1: 1 2 3 null ""
table2: 2 2  3 null null ""
	inner join  - 2 2 / 2 2 / 3 3 / empty empty / 
	left join - 1 null / 2 2 / 2 2/ 3 3/ empty empty / null null /

>> Find the second highest salary without using LIMIT, OFFSET, or TOP.:		emp_id, salary
	WITH RankedSalaries AS (SELECT salary, dense_rank() OVER(ORDER BY salary DESC) AS rank FROM employees)
	SELECT salary FROM RankedSalaries WHERE rank = 2;
	SELECT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 1;

>> Given a table of orders, write a query to find the running total (cumulative sum) of revenue for each day.:    order_date, revenue
	SELECT  order_date, revenue, SUM(revenue) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM  orders

>> Write an SQL query to identify employees who earn more than their managers.		EMPID, EMPNAME, MANAGERID, SLARY
	SELECT  e.employee_id,  e.name AS employee_name,  e.salary AS employee_salary,  m.name AS manager_name,  m.salary AS manager_salary 
	FROM  employees e JOIN  employees m ON e.manager_id = m.employee_id WHERE  e.salary > m.salary	ORDER BY  e.salary DESC;

>> Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount.    CustomerID | PurchaseAmount
	WITH RankedPurchases AS (
    		SELECT CustomerID, PurchaseAmount, DENSE_RANK() OVER (ORDER BY PurchaseAmount DESC) AS Rank FROM `your_dataset.your_table`)
	SELECT CustomerID, PurchaseAmount FROM RankedPurchases WHERE Rank <= N;

>> Identify consecutive login streaks for users where they logged in for at least three consecutive days.     user_id | login_date

>> Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.
	WITH ranked_records AS (
	  SELECT *, ROW_NUMBER() OVER (PARTITION BY column1, column2, column3 ORDER BY column1) AS row_num FROM `your_dataset.your_table`)
	DELETE FROM ranked_records WHERE row_num > 1;

>> Retrieve 1 order for each customer, ensuring that ties (customers with multiple 1 orders on the same date) are handled correctly.  customer_id | order_id | order_date
	WITH ranked_orders AS (
	  SELECT customer_id, order_id, order_date, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date, order_id) AS row_num  FROM `your_dataset.orders`)
	SELECT customer_id, order_id, order_date FROM ranked_orders WHERE row_num = 1;

>> Find products that were never purchased by any customer.    t1: product_id, product_name;  t2: product_id, order_id
	SELECT p.product_id, p.product_name FROM `your_dataset.products` p LEFT JOIN `your_dataset.orders` o
	ON p.product_id = o.product_id	WHERE o.product_id IS NULL;

>> Given a table with customer transactions, find customers who made transactions in every month of the year.    cust_id, date, purchage_amount
	WITH monthly_transactions AS (
	  SELECT customer_id, EXTRACT(MONTH FROM transaction_date) AS month FROM `your_dataset.transactions`  GROUP BY customer_id, month),
	customer_months AS (
	  SELECT customer_id, COUNT(month) AS months_count FROM monthly_transactions GROUP BY customer_id)
	SELECT customer_id FROM customer_months WHERE months_count = 12;

>> Find employees who have the same salary as another employee in the same department.    dept_id, emp_name, salary
	WITH EmployeeSalaries AS (
	  SELECT   department_id, salary, COUNT(*) AS employee_count  FROM employees GROUP BY department_id, salary HAVING COUNT(*) > 1)
	SELECT e.employee_id, e.department_id, e.salary	FROM employees e JOIN EmployeeSalaries es ON  e.dept_id = es.dept_id AND e.salary = es.salary;

>> Write an SQL query to retrieve the department with the highest total salary paid to employees.    dept_od, salary 
	SELECT DepartmentID, SUM(Salary) AS TotalSalary FROM `your_dataset.your_table` GROUP BY DepartmentID ORDER BY TotalSalary DESC LIMIT 1;

>> Use a window function to rank orders based on order value for each customer, and return only the top 3 orders per customer.    CustomerID, OrderID, OrderValue
	WITH RankedOrders AS (
	    SELECT CustomerID, OrderID, OrderValue, RANK() OVER (PARTITION BY CustomerID ORDER BY OrderValue DESC) AS Rank FROM `your_dataset.your_table`)
	SELECT CustomerID, OrderID, OrderValue FROM RankedOrders WHERE Rank <= 3;

>> Write a query to pivot a table where each row represents a sales transaction and transform it into a summary format where each column represents a month.
	transaction_id	sale_date	amount
	1	        2025-01-15	100
	year	Jan	Feb	Mar
	2025	250	450	300

	WITH sales_data AS (
	  SELECT EXTRACT(YEAR FROM sale_date) AS year, FORMAT_DATE('%b', sale_date) AS month, amount  FROM `project.dataset.sales`)
	SELECT * FROM sales_data	PIVOT (SUM(amount) FOR month IN ('Jan', 'Feb', 'Mar'))	ORDER BY year;

>> Find the moving average of sales for the last 7 days for each product in a sales table.	product_id, sale_amount, sale_date
	WITH SalesWithMovingAverage AS (SELECT ProductID, SaleDate, SlesAmount, 
		AVG(SalesAmount) OVER (PARTITION BY ProductID ORDER BY SaleDate ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS MovingAverage FROM `your_dataset.sales`)
	SELECT ProductID, SaleDate, SalesAmount, MovingAverage FROM SalesWithMovingAverage ORDER BY ProductID, SaleDate;

>> Given an events table, find the first and last occurrence of each event per user.        UserID,   EventID,  EventDate,
	WITH ranked_events AS (
	  SELECT    user_id, event_type, event_time,
	    ROW_NUMBER() OVER (PARTITION BY user_id, event_type ORDER BY event_time ASC) AS rn_first,
	    ROW_NUMBER() OVER (PARTITION BY user_id, event_type ORDER BY event_time DESC) AS rn_last	  FROM events)
	SELECT user_id, event_type,
	  MIN(CASE WHEN rn_first = 1 THEN event_time END) AS first_occurrence, MIN(CASE WHEN rn_last = 1 THEN event_time END) AS last_occurrence
	  FROM  ranked_events GROUP BY user_id, event_type ORDER BY user_id, event_type;
    
>> Identify users who have placed an order in two consecutive months but not in the third month.	user_id, order_date

>> Find the most frequently purchased product category by each user over the past year. 	user, product_category, order_id, sale_date
	WITH user_category_sales AS (
	  SELECT user_id,  product_category, COUNT(*) AS purchase_count FROM sales_table
	  WHERE sale_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) GROUP BY user_id, product_category),
	ranked_categories AS (
	  SELECT  user_id, product_category, purchase_count, RANK() OVER (PARTITION BY user_id ORDER BY purchase_count DESC) AS rank FROM user_category_sales)
	SELECT user_id, product_category, purchase_count FROM ranked_categories WHERE rank = 1;

>> Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year.   prodct_id, sale_amount, sale_date
	SELECT year, product_id, total_sales, RANK() OVER(PARTITION BY year ORDER BY total_sales DESC) AS rank
	FROM (SELECT EXTRACT(YEAR FROM sale_date) AS year, product_id, SUM(sales_amount) AS total_sales FROM sales_table GROUP BY year,  product_id) AS yearly_sale);

>>
input table:
emp_name emp_dep
abc   it
def   hr
ghi  sales
uio  dev
pqr   sales
jkl   it
output table:
emp_name emp_dep
abc   it
jkl   it
def   hr
ghi  sales
pqr  sales
uio  dev
the output order same as above. give bigquery sql query with custom logic
	WITH CustomOrder AS (SELECT emp_name, emp_dep, 
           CASE WHEN emp_dep = 'it' THEN 1 WHEN emp_dep = 'hr' THEN 2 WHEN emp_dep = 'sales' THEN 3 WHEN emp_dep ='dev'THEN 4 end as emp_dept_sort,
	   ROW_NUMBER() OVER(PARTITION BY emp_dep ORDER BY emp_name) AS row_num  FROM your_table_name)
	SELECT emp_name, emp_dep FROM CustomOrder ORDER BY emp_dept_sort, row_num;

>> Write a query to detect a sudden drop (>30%) in daily revenue for any restaurant. (restaurant_id, revenue_date, daily_revenue)
	lag to previous_revenue, sudden_drop > 30

>> Find the moving average of daily revenue for each restaurant over a 7-day window.	restarant | date | revenue
		AVG(revenue) OVER (PARTITION BY restaurant_id ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_revenue

>> Find restaurants whose best-selling dish makes up over 60% of their revenue. 	restaurant_id, dish_id,  dish_name,  quantity, price
	restaurant_total_revenue , best_selling_dish,   b.best_dish_revenue / r.restaurant_revenue AS revenue_share,  revenue_share > 0.6;

>> Rank users based on their total number of orders and total spend, giving more weight to spend. -- Weighted score: 70% weight to spend, 30% to order count
order_id	user_id		order_amount
1		U001		100
2		U001		150
	    (0.7 * total_spend + 0.3 * total_orders) AS weighted_score
	    RANK() OVER (ORDER BY weighted_score DESC) AS rank
	
>> Identify restaurants with high churn rates: drop in unique monthly customers for 3+ months.
order_id	user_id		restaurant_id	order_date
1		U001		R001		2024-01-15
2		U002		R001		2024-01-20
	WITH monthly_customers AS (SELECT restaurant_id, FORMAT_DATE('%Y-%m', DATE(order_date)) AS month,  COUNT(DISTINCT user_id) AS unique_customers
	  FROM `your_dataset.orders` GROUP BY restaurant_id, month),
	monthly_with_lag AS (SELECT restaurant_id, month, unique_customers, LAG(unique_customers, 1) OVER (PARTITION BY restaurant_id ORDER BY month) AS prev_month_1,
	    LAG(unique_customers, 2) OVER (PARTITION BY restaurant_id ORDER BY month) AS prev_month_2,
	    LAG(unique_customers, 3) OVER (PARTITION BY restaurant_id ORDER BY month) AS prev_month_3 FROM monthly_customers),
	churn_flags AS (SELECT restaurant_id, month, unique_customers, prev_month_1, prev_month_2, prev_month_3,
	    CASE WHEN prev_month_1 IS NOT NULL AND prev_month_2 IS NOT NULL AND prev_month_3 IS NOT NULL AND unique_customers < prev_month_1
	    AND prev_month_1 < prev_month_2 AND prev_month_2 < prev_month_3 THEN 1 ELSE 0 END AS churn_flag FROM monthly_with_lag)
	SELECT DISTINCT restaurant_id FROM churn_flags WHERE churn_flag = 1;

>> Calculate the percentage of abandoned carts (added to cart but not ordered). user & order tables
user_id		item_id		event_type	event_time
U001		I001		add_to_cart	2024-01-01 10:00:00
user_id		item_id		order_id	order_time
U001		I001		O001		2024-01-01 10:10:00
	  ROUND(100 * abandoned_count / total_added, 2) AS abandoned_percentage

>> Find cities where lunch orders (12–3 PM) have higher revenue than dinner orders (7–10 PM).
order_id	user_id		city	order_time		order_amount
1		U001		Delhi	2024-01-01 12:30:00	200
2		U002		Delhi	2024-01-01 19:30:00	150
	order_hour
	CASE WHEN order_hour BETWEEN 12 AND 14 THEN 'lunch'  WHEN order_hour BETWEEN 19 AND 21 THEN 'dinner' ELSE NULL END AS meal_period,
	pivoted_revenue 

>> Detect duplicated orders — same items, user, and restaurant within 5 minutes.
order_id	user_id		restaurant_id	item_id		order_time
1		U001		R001		I001		2024-01-01 10:00:00
2		U001		R001		I001		2024-01-01 10:03:00
3		U002		R002		I002		2024-01-01 11:00:00
	LAG(order_time) OVER (PARTITION BY user_id, restaurant_id, item_id  ORDER BY order_time) AS previous_order_time
	TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) AS minutes_diff
	TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) <= 5
	
>> For each cuisine, identify the most loyal customer (most repeat orders).
order_id	user_id		cuisine		order_date
1		U001		Italian		2024-01-01
	WITH order_counts AS (
	  SELECT   cuisine,    user_id,  COUNT(*) AS order_count FROM `your_dataset.orders` GROUP BY  cuisine, user_id),
	ranked_customers AS (
	  SELECT   cuisine,  user_id, order_count, ROW_NUMBER() OVER (PARTITION BY cuisine ORDER BY order_count DESC) AS rank FROM  order_counts)
	SELECT  cuisine,  user_id AS most_loyal_customer,  order_count	FROM  ranked_customers	WHERE rank = 1;

>> Write a query to return the top 5% of users by revenue contribution (Pareto Principle). (user_id,  revenue)
	   PERCENT_RANK() OVER (ORDER BY revenue DESC) AS revenue_percentile
	   revenue_percentile <= 0.05
	
>> Determine which users are likely bots based on their ordering patterns (e.g., 50+ orders/day). (user, order_timestamp, order_id)
	SELECT
	  user_id, DATE(order_timestamp) AS order_day, COUNT(order_id) AS orders_per_day FROM `your_project.your_dataset.orders`
	GROUP BY  user_id,  order_day HAVING orders_per_day >= 50;

>> Build a time-series summary table that aggregates revenue per day, per restaurant, for dashboarding.     (restaurant_id, order_day, revenue)
	SELECT  restaurant_id, order_day, SUM(revenue) AS daily_revenue FROM  `your_project.your_dataset.orders`
	GROUP BY  restaurant_id,  order_day ORDER BY  order_day,  restaurant_id;

>> Identify duplicate records in a table based on a specific column. (like email)
	select * from `table` where email in (SELECT email FROM `table` group by email having count(*)>1);
	
>> Find the department with the highest average salary. 	dept, salary
	SELECT department,  AVG(salary) AS average_salary FROM `table` group by  department ORDER BY average_salary DESC LIMIT 1;

>> List the top 3 most frequently sold products. (product_name , quantity_sold)
	SELECT  product_name,  SUM(quantity_sold) AS tqs FROM  `sales_data` GROUP BY product_name ORDER BY  tqs DESC LIMIT 3;

>> Fetch employees who were hired in the last 90 days.  (emp_name, hire_date)
	SELECT * FROM  `your_project.your_dataset.employee_data` WHERE hire_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);

>> Question:- Extract the Domain from the Email column in Employee Table.
	substring(email, strpos(email, '@')+1, length-opt) 

