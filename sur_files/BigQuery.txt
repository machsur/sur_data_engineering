================================================================================================================================================
DATE(string in iso) / PARSE_DAT(format, string) / SAFE.PARSE_DATE(format, invalid date)
DATE_DIFF()
extract(month from date)
DATE_TRUNC(order_date, MONTH)
FORMAT_DATE('%B %d, %Y', sale_date) 

=============================================================================================================================================

1. "write a query to how will you get a list of the top ten most expensive queries that have been executed in your project over the past 30 days:"
	SELECT project_id, job_id, user_email, creation_time, total_slot_ms, total_bytes_billed, total_bytes_processed, query FROM `region-eu`.INFORMATION_SCHEMA.JOBS
	WHERE creation_time between TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND CURRENT_TIMESTAMP()
	      AND job_type = "QUERY"
	      AND total_bytes_processed > 0
	ORDER BY total_bytes_billed DESC LIMIT 10;

2. difference b/w CAST, SAFE_CAST 
	If the conversion fails, SAFE_CAST returns NULL instead of throwing an error, allowing the query to continue.

3. Create table with metadata from existing table – use where 1 = 2 or false

4. Union all / Union distinct / Table wildcards: test_*` where _Table_Suffix > 5 / Except distinct / Intersect distinct /

5. 
>> Temporary table/cache: exist only for the specified period. - useful for intermediate data processing and caching query results.
	BEGIN
	  CREATE TEMPORARY TABLE temp_table AS
	  SELECT * FROM `elegant-circle-454709-h5.surtest11.surtt1`;
	END;

	CREATE TEMPORARY TABLE temp_table
	OPTIONS (format = 'AVRO',  uris = ['gs://your-bucket/your-file.avro']);

>> Internal tables: stored in bq - take adv of opti like partitioning & clustering - Ideal for frequently accessed and processed data.
>> External table: Data remains in its original location (e.g., Cloud Storage) - avoid storage costs - Suitable for occasional queries on large datasets.
	CREATE EXTERNAL TABLE my_dataset.my_external_table
	OPTIONS (format = 'CSV',  uris = ['gs://my_bucket/my_file.csv']);

6. Time travel concept: Query the state of the table as it was at a specific timestamp
	create table `your_dataset.your_table11` as
	SELECT * FROM `your_dataset.your_table`
	FOR SYSTEM_TIME AS OF TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)

	bq cp pr_id:mydataset.mytable@7200000 pr_id:mydataset.mytable_restored (millisecounds)


7. Creation of view/materialized view and auth view: efficiently manage and secure your data access in BigQuery
   Create view: virtual tables / view hits base table and display data / frequent updates on base table/ infrequent data access / 
   Materialized view: data stored on disk / better performance/ frequent access on data / infrequent updates on base table /
   Authorized views and authorized materialized views: let you share query results with particular users and groups withoutgiving them access to the underlying source data.
         CREATE VIEW `your_dataset.your_view` AS
         SELECT customer_id, order_date, total_amount FROM `your_dataset.orders` WHERE order_date >= '2025-01-01'

        CREATE MATERIALIZED VIEW `your_dataset.your_materialized_view` AS
        SELECT customer_id, order_date, SUM(total_amount) AS total_spent FROM `your_dataset.orders` GROUP BY customer_id, order_date

8. Partitioning and clustering:
   Partitioning: dividing a table into smaller segments. - single column (time-based & inter range), 4000 max
   clustering: sorts the partition data in storage blocks. - four columns, 
   Improved Query Performance: By scanning only relevant partitions and clustered blocks, queries run faster.
   Cost Efficiency: Reduced data scanned means lower costs, as BigQuery charges based on the amount of data processed.
      CREATE TABLE `your_dataset.your_table`
      PARTITION BY DATE(order_date) / _PARTITIONTIME / RANGE_BUCKET(product_id, GENERATE_ARRAY(1, 1000, 100))
      CLUSTER BY customer_id, product_id
      AS
      SELECT * FROM `source_dataset.source_table`

9. Joins: -  combine rows from two or more tables based on a related column between them.
   INNER JOIN / left join / right join / full join / cross join / self join /
 	SELECT c.CustomerID, c.CustomerName, o.OrderID, o.OrderDate, oi.OrderItemID,
	FROM `your_dataset.customers` AS c 
	JOIN `your_dataset.orders` AS o ON c.CustomerID = o.CustomerID
	JOIN `your_dataset.order_items` AS oi ON o.OrderID = oi.OrderID;

	SELECT * FROM  `project.dataset.table1` AS a CROSS JOIN `project.dataset.table2` AS b;

9.1 table1 --> 1, 2, 2, 3 table2 --> 1, 1, 2, 3, 3, 3
	SELECT * FROM  `project.dataset.table1` a JOIN `project.dataset.table2` AS b 
	on a.id = b.id;  1 1 / 1 1 / 2 2 / 2 2 / 3 3 / 3 3 /3 3 /
	on a.id > b.id;  2 1 / 2 1 / 2 1 / 2 1 / 3 1 / 3 1 / 3 2/
	on a.id < b.id;  1 2 / 1 3 / 1 3 / 1 3 / 2 3 / 2 3 / 2 3/ 2 3 / 2 3 / 2 3 /

10. With statement: also known as Common Table Expressions (CTEs), allows you to define temporary result sets.
   Adv: code more readable, resusebility - referenced in several places within the main SQL query. 
	WITH orders as (select statement)
	select * from orders

11. Pivoting and unpivoting: help in reshaping your data for analysis or reporting
   Pivot: row into columns
	product	| month	| sales
	select product, IFNULL(Jan, 0) as Jan, IFNULL(Feb, 0) as Feb, IFNULL(Mar, 0) as Mar from table 
	pivot(sum(sales) for month in ('Jan', 'Feb', 'Mar'));	
	or use case statement
	select product, 
	sum(case when month = 'Jan' then sales else 0 end) as Jan,
	sum(case when month = 'Feb' then sales else 0 end) as Feb,
	sum(case when month = 'Mar' then sales else 0 end) as Mar,
	from table group by product;

   unpivot: columns into rows
	product | Jan | Feb | Mar
	select product, month, sales from table 
	unpivot(sales for month in (Jan, Feb, Mar));

12. Normalization – avoid redundant data (lesser storage costs)- poor query performance (joining tables)
   De Normalization – redundant data (high storage cost) – better Query performance 
   Why array & structs? – for better query per and lesser storage costs
   
   Arrays – list of items having same data type
   SELECT element FROM mydataset.mytable, UNNEST(my_array) AS element;

   Struct – record with nested fields having different data types
   SELECT person.id, person.name FROM mydataset.mytable;
 
13. Window functions: performing advanced calculations over a set of rows, go beyond simple aggregates or filters
   max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /row_number() / rank() / dense_rank() 
   SUM(column2) OVER (PARTITION BY column1 ORDER BY column2 ROWS BETWEEN UNBOUNDED/2 PRECEDING AND CURRENT ROW) AS rt
   lag(cn) / lead(cn): Accesses data from a previous or subsequent row in the same result set without use of a self-join.
   LAG(column2, 1) OVER (PARTITION BY column1 ORDER BY column2) AS previous_value,
   first_value(cn) / last_value(cn) / nth_value(cn, n) 
   FIRST_VALUE(column2) OVER (PARTITION BY column1 ORDER BY column2) AS first_value,
   ntile(n) – n groups 
   NTILE(4) OVER (PARTITION BY column1 ORDER BY column2) AS quartile
   cum_dist() / percent_rank() 
   CUME_DIST= Number of rows with values less than or equal to the current row / Total number of rows
​   PERCENT_RANK= Rank of current row−1 / Total rows−1
​   Rows consider current row even if duplicates / range considers bottom row of duplicates.

14. Regular expressions: used for pattern matching and string manipulation
	   REGEXP_EXTRACT(column_name, r'pattern') / select regexp_extract('file_name_20250430123056.csv', r'^(.*)_\d{14}\.csv$')
	   REGEXP_EXTRACT_ALL(column_name, r'pattern') / select regexp_extract_all('abc123def456ghi789', r'\d+')
	   REGEXP_REPLACE(column_name, r'pattern', 'replacement') / REGEXP_REPLACE(file_path, r'_\d{14}\.csv$', '_TIMESTAMP.csv') 
	   REGEXP_CONTAINS('abc123def456ghi', r'\d') AS contains_digits
	   REGEXP_SUBSTR('abc123def456ghi', r'\d+', 5, 2) AS first_digits

15. BigQuery slots vs reservations? 
	Slots are units of computational capacity in BigQuery. (CPU and memory)
	Reservations allow you to purchase dedicated slots (called commitments) and allocate them to specific workloads or projects.

15.1. Big Query cost optimization
	   -- bq compute optimization: 
		on-demaned pricing - cost per tb, good for unpredectable workloads
		Capacity Pricing - purchase dedicated slots, charged per slot-hour (different editions like Enterprise and Enterprise Plus, which offer options for one- or 		three-year commitments at discounted rates)- decide based on your workloads.
	   -- BQ data storage – 
		Billing model: logical- data in uncompressed format/ physical- data in compressed format) 
	         # For logical bytes billing:   bq update --dataset --storage_billing_model=LOGICAL my_dataset
	         # For physical bytes billing:  bq update --dataset --storage_billing_model=PHYSICAL my_dataset
	   	Use Table Expiration Settings, Optimize Data Storage Format (cloumner format - paqruet. ORC), Use Long-Term Storage Pricing (> 90 days)
	   -- bq query optimization

16. Optimization of query performance in BigQuery?
	   create dataset in region wehere customer operates
	   Use Partitioned Tables and Clustered Tables
	   use avro format to bigquery data load - google rec compressed avro for quick data load
	   preview option, 
	   select columns instead of *, use truncate instead of delete
	   Join pattern larger table join smaller tables, avoid cross join
	   where class first condition in such way that eliminates most data
	   late aggregation
	   Use WITH Clauses for Subquery Caching (Avoid Repeated Scalar Subqueries)
	   Take Advantage of Query Caching
	   Aggregate Data at the Source: Use Approximate Aggregations: 
	   Create a materialized view based on frequently run queries.
	   Optimize Window Functions
	   Monitor and Tune Query Performance
	   nest repeated data, Denormalize Data Where Appropriate (Use ARRAY Functions for Multi-Value Fields)
  
17. Cost Controls: 
	   budget alerts, Set query usage limits in IAM quotas, Billing reports, Schedule queries during off-peak hours

18. Explain about data modelling concepts( Star and snowflake schema)? 
	Star Schema: Faster query performance due to fewer joins.
	Snowflake Schema: More normalized and reduces data redundancy. May require more joins, potentially impacting query performance.

19.How to get metadata of datasets and tables in BigQuery? 	
	to get metadata about datasets:  SELECT * FROM `project_id`.`region-europe-west2`.INFORMATION_SCHEMA.SCHEMATA;
	to get metadata about tables: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.TABLES;
	to get metadata about columns: SELECT * FROM `project_id`.`dataset_id`.INFORMATION_SCHEMA.COLUMNS;

20. Database, DWH, DataLake
	Database: OLTP system (day to day operational data) - - structured data with defined schema. mysql
	DWH: Stores current and historical data from multiple sources for BI, analysis and reporting. terradata, bq
	DataLake: Stores large volumes of raw data in its native format (Structured, semi-structured, and unstructured data), Big data analytics, machine learning. gcs, hdfs

21. ETL VS ELT


=============================================================================================================================================

1. To find the department names where no employees are found, you can use the following SQL query:
   emp -->> employee_id	employee_name	dept_id
   Department Table -->> dept_id	dept_name
   SELECT d.dept_name FROM dept d LEFT JOIN employee e ON d.dept_id = e.dept_id WHERE e.dept_id IS NULL;

2. Given two tables, find the count of records for Left Outer Join and Inner Join:
Table A: Table B:
1 1
1 1
1 1 
1 
Ans. 12 & 12

3. Give the output for DENSE_RANK() and RANK() functions for the below dataset:
Nums 
85 
85 
80 
75 
75 
70

4. Given a table with column 'Country', select data in the below sequence:
Table: Matches
Country 
India 
Australia 
Pakistan 
Output:
India vs Australia 
India vs Pakistan 
Australia vs Pakistan
Ans. 
    SELECT a.countryc || ' vs ' || b.countryc as country
    FROM
      `powerful-layout-445408-p5.sur_test_ds.countryt` a
    join
      `powerful-layout-445408-p5.sur_test_ds.countryt` b
    on a.countryc > b.countryc;  -----> or <

5. Given two tables, output the result of INNER, LEFT, RIGHT, FULL JOINS.
Table1:
col1 
---- 
1 
1 
Table2:
---- 
b 
a 
1 

6. Find the 777th highest salary from a table.
      SELECT salary FROM 
		(SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num FROM `your_dataset.your_table`) WHERE row_num = 777

7. Bigquery arch
   storage and compute - decoupled/isolated (cost efficient)
   layers:
   colossus - is google distributed file system storage layer in column orientaed - fault tolerance by data replicas across different machines.
   dremel - is compute engine - have root server (co-ordinate mixers & leaf nodes) / mixers (do aggreagtions) / leaf nodes (read data from storage)
   jupiter- high speed n/w connector - to connect dremel with colossus
   borg - scheduling jobs like run queries

8. Identify customers who placed orders in consecutive months.
	cust, order_date, order

         WITH orders AS (
           SELECT customer_id, DATE_TRUNC(order_date, MONTH) AS order_month FROM `your_dataset.your_table`),
         consecutive_orders AS (
           SELECT customer_id, order_month, LEAD(order_month) OVER (PARTITION BY customer_id ORDER BY order_month) AS next_order_month FROM orders)
         SELECT customer_id, order_month, next_order_month FROM consecutive_orders WHERE DATE_DIFF(next_order_month, order_month, MONTH) = 1

9. Query to get the total number of patients per doctor, including unassigned patients.
	docter_id | pationt_id, doctor_id

         SELECT d.doctor_id, COUNT(p.patient_id) AS total_patients FROM `your_dataset.doctors` d  LEFT JOIN `your_dataset.patients` p
         ON  d.doctor_id = p.doctor_id GROUP BY d.doctor_id

10. Handling NULL values in employee salary using the average salary.
	emp_id, salary

         IFNULL(expression, replacement_value) -->> It takes two arguments. If the first argument is NULL, it returns the second argument.
         COALESCE(expression1, expression2, ..., expressionN) -->> It can take multiple arguments. It returns the first non-NULL argument from the list.
         WITH avg_salary AS (
           SELECT AVG(salary) AS average_salary  FROM `your_dataset.your_table`)
         SELECT employee_id,  COALESCE(salary, (SELECT average_salary FROM avg_salary)) AS adjusted_salary  FROM `your_dataset.your_table`

11. Difference between Subquery and Materialized Views.
    Performance: Materialized views can improve query performance by storing precomputed results, while subqueries are recalculated each time the main query runs.
    Storage: Materialized views consume storage space to store the precomputed results, whereas subqueries do not.
    Maintenance: Materialized views need to be refreshed to stay up-to-date with the underlying data, while subqueries always reflect the current state of the data.

12. CTE vs Subquery in SQL and their performance impact.
    CTe: same code used at multiple places, more readability, used recursively

13. Change Data Capture (CDC) -->> identify and capture changes made to data in a database --> It helps keep track of inserts, updates, and deletes in real-time.
      Benefits:
      Real-time updates: Ensures data consistency across systems.
      Efficiency: Reduces the need for full data scans.
      Accuracy: Minimizes errors by capturing changes as they happen.
      how to implement:
      log based --> very efficient, no impact on source system
      query based on timestamp
      triggers load changes into new tables

14.
	Src					
	id	name											
	2	b1							
	4	d								
						
	beforeTgt					
	id	name
	1	a
	2	b
	3	c
	
	after source ingestion Tgt	
	id	name
	1	a
	2	b1
	3	c
	4	d

	MERGE INTO target_table AS T   USING source_table AS S
	ON T.id = S.id
	WHEN MATCHED THEN UPDATE SET T.name = S.name
	WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);

15.
ORDER_DAY         ORDER_ID            PRODUCT_ID   QUANTITY     PRICE
------------------ -------------------- ---------- ---------- ----------
01-JUL-11         O1                  P1                 5          5
02-JUL-11         O8                  P5                 1         50
Get me all products that got sold both the days and the number of times the product is sold.  
	
	PRODUCT_ID  COUNT
	P1            3
	P2            2
	P3            2

	SELECT PRODUCT_ID, COUNT(*) AS COUNT FROM orders WHERE ORDER_DAY IN ('01-JUL-11', '02-JUL-11') GROUP BY PRODUCT_ID HAVING COUNT(DISTINCT ORDER_DAY) = 2;

16. 
INPUT
order_id	customer_id	region	   order_date	  sales_amount
1	        101	        North	   1/1/2022	  100

OUTPUT
customer_id	north_sales	south_sales	East sales	West Sales
101	500	0	700	0
102	0	800	0	1000
103	0	0	500	0
104	0	300	0	500

	SELECT customer_id,
	  SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales,
	  SUM(CASE WHEN region = 'South' THEN sales_amount ELSE 0 END) AS south_sales,
	  SUM(CASE WHEN region = 'East' THEN sales_amount ELSE 0 END) AS east_sales,
	  SUM(CASE WHEN region = 'West' THEN sales_amount ELSE 0 END) AS west_sales
	FROM orders GROUP BY customer_id;

18. Null doesnt include matching, empty string includes matching
table1
1
2
3
null
""

table2
2
2
3
null
null
""

	inner join  - 2 2 / 2 2 / 3 3 / empty empty / 
	left join - 1 null / 2 2 / 2 2/ 3 3/ empty empty / null null /

19. Find the second highest salary without using LIMIT, OFFSET, or TOP.
    emp_id, salary

	WITH RankedSalaries AS (SELECT salary, dense_rank() OVER(ORDER BY salary DESC) AS rank FROM employees)
	SELECT salary FROM RankedSalaries WHERE rank = 2;
	
	SELECT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 1;

20. Given a table of orders, write a query to find the running total (cumulative sum) of revenue for each day.
    order_date, revenue

	SELECT  order_date, revenue, SUM(revenue) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total FROM  orders

21. Write an SQL query to identify employees who earn more than their managers.
	EMPID, EMPNAME, MANAGERID, SLARY
	SELECT  e.employee_id,  e.name AS employee_name,  e.salary AS employee_salary,  m.name AS manager_name,  m.salary AS manager_salary 
	FROM  employees e JOIN  employees m ON e.manager_id = m.employee_id WHERE  e.salary > m.salary	ORDER BY  e.salary DESC;

22. Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount.
    CustomerID | PurchaseAmount
	WITH RankedPurchases AS (
    		SELECT CustomerID, PurchaseAmount, DENSE_RANK() OVER (ORDER BY PurchaseAmount DESC) AS Rank FROM `your_dataset.your_table`)
	SELECT CustomerID, PurchaseAmount FROM RankedPurchases WHERE Rank <= N;

23. Identify consecutive login streaks for users where they logged in for at least three consecutive days.
    user_id | login_date
	WITH login_data AS (
	  SELECT user_id, login_date,
	         LAG(login_date, 1) OVER (PARTITION BY user_id ORDER BY login_date) AS prev_login_date,
	         LAG(login_date, 2) OVER (PARTITION BY user_id ORDER BY login_date) AS prev_prev_login_date
	  FROM `your_dataset.your_table`
	),
	streaks AS (
	  SELECT user_id, login_date,
	         CASE  WHEN DATE_DIFF(login_date, prev_login_date, DAY) = 1 AND DATE_DIFF(prev_login_date, prev_prev_login_date, DAY) = 1 THEN 1  ELSE 0 END AS is_streak
	  FROM login_data)
	SELECT user_id, login_date FROM streaks	WHERE is_streak = 1 ORDER BY user_id, login_date;

24. Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.
	WITH ranked_records AS (
	  SELECT *, ROW_NUMBER() OVER (PARTITION BY column1, column2, column3 ORDER BY column1) AS row_num FROM `your_dataset.your_table`)
	DELETE FROM ranked_records WHERE row_num > 1;

	-- Create a temp table to store unique records, Delete all records from the original table, Insert unique records back into the original table, Drop the temp table

25. Retrieve the first order for each customer, ensuring that ties (customers with multiple first orders on the same date) are handled correctly.
    customer_id | order_id | order_date
	
	WITH ranked_orders AS (
	  SELECT customer_id, order_id, order_date, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date, order_id) AS row_num
	  FROM `your_dataset.orders`)
	SELECT customer_id, order_id, order_date FROM ranked_orders WHERE row_num = 1;

26. Find products that were never purchased by any customer.
    t1: product_id, product_name;  t2: product_id, order_id

	SELECT p.product_id, p.product_name FROM `your_dataset.products` p LEFT JOIN `your_dataset.orders` o
	ON p.product_id = o.product_id	WHERE o.product_id IS NULL;

27. Given a table with customer transactions, find customers who made transactions in every month of the year.
    cust_id, date, purchage_amount

	WITH monthly_transactions AS (
	  SELECT customer_id, EXTRACT(MONTH FROM transaction_date) AS month FROM `your_dataset.transactions`  GROUP BY customer_id, month),
	customer_months AS (
	  SELECT customer_id, COUNT(month) AS months_count FROM monthly_transactions GROUP BY customer_id	)
	SELECT customer_id FROM customer_months WHERE months_count = 12;

28. Find employees who have the same salary as another employee in the same department.
    dept_id, emp_name, salary
	WITH EmployeeSalaries AS (
	  SELECT   department_id, salary, COUNT(*) AS employee_count  FROM employees GROUP BY department_id, salary HAVING COUNT(*) > 1)
	SELECT e.employee_id, e.department_id, e.salary	FROM employees e JOIN EmployeeSalaries es
	ON  e.department_id = es.department_id AND e.salary = es.salary
	ORDER BY e.department_id, e.salary;

29. Write an SQL query to retrieve the department with the highest total salary paid to employees.
    dept_od, salary 
	SELECT DepartmentID, SUM(Salary) AS TotalSalary FROM `your_dataset.your_table` GROUP BY DepartmentID ORDER BY TotalSalary DESC LIMIT 1;

30. Use a window function to rank orders based on order value for each customer, and return only the top 3 orders per customer.
    CustomerID, OrderID, OrderValue
	WITH RankedOrders AS (
	    SELECT CustomerID, OrderID, OrderValue, RANK() OVER (PARTITION BY CustomerID ORDER BY OrderValue DESC) AS Rank FROM `your_dataset.your_table`)
	SELECT CustomerID, OrderID, OrderValue FROM RankedOrders WHERE Rank <= 3;

31. Find the median salary of employees using SQL (without using built-in median functions).

	SELECT PERCENTILE_CONT(salary, 0.5) OVER() AS median_salary FROM `project.dataset.employees` LIMIT 1;
	or
	WITH OrderedSalaries AS (
	    SELECT Salary, ROW_NUMBER() OVER (ORDER BY Salary ASC) AS RowAsc, ROW_NUMBER() OVER (ORDER BY Salary DESC) AS RowDesc FROM `your_dataset.employees`),
	MedianSalary AS (
	    SELECT Salary FROM OrderedSalaries WHERE RowAsc = RowDesc OR RowAsc + 1 = RowDesc)
	SELECT AVG(Salary) AS MedianSalary FROM MedianSalary;

32. Write a query to pivot a table where each row represents a sales transaction and transform it into a summary format where each column represents a month.
	transaction_id	sale_date	amount
	1	        2025-01-15	100
	year	Jan	Feb	Mar
	2025	250	450	300

	WITH sales_data AS (
	  SELECT EXTRACT(YEAR FROM sale_date) AS year, FORMAT_DATE('%b', sale_date) AS month, amount  FROM `project.dataset.sales`)
	SELECT * FROM sales_data	PIVOT (SUM(amount) FOR month IN ('Jan', 'Feb', 'Mar'))	ORDER BY year;

33. Find the moving average of sales for the last 7 days for each product in a sales table.
	product_id, sale_amount, sale_date

	WITH SalesWithMovingAverage AS (
	    SELECT ProductID, SaleDate, SlesAmount, AVG(SalesAmount) OVER (PARTITION BY ProductID ORDER BY SaleDate ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS 		    MovingAverage FROM `your_dataset.sales`)
	SELECT ProductID, SaleDate, SalesAmount, MovingAverage FROM SalesWithMovingAverage ORDER BY ProductID, SaleDate;

34. Given an events table, find the first and last occurrence of each event per user.
        UserID,   EventID,  EventDate,
        
35. Identify users who have placed an order in two consecutive months but not in the third month.
	user_id, order_date

	WITH MonthlyOrders AS (
	    SELECT UserID, FORMAT_TIMESTAMP('%Y-%m', OrderDate) AS OrderMonth FROM `your_dataset.orders` GROUP BY UserID, OrderMonth),
	ConsecutiveMonths AS (
	    SELECT UserID, OrderMonth,
	        LAG(OrderMonth, 1) OVER (PARTITION BY UserID ORDER BY OrderMonth) AS PreviousMonth,
	        LEAD(OrderMonth, 1) OVER (PARTITION BY UserID ORDER BY OrderMonth) AS NextMonth
	    FROM MonthlyOrders),
	FilteredUsers AS (
	    SELECT UserID, OrderMonth FROM ConsecutiveMonths WHERE 
	        PreviousMonth IS NOT NULL AND NextMonth IS NOT NULL
	        AND DATE_SUB(PARSE_TIMESTAMP('%Y-%m', OrderMonth), INTERVAL 1 MONTH) = PARSE_TIMESTAMP('%Y-%m', PreviousMonth)
	        AND DATE_ADD(PARSE_TIMESTAMP('%Y-%m', OrderMonth), INTERVAL 1 MONTH) = PARSE_TIMESTAMP('%Y-%m', NextMonth)
	)
	SELECT UserID	FROM FilteredUsers GROUP BY UserID HAVING COUNT(OrderMonth) = 2;

37. Find the most frequently purchased product category by each user over the past year.
	user, product_category, order_id, sale_date

	WITH user_category_sales AS (
	  SELECT user_id,  product_category, COUNT(*) AS purchase_count FROM sales_table
	  WHERE sale_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) GROUP BY user_id, product_category),
	ranked_categories AS (
	  SELECT  user_id, product_category, purchase_count, RANK() OVER (PARTITION BY user_id ORDER BY purchase_count DESC) AS rank FROM user_category_sales)
	SELECT user_id, product_category, purchase_count FROM ranked_categories WHERE rank = 1;

38. Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year.
	prodct_id, sale_amount, sale_date

	SELECT year, product_id, total_sales, RANK() OVER(PARTITION BY year ORDER BY total_sales DESC) AS rank
	FROM (
	  SELECT EXTRACT(YEAR FROM sale_date) AS year, product_id, SUM(sales_amount) AS total_sales FROM sales_table
	  GROUP BY year,  product_id) AS yearly_sales
	ORDER BY  year, rank;

39. SCD: dimensions that change over time
	SCD0 - NO CHANGE - change in source, no update on DWH table - change column is not relavant to the DWH table anymore (ex: fax number)	
	SCD1 - maintain latest snapshot, no histroy and its overwritten by latest snap (ex: emp name designation has wrongly entered)
	SCD2 - every change in source, add new record in DWH table and flag status is Y indicating active record
	SCD3 - not rec
	SCD4 - scd1+scd2 - 
	scd6 - 1+2+3 - 
	

41.
input table:
emp_name emp_dep
abc   it
def   hr
ghi  sales
uio  dev
pqr   sales
jkl   it
output table:
emp_name emp_dep
abc   it
jkl   it
def   hr
ghi  sales
pqr  sales
uio  dev
the output order same as above. give bigquery sql query with custom logic

	WITH CustomOrder AS 
	  (SELECT emp_name, emp_dep, 
		CASE WHEN emp_dep = 'it' THEN 1 WHEN emp_dep = 'hr' THEN 2 WHEN emp_dep = 'sales' THEN 3 WHEN emp_dep = 'dev'  THEN 4 end as emp_dept_sort,
		ROW_NUMBER() OVER(PARTITION BY emp_dep ORDER BY emp_name) AS row_num  FROM your_table_name)
	SELECT emp_name, emp_dep FROM CustomOrder ORDER BY emp_dept_sort, row_num;


48. Identify users who increased their average order value consistently over the last 3 months.

49. Write a query to detect a sudden drop (>30%) in daily revenue for any restaurant.
	WITH revenue_changes AS (
	  SELECT
	    restaurant_id,
	    revenue_date,
	    daily_revenue,
	    LAG(daily_revenue, 1) OVER (PARTITION BY restaurant_id ORDER BY revenue_date) AS previous_revenue
	  FROM
	    `your_project.your_dataset.restaurant_revenue`
	),
	sudden_drop AS (
	  SELECT
	    restaurant_id,
	    revenue_date,
	    daily_revenue,
	    previous_revenue,
	    (previous_revenue - daily_revenue) / previous_revenue * 100 AS drop_percentage
	  FROM
	    revenue_changes
	  WHERE
	    previous_revenue IS NOT NULL AND
	    (previous_revenue - daily_revenue) / previous_revenue * 100 > 30
	)
	SELECT
	  restaurant_id,
	  revenue_date,
	  daily_revenue,
	  previous_revenue,
	  drop_percentage
	FROM
	  sudden_drop;

50. For each user, calculate their order streak — consecutive days with at least one order.


51. Find the moving average of daily revenue for each restaurant over a 7-day window.
	restarant | date | revenue
		SELECT
		  restaurant_id,
		  date,
		  revenue,
		  AVG(revenue) OVER (
		    PARTITION BY restaurant_id
		    ORDER BY date
		    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
		  ) AS moving_avg_revenue
		FROM
		  your_table_name
		ORDER BY
		  restaurant_id, date;

52. Detect anomalies in delivery times using the IQR method (Interquartile Range) in SQL.

53. Calculate month-over-month growth in active users for each city.
	user_id	| city	| activity_date
		WITH monthly_active_users AS (
		  SELECT
		    city,
		    EXTRACT(YEAR FROM activity_date) AS year,
		    EXTRACT(MONTH FROM activity_date) AS month,
		    COUNT(DISTINCT user_id) AS active_users
		  FROM
		    user_activity
		  GROUP BY
		    city, year, month
		),
		growth_calculation AS (
		  SELECT
		    city,
		    year,
		    month,
		    active_users,
		    LAG(active_users) OVER (
		      PARTITION BY city
		      ORDER BY year, month
		    ) AS prev_month_active_users
		  FROM
		    monthly_active_users
		)
		SELECT
		  city,
		  year,
		  month,
		  active_users,
		  prev_month_active_users,
		  (active_users - prev_month_active_users) / prev_month_active_users * 100 AS month_over_month_growth
		FROM
		  growth_calculation
		WHERE
		  prev_month_active_users IS NOT NULL
		ORDER BY
		  city, year, month;


54. Find restaurants whose best-selling dish makes up over 60% of their revenue.
	restaurant_id, dish_id,    dish_name,  quantity, price

	WITH dish_revenue AS (
	  SELECT
	    restaurant_id,
	    dish_id,
	    dish_name,
	    SUM(quantity * price) AS total_revenue
	  FROM
	    `your_dataset.orders`
	  GROUP BY
	    restaurant_id, dish_id, dish_name
	),
	
	restaurant_total_revenue AS (
	  SELECT
	    restaurant_id,
	    SUM(total_revenue) AS restaurant_revenue
	  FROM
	    dish_revenue
	  GROUP BY
	    restaurant_id
	),
	
	best_selling_dish AS (
	  SELECT
	    restaurant_id,
	    dish_name,
	    MAX(total_revenue) AS best_dish_revenue
	  FROM
	    dish_revenue
	  GROUP BY
	    restaurant_id, dish_name
	),
	
	combined AS (
	  SELECT
	    b.restaurant_id,
	    b.dish_name AS best_selling_dish,
	    b.best_dish_revenue,
	    r.restaurant_revenue,
	    b.best_dish_revenue / r.restaurant_revenue AS revenue_share
	  FROM
	    best_selling_dish b
	  JOIN
	    restaurant_total_revenue r
	  ON
	    b.restaurant_id = r.restaurant_id
	)
	
	SELECT
	  restaurant_id,
	  best_selling_dish,
	  best_dish_revenue,
	  restaurant_revenue,
	  revenue_share
	FROM
	  combined
	WHERE
	  revenue_share > 0.6;

55. Identify users who place high-value orders only on weekends.
     user_id,     order_id,	    order_amount,    order_date

	WITH orders AS (
	  SELECT 
	    user_id,
	    order_id,
	    order_amount,
	    EXTRACT(DAYOFWEEK FROM order_date) AS day_of_week
	  FROM 
	    `your_dataset.orders_table`
	),
	
	-- Identify users who placed high-value orders on weekends
	weekend_high_value_orders AS (
	  SELECT DISTINCT user_id
	  FROM orders
	  WHERE 
	    order_amount > 1000 AND
	    day_of_week IN (1, 7)  -- 1 = Sunday, 7 = Saturday
	),
	
	-- Identify users who placed any orders on weekdays
	weekday_orders AS (
	  SELECT DISTINCT user_id
	  FROM orders
	  WHERE 
	    day_of_week BETWEEN 2 AND 6  -- Monday to Friday
	)
	
	-- Final result: users who placed high-value orders only on weekends
	SELECT user_id
	FROM weekend_high_value_orders
	WHERE user_id NOT IN (SELECT user_id FROM weekday_orders);

56. Compare the conversion rate of users who visited the app vs. those who ordered.
	user_id, event_type(visit)

	WITH app_visits AS (
	  SELECT DISTINCT user_id
	  FROM `your_dataset.app_events`
	  WHERE event_type = 'visit'
	),
	
	orders AS (
	  SELECT DISTINCT user_id
	  FROM `your_dataset.orders`
	),
	
	conversion_analysis AS (
	  SELECT
	    COUNT(DISTINCT v.user_id) AS total_visitors,
	    COUNT(DISTINCT o.user_id) AS total_converted
	  FROM
	    app_visits v
	  LEFT JOIN
	    orders o
	  ON
	    v.user_id = o.user_id
	)
	
	SELECT
	  total_visitors,
	  total_converted,
	  ROUND(total_converted / total_visitors * 100, 2) AS conversion_rate_percentage
	FROM
	  conversion_analysis;

57. Determine the average time between sign-up and first order, grouped by acquisition channel.
	user_id,     acquisition_channel,    signup_date
	user_id,    order_date

	WITH user_signups AS (
	  SELECT
	    user_id,
	    acquisition_channel,
	    signup_date
	  FROM
	    `your_dataset.users`
	),
	
	first_orders AS (
	  SELECT
	    user_id,
	    MIN(order_date) AS first_order_date
	  FROM
	    `your_dataset.orders`
	  GROUP BY
	    user_id
	),
	
	signup_to_order AS (
	  SELECT
	    u.acquisition_channel,
	    u.signup_date,
	    f.first_order_date,
	    DATE_DIFF(f.first_order_date, u.signup_date, DAY) AS days_to_first_order
	  FROM
	    user_signups u
	  JOIN
	    first_orders f
	  ON
	    u.user_id = f.user_id
	  WHERE
	    f.first_order_date IS NOT NULL
	)
	
	SELECT
	  acquisition_channel,
	  AVG(days_to_first_order) AS avg_days_to_first_order
	FROM
	  signup_to_order
	GROUP BY
	  acquisition_channel
	ORDER BY
	  avg_days_to_first_order;

11/ Use a recursive CTE to analyze multi-level referral networks.
12/ Rank users based on their total number of orders and total spend, giving more weight to spend.
13/ Identify restaurants with high churn rates: drop in unique monthly customers for 3+ months.
14/ Calculate the percentage of abandoned carts (added to cart but not ordered).
15Find cities where lunch orders (12–3 PM) have higher revenue than dinner orders (7–10 PM).
16/ Detect duplicated orders — same items, user, and restaurant within 5 minutes.
17/ For each cuisine, identify the most loyal customer (most repeat orders).
18/ Write a query to return the top 5% of users by revenue contribution (Pareto Principle).
19/ Determine which users are likely bots based on their ordering patterns (e.g., 50+ orders/day).
20/ Build a time-series summary table that aggregates revenue per day, per restaurant, for dashboarding.


>> Identify duplicate records in a table based on a specific column.

>> Find the department with the highest average salary.
	dept, salary

	SELECT
	  department,
	  AVG(salary) AS average_salary
	FROM
	  `your_project.your_dataset.employee_data`
	GROUP BY
	  department
	ORDER BY
	  average_salary DESC
	LIMIT 1;


>> List the top 3 most frequently sold products.
product_name , quantity_sold

	SELECT
	  product_name,
	  SUM(quantity_sold) AS total_quantity_sold
	FROM
	  `your_project.your_dataset.sales_data`
	GROUP BY
	  product_name
	ORDER BY
	  total_quantity_sold DESC
	LIMIT 3;

>> Fetch employees who were hired in the last 90 days.
	SELECT
	  *
	FROM
	  `your_project.your_dataset.employee_data`
	WHERE
	  hire_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);


>> Question:- Extract the Domain from the Email column in Employee Table.
	substring(email, strpos(email, '@'), length-opt) 

