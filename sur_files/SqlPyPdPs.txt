===========
>>  execution order
SQL: FROM: JOIN: WHERE: GROUP BY: Aggregate: HAVING: SELECT: ORDER BY: LIMIT: 
    SELECT d.department, COUNT(e.id) AS employee_count 
    FROM `project.dataset.employees` e JOIN `project.dataset.departments` d ON e.department_id = d.id
    WHERE e.age > 30 GROUP BY d.department HAVING COUNT(e.id) > 1 ORDER BY d.department LIMIT 10;
    
Pandas: Read Data: Merge: Filter Rows: Select Columns: Group By: Aggregate: Filter Groups: Sort: Limit:
    join_df = pd.merge(employees, departments, left_on='department_id', right_on='id', how='left')
    filtered_df = df[df['age'] > 30]
    selected_df = filtered_df[['name', 'department']]
    grouped_df = selected_df.groupby('department').size().reset_index(name='count')
    having_df = grouped_df[grouped_df['count'] > 1]
    sorted_df = having_df.sort_values(by='department')
    result = sorted_df.head(10)
    print(result)
PySpark: read : Join : filter : groupBy: agg : filter : select : orderBy : Limit: 
    df_employees = spark.createDataFrame(employees) 
    outer_join_df = df_employees.join(df_departments, df_employees.department_id == df_departments.id, how='outer')
    filtered_df = joined_df.filter(joined_df['age'] > 30)  ---> joined_df['age'] or joined_df.age or col('age')
    grouped_df = filtered_df.groupBy('department').count()
    having_df = grouped_df.filter(grouped_df['count'] > 1)
    selected_df = having_df.select('department', 'count')
    sorted_df = selected_df.orderBy(asc('department'))
    result = sorted_df.limit(10)
    result.show()

>>  union tables: 
	SELECT column1 FROM table1 UNION DISTINCT/UNION ALL/EXCEPT DISTINCT/INTERSECT DISTINCT SELECT column1 FROM table2;
	union_distinct = pd.concat([df1, df2]).drop_duplicates()  / union_all = pd.concat([df1, df2])
	union_distinct = df1.union(df2).distinct()  /union/subtract/intersect/


>> Requirements:
	select columns, list columns, add new column, drop columns, column name change, column type change
	filter dataframe, null values, non-null values ---> df.filter(df['cn'] > 10 / .isNull() / .isNotNull())
	df distanct values, count, drop duplicates ----> df.select('cn1', 'cn2').distinct().count() ----> df.dropDuplicates(['cn1', 'cn2'])
	agg functions --> df.select(sum('CN')).show() -----> max, min, avg, count, countDistanct  ------> note: ignore null values
	grouping and agg -----> df.groupBy('CN').agg(sum('CN')).show() -------> min, max, avg, count
	captialization -----> df.select(initcap('CN'))----------> lower, upper 
	triming spaces, padding values (ltrim, rteim, trim, lpad, rpad)
	result_df = df.select( col("EmployeeID"), ltrim(col("Name")).alias("ltrim_Name"), lpad(col("Name"), 10, "X").alias("lpad_Name"))
	splitng column, size of array, indexing, explode, array_contains  ---------> df = df.select(split(df.FullName, " ").alias('cn'))
	concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName))
	create date, timestamp columns in df, add/subtract days, 


=====================
Python:

>> Interpreter: The programme that runs your code.
   Virtual Environment: A separate "workspace" that uses a Python interpreter but has its own packages and dependencies.

>> configure environment variable path - so that execute programme from terminal

>> pip - tool for install python packages
   pypi - centralized repository for py packages
>>  
    python -m venv myenv                   --- Create a Virtual Environment
    or
    pip install virtualenv 
    virtualenv myenv                     ----- Create a Virtual Environment
    myenv\Scripts\activate               ---- Activate the Virtual Environment
    deactivate                            ------ Deactivate the Virtual Environment
    pip install pandas
    pip freeze > requirements.txt             ---- download req file from venv
    pip install -r requirements.txt           ---- navigate to file loc and install into venv

>> exit(0) - terminate the programme
    #!/usr/bin/python3.5 : This is a special sequence that tells the operating system to execute the file using the             specified program or interpreter.

>> logging: avoid print - avoid garbage values in prod - helps in debugging, disable/enable logging easily
    DEBUG (10): Example: Variable values during execution.
    INFO (20): Example: Confirmation that a function has started or completed.
    WARNING (30):Example: Deprecated features, nearing disk space limit.
    ERROR (40):Example: Failed database connections.
    CRITICAL (50):Example: System crashes or unavailable resources.

>> Hiding Passwords and Secret Keys in Environment Variables (linux/mac/Windows) and access them into code -------> os.environ.get("key")
 
>> unique identifiers (UUIDs)

>> requests module

>> try: / except ZeroDivisionError as e: # Handling specific exception (division by zero) /  except Exception as e:
    # Handling any other exceptions / else: # Code to execute if no exception occurred / finally:

>> with open('sur.log', 'r') as file:

>> Types of Config Files 
    .ini or .cfg or .properties files - often used with configparser.
    JSON files: Lightweight and hierarchical, ideal for structured data.
    YAML files: Human-readable and popular for complex configurations.
    Environment files (.env): For environment variables, often used in web development for sensitive settings.
    
        Format	    Library	                                            Command
        .ini/.cfg	configparser	                                    import configparser
        JSON	    Built-in json	                                    import json
        YAML	    PyYAML (install: pip install pyyaml)	            import yaml
        .env	    python-dotenv (install: pip install python-dotenv)	from dotenv import load_dotenv

>> Modules: re-use, no impact on other modules
    datetime module: used to work with dates
    math module: to utilize built-in functions
    user input: we can ask user to provide input
    string format method

>> Handlling errors: 
    compile time error: Syntatical - missing :
    Logical error: wrong output 2+3 = 7
    run time error: not running for specific time (mistake use done by user: 5/0) - as a devloper u have to understand user     prospective 

>>  break / continue / pass

>> if, if else, nested if, if elif else / while loop / for loop / for else

>> List, tuple, set, dictionary, array

>> functions: reuse block of code / 
    Formal arguments:
    Actuval arguments:
    positional / keyworded / variable length: def sum(a, *b): / keyword variable length: def person(name, **data):  /
    Global variable (outside fun) / local variable (inside fun)
    recursion: function calling it self ex: factorial
    anonymous function: x = lambda a: a * a / print(x(5))
    filter: list(filter(lambda a: a%2 == 0, lt))
    map: list(map(lambda a: a * 2, lt))
    reduce: from functools import reduce / sum = reduce(lambda a, b: a + b, lt) /

>> create python packages
    -- package python code as .whl file
    -- setuptools (build and package projects), whell (create & install .whl files), twine (upload packages to pypi)
    -- create code, __init__.py, setup.py 
        python setup.py bdist_wheel - created wheel file
        install whl package

>> memory shared tasklist of each programe, python programme is one of task list - within this portion memory used.
    Threading: by default, the python programee executes on single threading
    Multi threading: no control in execution, more memory and faster execution - NO (program execute on one thread and             other threads are pause)

=================================
NumPY and Pandas:

>>  Array: same data type (1-D) - Simple, basic collections (like lists) for small collections.
    NumPy: N-D - leniar algebra library - numerical calculations for large datasets.
    Pandas: Data analysis library for handling and manipulating structured data, great for dataframes (tables), series.

>> Data scientist or data analytics - Answer the quetions on large data points

>> why Python Pandas
    Excel - Can not handle large sets of data
    Python - Need to write code to find answers
    Pandas dataframe: its python modules - makes data scientist life easier and effective

>> Series is very similar to a NumPy array. 
    Series can have axis labels. 
    it can hold any arbitrary Python Object.
    Dataframe : combination of series.


Create dataframe:
labels = ['a','b','c']
my_list = [10,20,30]
arr = np.array([10,20,30])
d = {'a':10,'b':20,'c':30}

df = pd.read_csv('Salar2.csv')
df1 = client.query(sql).to_dataframe(create_bqstorage_client=False)
df2 = pd.DataFrame(data = [['sure', 20], ['ses', 25], ['siv', 30]], index = ['a', 'b', 'd'], columns = ['name', 'age']) or
from numpy.random import randn
df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split())

----------

df.head(2)
df.set_index("Id", inplace=True)
df = df.reindex(new_index)

df['W'] / df[['W', 'Y']] / df['new'] = df['W'] + df['Y']
df.loc['C'] / df.loc[['C', 'D']] / df.loc['C', 'X'] / df.loc[['C', 'E'], ['X', 'Z']]
type(df) / type(df['BasePay'])
df.drop('new',axis=1,inplace=True)
df[df['W']>0][['Y','X']]
df[(df['W']>0) &| (df['Y'] > 1)]

>>  Cleaning Missing values:
    df.dropna(axis=0, how='any', thresh=2, subset=['col1', 'col2'], inplace=False)
    df.fillna(value=2 or df['col1'].mean() or {'col1': 2, 'col2': 4}, method='ffill', axis=0, inplace=False, limit=2)
    df.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=None)

>>  df.groupby('Company')['column'].mean()/std()/min()/max()/count()
    df.groupby('Company').describe().transpose()

>> combining data frames: concat, merge, join
    pd.concat([df1,df2,df3], axis=1) 
    pd.merge(left, right, how='outer/inner/left/right', on=['key1', 'key2']) - based on keys
    left.join(right, how='outer') - combining the columns of two potentially differently-indexed DataFrames 

>> df['col2'].unique() / df['col2'].nunique() / df['col2'].value_counts()
    df.drop_duplicates(subset='Name') / df.drop_duplicates(subset=['col2'], keep='last').reset_index(drop=True) / df.duplicated().sum() / 
    newdf = df[(df['col1']>2) & (df['col2']==444)]
    df['col1'].apply(lambda x: x * 2 (or) times2 function) /  df['col3'].apply(len) /
    del df['col1'] / df.columns / df.index /  df.isnull() /
    df.sort_values(by='col2') #inplace=False by default /
    df.pivot_table(values='D',index=['A', 'B'],columns=['C'])

>> Spiting:

df[['fn', 'ln']] = df['EmployeeName'].str.split(' ', expand = True)
or
df[['ffn', 'lln']] = df['EmployeeName'].apply(lambda x: pd.Series(str(x).split(' ')))

df.head(2)


---------------



--------------
data type casting:

df["Date"] = pd.to_datetime(df["Date"], format='%d/%m/%Y') or df["Date"] = df["Date"].astype('datetime64[ns]')
df["Id"] = df["Id"].astype(str)
df["Id"] = df["Id"].astype(float)
df["Id"] = df["Id"].astype(int)

----------------

cleaning wrong data:

df.loc[5, 'BasePay'] = 110000

for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.drop(i, inplace = True)
        
for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.loc[i, 'BasePay'] = 10000
        

=======================================

df.duplicated()
df.drop_duplicates(inplace = True)

=======================================

df2.corr()

============================

import matplotlib.pyplot as plt
df2.plot(kind='scatter', x = 'Year', y = 'BasePay')
plt.show()

===========================
data structures (containers storing data in memory) + code instructions = software apply
selecting right data structure for a given problem - makes efficient programme

bigo - measure running time or space rwuirements for programme as input size grows

time complexity (o(1) // log(n) //  o(n) // o(n2) //

================
Python / pyspark /
===============
https://github.com/itversity
https://github.com/dgadiraju?tab=repositories
https://spark.apache.org/

>> read data and process using python - not recommend these low level apis (use for reading schemas/properties)
>> read data and process using python pandas -  recommend for small/medium datasets

>>  Bigdata and datalakes:

-- databases / use cases / tchnologies / volume: 
    RDBMS - order management system, point of sales, transaction involve b/w two parties - oracle, sql server, mysql,       postgres , IBM DB2  - small to medium
    Data lake - low-cost storage (data from all relavant sources)- GCS, S3, ADLS - high
    data warehouse - for reporting purpose - snowflake, teradata, databricks (these 3 available on all cloud), oracle exadata, BQ, Amazon redshift, Azure Synapse - medium (typical) to high
    no sql - operational stroes (chats, products catalog in Amazon, endorsements, recommendations) - Bigtable, Amazon Dynamodb, Azusre(multiple), casandra, mangodb - medium (typical) to high
    graph - network -ne04j - small to medium
    seach -search quickly - elastic search, solr, lucene - medium

    small - few hundred of gb
    medium - under 1 tb
    high - few gundred of tb / pb

-- Big data: 
    characteristics - volume, varity, velocity
    for google search engine, introduced 3 tech GFS, GMR, big table - popular and published white paper in 2000 
    based on white papers, hadoop and then spark came into picture.

-- Google white papers------> Haddop ecosystem------------> Spark

-- HES: tech - HDFS based on GFS, HMR based on GMR, HBASE based on GBT, later introduced Sqoop/dead, flume/dead, hive, oozie - doest use memory efficiently and cloud -so, spark came into picture

    HDFS - large datasets distribute on multiple machines - replicate them for fault tolerance
    map reduce - batch jobs break down - process them across cluster - combine them / HBASE - nosql for real-time access to data / both will provide compute capabilities on top of hdfs
    Sqoop - import and export data from/to structured databases into haddop - underhood it uses map reduce
    Hive - querying and manage large datasets using sql like lang HiveQL - underhood it uses map reduce
    Flume - move large vol of web server log, event data to HDFS or Hbase
    Oozie - workflow scheduling tool for Hadoop jobs - underhood it uses map reduce

    challemges - steep learining curve / setup time and cost / maintanance of cluster / application development life cycle / ineeficient usage of cluster / slow performance during data processing
    
    These challenges in HES (datalake using HES) are overcome by datalake using cloud

//////////////////////////////////////

20. Overview of spark and its architexture

-- data processing: source (files,db, rest payloads) -----> process (sql, libraries/pandas/dask, frameworks/spark) ------->
    target(files,db, rest payloads)

-- data processing libraries: python pandas/limitations whth large datasets - introduces python dask 
(uses server efficiently and multi nodes)  - pyspark works well spark clusers with multiple nodes when compared to dask 

-- distributed computung: ex: hadoop and spark
    single server (etl) - problem is sla in prod and capcity issues even if you use efficiently with multithreading
    cluster (have multiple server)

-- databricks have spark based runtime - AWS (EMR & GLUE), both spark embedded in them - in gcp dataproc, we got hadoop and spark - Azure synaps, spark embedded it - 
    snow flake most popular DWH they introduced snowpark based on spark

-- spark: multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
    key features: batch/streaming data - sql analytics  - datascience at scale - machine learning
    architexture: https://spark.apache.org/docs/latest/cluster-overview.html
    cluster capacity, spark cluster: cluster(D,W) nodes/OS/Spark engine/executors(JavaVM) in worker, vcpu or slots unused capacity, tasks used capacity/network/


=====================
- problem: big data processing - cant handle by excel , sql db, etc.
- Sol : parallel processing - spark 
        https://spark.apache.org/
- Spark : 
        > dirver node (spark contest) - cluster/resource manager - worker nodes (executor, task, cache).
        > ram in worker nodes has divided into partitions - these partitions can be inside specific data structure RDD (write code in py) / DF (strucre databases).
        > RDD - read only - immutable - fault tolerant using DAG concept - 1. transformation (new rdd - not human readble) 2. action (to read)

- Platforms for pyspark:
        > https://colab.research.google.com/
        > VM vare on desktop version
        > databricks community edition / on cloud
        > dataproc on GCP


why PySpark not python for bigdata? :
python - take sample of data and apply statistics to develop histograms
pyrhon - can process large datasets with multithreads 
problem - when data is big and stays in memory / we cant take entire data on compute and use python on it.
        - data cant stay in one computer and it can be distributed.
=========
Apache spark:
- opensource distributed processing system for big data.
- in-memory caching
- uses optimized query execution for fast queries
- provides api in java, scala, python, r
- supports batch processing, inetractive queries, real-time analytics, ml and graph processing



