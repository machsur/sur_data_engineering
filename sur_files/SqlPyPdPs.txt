===========
>>  execution order
SQL: FROM: JOIN: WHERE: GROUP BY: Aggregate: HAVING: SELECT: ORDER BY: LIMIT: 
    SELECT d.department, COUNT(e.id) AS employee_count 
    FROM `project.dataset.employees` e JOIN `project.dataset.departments` d ON e.department_id = d.id
    WHERE e.age > 30 GROUP BY d.department HAVING COUNT(e.id) > 1 ORDER BY d.department LIMIT 10;
    
Pandas: Read Data: Merge: Filter Rows: Select Columns: Group By: Aggregate: Filter Groups: Sort: Limit:
    join_df = pd.merge(employees, departments, left_on='department_id', right_on='id', how='left')
    filtered_df = df[df['age'] > 30]
    selected_df = filtered_df[['name', 'department']]
    grouped_df = selected_df.groupby('department').size().reset_index(name='count')
    having_df = grouped_df[grouped_df['count'] > 1]
    sorted_df = having_df.sort_values(by='department')
    result = sorted_df.head(10)
    print(result)
PySpark: read : Join : filter : groupBy: agg : filter : select : orderBy : Limit: 
    ==>>
    df = spark.createDataFrame(data, columns)
    df_from_pandas = spark.createDataFrame(pandas_df/data_dict)
    df_empty = spark.createDataFrame([], columns)
    df_csv = spark.read.csv("/path/to/file.csv", header=True, inferSchema=True)
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
    custom_schema = StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("age", IntegerType(), True)])
    custom_schema = ''' ID Integer, Name String, Age Integer, Salary Double '''
    df = spark.read.csv("your_file.csv", schema=custom_schema, header=True)
    file_paths = ["file1.csv", "file2.csv", "file3.csv"] 
    df = spark.read.csv(file_paths, header=True, inferSchema=True, sep=',')
    ==>> join_df = df1.join(df2, df1.department_id == df2.id/on='cn, how='inner/left/right/outer/left_semi/left_anti') 
         join_df = df1.crossjoin(df2)
         broadcast_join = df1.join(broadcast(df2), on="id", how="inner")
    ==>> filtered_df = df.filter(df['cn'] > 10 / .isNull() / .isNotNull())  or df.filter((df["Age"] > 30) &| (df["Department"] == "IT"))
    ==>> grouped_df = df.groupBy('department').count()   --> max("sal") / min("sal") / avg("sal") / sum("sal")
    ==>> having_df = df.filter(grouped_df['count'] > 1)
    ==>> selected_df = df.select("customer_id", col("Name").alias('EmployeeName'), column("last_name"), df.email, df["city"] )
                    = df.selectExpr("Name as EmployeeName", "Salary as EmployeeSalary", "Department").show()
    ==>> sorted_df = df.orderBy(asc('department'), col("country).desc(), nulls_last=True)
    ==>> result = df.limit(10)
    ==>> df.show(n=3, truncate=25, vertical=True) / df.printSchema() / display(df) / print(df1.columns)


>>  union tables: 
	SELECT column1 FROM table1 UNION DISTINCT/UNION ALL/EXCEPT DISTINCT/INTERSECT DISTINCT SELECT column1 FROM table2;
	union_distinct = pd.concat([df1, df2]).drop_duplicates()  / union_all = pd.concat([df1, df2])
	union_distinct = df1.union(df2).distinct()  /union/subtract/intersect/


>> Requirements:
	==>> 
	add new column ---> newdf = df.withColumn("NewColumn", lit(1))
	drop columns -->> df2 = df.drop("Country", "Region")
	column name change -->> new_df = df.withColumnRenamed("oldColumnName", "newColumnName")
	column type change --> df = df.withColumn("Phone", col("Phone").cast("string"))
	==>> df distanct values, count, drop duplicates ----> df.select('cn1', 'cn2').distinct().count() ----> df.dropDuplicates(['cn1', 'cn2'])
	==>> df.select(initcap(col("country")))  -->> initcap / lower / upper
	==>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName))
			  --> df.select(concat_ws(' | ', col("Region"), col("Country")))
agg functions --> df.select(sum('CN')).show() -----> max, min, avg, count, countDistanct  ------> note: ignore null values
	grouping and agg -----> df.groupBy('CN').agg(sum('CN')).show() -------> min, max, avg, count
	captialization -----> df.select(initcap('CN'))----------> lower, upper 
	triming spaces, padding values (ltrim, rteim, trim, lpad, rpad)
	result_df = df.select( col("EmployeeID"), ltrim(col("Name")).alias("ltrim_Name"), lpad(col("Name"), 10, "X").alias("lpad_Name"))
	splitng column, size of array, indexing, explode, array_contains  ---------> df = df.select(split(df.FullName, " ").alias('cn'))
	create date, timestamp columns in df, add/subtract days, 
	==>>
	DataFrames are immutable in Sparkâ€”transformations result in new DataFrames, leaving the original unchanged.

=====================
Python:
>> Interpreter: The programme that runs your code.
   Virtual Environment: A separate "workspace" that uses a Python interpreter but has its own packages and dependencies.
>> pip - tool for install python packages
   pypi - centralized repository for py packages
>>  Create a Virtual Environment, Activate the Virtual Environment, Deactivate the Virtual Environment
    install packages into virtual env, download packages from venv
>> terminate the python programme, execute the file using the specified program or interpreter (#!/usr/bin/python3.5)
>> logging
>> Hiding Passwords and Secret Keys in Environment Variables (linux/mac/Windows) and access them into code -------> os.environ.get("key")
>> try: / except / else / finally:
>> with open('sur.log', 'r') as file:
>> Types of Config Files, requests module, unique identifiers (UUIDs)
>> Modules (py file) vs package (collection of modules)
>> Handlling errors: compile time error: Syntatical - missing /Logical error: wrong output 2+3 = 7 /run time error:  5/0.
>>  break / continue / pass
>> if, if else, nested if, if elif else / while loop / for loop / for else
>> List, tuple, set, dictionary, array
>> functions: reuse block of code / 
    positional / keyworded / variable length: def sum(a, *b): / keyword variable length: def person(name, **data)
    Global variable (outside fun) / local variable (inside fun)
    recursion: function calling it self ex: factorial
    anonymous function: x = lambda a: a * a / print(x(5))
    filter: list(filter(lambda a: a%2 == 0, lt))
    map: list(map(lambda a: a * 2, lt))
    reduce: from functools import reduce / sum = reduce(lambda a, b: a + b, lt) /
>> create python packages
    -- package python code as .whl file
    -- setuptools (build and package projects), whell (create & install .whl files), twine (upload packages to pypi)
    -- create code, __init__.py, setup.py 
        python setup.py bdist_wheel - created wheel file
        install whl package

>> memory shared tasklist of each programe, python programme is one of task list - within this portion memory used.
    Threading: by default, the python programee executes on single threading
    Multi threading: no control in execution, more memory and faster execution - NO (program execute on one thread and             other threads are pause)

=================================
NumPY and Pandas:

>>  Array: same data type (1-D) - Simple, basic collections (like lists) for small collections.
    NumPy: N-D - leniar algebra library - numerical calculations for large datasets.
    Pandas: Data analysis library for handling and manipulating structured data, great for dataframes (tables), series.

>> Data scientist or data analytics - Answer the quetions on large data points

>> why Python Pandas
    Excel - Can not handle large sets of data
    Python - Need to write code to find answers
    Pandas dataframe: its python modules - makes data scientist life easier and effective

>> Series is very similar to a NumPy array. 
    Series can have axis labels. 
    it can hold any arbitrary Python Object.
    Dataframe : combination of series.


Create dataframe:
labels = ['a','b','c']
my_list = [10,20,30]
arr = np.array([10,20,30])
d = {'a':10,'b':20,'c':30}

df = pd.read_csv('Salar2.csv')
df1 = client.query(sql).to_dataframe(create_bqstorage_client=False)
df2 = pd.DataFrame(data = [['sure', 20], ['ses', 25], ['siv', 30]], index = ['a', 'b', 'd'], columns = ['name', 'age']) or
from numpy.random import randn
df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split())

----------

df.head(2)
df.set_index("Id", inplace=True)
df = df.reindex(new_index)

df['W'] / df[['W', 'Y']] / df['new'] = df['W'] + df['Y']
df.loc['C'] / df.loc[['C', 'D']] / df.loc['C', 'X'] / df.loc[['C', 'E'], ['X', 'Z']]
type(df) / type(df['BasePay'])
df.drop('new',axis=1,inplace=True)
df[df['W']>0][['Y','X']]
df[(df['W']>0) &| (df['Y'] > 1)]

>>  Cleaning Missing values:
    df.dropna(axis=0, how='any', thresh=2, subset=['col1', 'col2'], inplace=False)
    df.fillna(value=2 or df['col1'].mean() or {'col1': 2, 'col2': 4}, method='ffill', axis=0, inplace=False, limit=2)
    df.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=None)

>>  df.groupby('Company')['column'].mean()/std()/min()/max()/count()
    df.groupby('Company').describe().transpose()

>> combining data frames: concat, merge, join
    pd.concat([df1,df2,df3], axis=1) 
    pd.merge(left, right, how='outer/inner/left/right', on=['key1', 'key2']) - based on keys
    left.join(right, how='outer') - combining the columns of two potentially differently-indexed DataFrames 

>> df['col2'].unique() / df['col2'].nunique() / df['col2'].value_counts()
    df.drop_duplicates(subset='Name') / df.drop_duplicates(subset=['col2'], keep='last').reset_index(drop=True) / df.duplicated().sum() / 
    newdf = df[(df['col1']>2) & (df['col2']==444)]
    df['col1'].apply(lambda x: x * 2 (or) times2 function) /  df['col3'].apply(len) /
    del df['col1'] / df.columns / df.index /  df.isnull() /
    df.sort_values(by='col2') #inplace=False by default /
    df.pivot_table(values='D',index=['A', 'B'],columns=['C'])

>> Spiting:

df[['fn', 'ln']] = df['EmployeeName'].str.split(' ', expand = True)
or
df[['ffn', 'lln']] = df['EmployeeName'].apply(lambda x: pd.Series(str(x).split(' ')))

df.head(2)


---------------



--------------
data type casting:

df["Date"] = pd.to_datetime(df["Date"], format='%d/%m/%Y') or df["Date"] = df["Date"].astype('datetime64[ns]')
df["Id"] = df["Id"].astype(str)
df["Id"] = df["Id"].astype(float)
df["Id"] = df["Id"].astype(int)

----------------

cleaning wrong data:

df.loc[5, 'BasePay'] = 110000

for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.drop(i, inplace = True)
        
for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.loc[i, 'BasePay'] = 10000
        

=======================================

df.duplicated()
df.drop_duplicates(inplace = True)

=======================================

df2.corr()

============================

import matplotlib.pyplot as plt
df2.plot(kind='scatter', x = 'Year', y = 'BasePay')
plt.show()

===========================
data structures (containers storing data in memory) + code instructions = software apply
selecting right data structure for a given problem - makes efficient programme

bigo - measure running time or space rwuirements for programme as input size grows

time complexity (o(1) // log(n) //  o(n) // o(n2) //

=============================================
pyspark:

>> read data and process using python - not recommend these low level apis (use for reading schemas/properties)
>> read data and process using python pandas -  recommend for small/medium datasets

>> Big data problem: volume, varity, velocity (rdbms failed to handle)
>> Big data solution
	Monolithic approach: massive resource (massive system) - vartical scalable, not foult tolerant, expensive
	distributed approach: resource pool (cluster) - Horizantal scalable, foult tolerable & HA, economica

>> Haddop came as disributed big data processing platform
	Google white papers------> Haddop ecosystem------------> Spark
	Yarn (cluster operating system), HDFS (distributed storage), HMR (distributed computing)
	other tools developed on top hadoop core platform:
	HBASE hive - databases
	Sqoop - data ingetion tool
	pig - scripting language
	Flume - move large vol of web server log, event data to HDFS or Hbase
	Oozie - workflow scheduling tool for Hadoop jobs

    challemges - steep learining curve / setup time and cost / maintanance of cluster / application development life cycle / ineeficient usage of cluster / slow performance during data processing
    
    These challenges in HES (datalake using HES) are overcome by datalake using cloud

>> Apache spark:
- opensource distributed/cluster/parallel processing system for big data.
- in-memory caching
- uses optimized query execution for fast queries
- provides api in java, scala, python, r
- supports batch processing, inetractive queries, real-time analytics, ml and graph processing
20. Overview of spark and its architexture

-- data processing: source (files,db, rest payloads) -----> process (sql, libraries/pandas/dask, frameworks/spark) ------->
    target(files,db, rest payloads)

-- data processing libraries: python pandas/limitations whth large datasets - introduces python dask 
(uses server efficiently and multi nodes)  - pyspark works well spark clusers with multiple nodes when compared to dask 

-- distributed computung: ex: hadoop and spark
    single server (etl) - problem is sla in prod and capcity issues even if you use efficiently with multithreading
    cluster (have multiple server)

-- databricks have spark based runtime - AWS (EMR & GLUE), both spark embedded in them - in gcp dataproc, we got hadoop and spark - Azure synaps, spark embedded it - 
    snow flake most popular DWH they introduced snowpark based on spark

-- spark: multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
    key features: batch/streaming data - sql analytics  - datascience at scale - machine learning
    architexture: https://spark.apache.org/docs/latest/cluster-overview.html
    cluster capacity, spark cluster: cluster(D,W) nodes/OS/Spark engine/executors(JavaVM) in worker, vcpu or slots unused capacity, tasks used capacity/network/


=====================
- problem: big data processing - cant handle by excel , sql db, etc.
- Sol : parallel processing - spark 
        https://spark.apache.org/
- Spark : 
        > dirver node (spark contest) - cluster/resource manager - worker nodes (executor, task, cache).
        > ram in worker nodes has divided into partitions - these partitions can be inside specific data structure RDD (write code in py) / DF (strucre databases).
        > RDD - read only - immutable - fault tolerant using DAG concept - 1. transformation (new rdd - not human readble) 2. action (to read)

why PySpark not python for bigdata? :
python - take sample of data and apply statistics to develop histograms
pyrhon - can process large datasets with multithreads 
problem - when data is big and stays in memory / we cant take entire data on compute and use python on it.
        - data cant stay in one computer and it can be distributed.
- Platforms for pyspark:
        > https://colab.research.google.com/
        > VM vare on desktop version
        > databricks community edition / on cloud
        > dataproc on GCP



=========
https://github.com/itversity
https://github.com/dgadiraju?tab=repositories
https://spark.apache.org/



