>> Databricks: big data processing 
1. signup for databricks from marketplace to use on GCP. then page redirected to workspaces.
2. create databricks workspace on GCP
   data bricks run on cloud platform and utilizes some resorces from underlying gcp project.
3. open workspace UI and spin up databrick clusters on GCP.(cluster mode, node type, runtime version-scala,spark)
4. getting started with databrick notebook on cluster. (do developement using python code and use magic commands to interact with databricks fs)
5. setup databricks cli/sdk or restApi for clients/servers on winodws.
6. manage databricks clusters/fs/jobs/secreats using cli
7. copy datasets from local dir to dbfs using cli
8. process data in dbfs using spark sql in notebooks
9. spark sql to find daily product revenue from the dbfs files
10. convert csv to parquet with schema using pyspark.

11. overview of databricks workflows
    task1(py) --> task2(databricks app) --> task3a/3b(databricks app)--> task4(db app) --> task5(py app)
               |                    databricks workflow                                 |
    |                               external orchestration (airflow)                                     |
12. Pass arguments to databricks python notebooks/ sql notebooks
13. click on dataflow workflow - create job - under job define several tasks and dependency, also utlize cluster type based 
     task load 
    --> create and run first databricks job.
    --> pass arg at task level / job level.
    --> schedule the db workflow job in Job ui (but usally schedule these jobs using external orchestration i.e. airflow)
14. real time databricks job
    --> cleanup database and datasets. Task1
    --> convert csv to parquet by applying schema. Task2
    --> create spark sql table on top of parquet files. Task3
    --> claucate daily product revenue from sql  tables and write result to target in parquet format. Task4
    --> validata app (tasks) for ELT pipeline using databricks.
    --> build ELT pipeline using databricks job in workflows.
    --> run and review execution elt pipeline using databricks job.


>> databricks --> developed by founder of apache spark, consists of spark engine, integrates with diff languages & cloud

>> databricks concepts
   -->> interface : UI / rest api / cli
   -->> workspace : notebooks, dasboards, libraries, experiments
