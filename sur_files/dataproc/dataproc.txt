
>> 
==> HDFS terminaligy: filesystem (types: standalone, distributed), Block, cluster and node, process & demon process, metadata, replication
==> why hdfs, HDFS architexture, block in hdfs (QA change block size?), 
==> 

>> Apache Spark - multi-language engine (single-node machines or clusters) suitable for large-scale data workloads, and can be --         deployed on-premises or in the cloud.
   Google Cloud Dataproc is a managed service that runs Apache Spark (and other frameworks),

>> enable API, create dataproc cluster on vm/gke.
   -- cluster type: single node 1m0w, standard for dev team 1m Nw, high availability for prod 3m Nw
   -- allocate static ip to it >> vm instance - external ip - view detais - n/w page ip addresses - reseve to static 

>> validate ssh connectivity to master node vm of dataproc cluster: gcloud ssh cammand helps to copy key on vm and able to ssh vm (gcloud compute ssh --zone "us-central1-a"    "dataprocclu1-m" --project "possible-dream-433814-b4")
   -- establish an SSH connection to a remote server VM instance: ssh -i C:\Users\surmacha\.ssh\google_compute_engine surmacha@35.184.234.232
   -- setup vs code remote window for dataproc vm of master node - create below config file in .ssh - select remote window button on vs code - host in search 
   Host Sur-DP-Cluster-VSC
    HostName 35.238.68.160
    IdentityFile C:\Users\surmacha\.ssh\google_compute_engine
    User surmacha
   -- copy material to the VM master node using git clone command.

>> copy local files in master node of vm to hdfs in dataproc/ gcs blobs into hdfs on dataproc.

6. validate pyspark / spark-shell (spark scala) / spark-sql CLI in dataproc cluster.
7. submit dataproc job using spark sql in UI
8. create dataproc workflow using jobs
   cleanup --> convert json file to parquet for orders/order_items --> compute daily product revenue and save back to gcs.
   >> review the scripts.
   >> apply unit tests/validation in local vs code environment configured with dataproc cluster master node.
   >> use scripts in gcs rather then hdfs (temproray based on cluster)
   >> copy spark-sql scripts to gcs location.
   >> run and validate spark-sql scripts placed in gcs using vs code configured with dataproc master node
   >> limitations of running spark sql scripts using dataproc jobs
         submtting jobs by defining query file in gcs via ui is not working (gcloud cli is working).
         submit jobs using gcloud from external cli but not from vs code dataproc master node
   >> submit spark sql jobs: list spark-sql scripts in gcs and review content then submit job using dataproc gcloud command.
   >> dataproc workflow templates: orchestrated pipeline within gcp dataproc
         UI- create template using jobs defined by query text (query file not working in job) - not use in practical
         gcloud commands (more flexibility than ui):
               1. create dataproc workflow template (gcloud dataproc workflow-templates create ----)
               2. attach workflow template with existing cluster (gcloud dataproc workflow-templates add-cluster-selector---)
               3. add jobs to workflow template (gcloud dataproc workflow-templates add-job ----)
               4. instantiate workflow template (gcloud dataproc workflow-templates instantiate ----)
   >> stop / delete cluster using ui, gcloud
----------------
>> place script in gcs location: gs://surretail/scripts/daily_product_revenue/gcsToBq.py (or) gcsToBq.sql (or) gcsToBq.scala
>> execute gcs script 
   ---> run pyspark script on master node of VM (ex: spark-submit -f gs://surretail/scripts/daily_product_revenue/gcsToBq.py)
   ---> submit pyspark job using UI 
   ---> submit spark-sql job using gcloud (gcloud dataproc jobs submit spark-sql --cluster=cluster-a695 -f          
        gs://surretail/scripts/daily_product_revenue/compute_daily_product_revenue.sql --params=bucket_name=surretail)
-------------------------------
>> Execution Flow in PySpark
   Start SparkSession â†’ spark = SparkSession.builder.appName("App").getOrCreate()
   Load Data (from a list, CSV, Parquet, etc.)
   Apply Transformations (e.g., filter(), select(), groupBy())
   Trigger an Action (e.g., show(), collect(), count())
   Stop SparkSession â†’ spark.stop()
------------------------------



===========================================================================================================================
PySpark Syntax: read : Join : filter : groupBy: agg : filter : select : orderBy : Limit: 

hdfs dfs -ls -R /public/retail_db
hdfs dfs -mkdir /path/to/new_directory
hdfs dfs -copyFromLocal /local/path/to/file /hdfs/path/to/destination 		or
hdfs dfs -copyToLocal /hdfs/path/to/file /local/path/to/destination      or
hdfs dfs -put data/retail_db /public (local to hdfs -put)
hdfs dfs -cp gs://surretail/retail_db /public/retail_db (hdfs to hdfs, gcs to hdfs, hdfs to gcs, gcs to gcs -cp)
hdfs dfs -mv /path/to/source /path/to/destination
hdfs dfs -rm -R -skipTrash /public/retail_db
hdfs dfs -cat /path/to/file
hdfs dfsadmin -report
hdfs dfs -df -h
hdfs dfs -du -s -h /path/to/directory
hdfs dfs -chmod 755 /path/to/file_or_directory
   


    ==>>
    df = spark.createDataFrame(data, columns)
    df_from_pandas = spark.createDataFrame(pandas_df/data_dict)
    df_empty = spark.createDataFrame([], columns)
    df_csv = spark.read.csv("/path/to/file.csv", header=True, inferSchema=True)
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
    custom_schema = StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("age", IntegerType(), True)])
    custom_schema = ''' ID Integer, Name String, Age Integer, Salary Double '''
    df = spark.read.csv("your_file.csv", schema=custom_schema, header=True)
    file_paths = ["file1.csv", "file2.csv", "file3.csv"] 
    df = spark.read.csv(file_paths, header=True, inferSchema=True, sep=',')
    df = spark.read.option("modifiedBefore", "2022-02-27T05:30:00").json("file:///path_to_file/data_files/")
    ==>> join_df = df1.join(df2, df1.department_id == df2.id/on='cn, how='inner/left/right/outer/left_semi/left_anti') 
         join_df = df1.crossjoin(df2)
         broadcast_join = df1.join(broadcast(df2), on="id", how="inner")
    ==>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  &|   /
    ==>> grouped_df = df.groupBy('department').agg(max("sal"))   		
    ==>> having_df = df.filter(grouped_df['count'] > 1)
    ==>> selected_df = df.select("customer_id", col("Name").alias('EmployeeName'), column("last_name"), df.email, df["city"] )
                    = df.selectExpr("Name as EmployeeName", "Salary as EmployeeSalary", "Department").show()
    ==>> sorted_df = df.orderBy(asc('department'), col("country).desc())  / df11.orderBy(asc_nulls_last('value')).show() /
    ==>> df_limit = df.limit(10)
    ==>> df.show(n=3, truncate=25, vertical=True) / print(df.collect()) / print(df.count()) / df.printSchema() / display(df) / print(df1.columns) 


	==>> union tables: union_distinct = df1.union(df2).distinct()  /union/subtract/intersect/   -->> positional col match / schema must be identical
	     union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col match by name / schema can differ / missing col handle with null values
	     distanct values/drop duplicates ----> df.select('cn1', 'cn2').distinct() ----> df.dropDuplicates(['cn1', 'cn2'])
	==>> 
	add new column ---> newdf = df.withColumn("NewColumn", lit(1))
	drop columns -->> df2 = df.drop("Country", "Region")
	column name change -->> new_df = df.withColumnRenamed("oldColumnName", "newColumnName")
	column type change --> df = df.withColumn("Phone", col("Phone").cast("string"))
	==>> df.select(initcap(col("country")))  -->> initcap / lower / upper
	==>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName))
			  --> df.select(concat_ws(' | ', col("Region"), col("Country")))
	==>> splitng column -->> df = df.select(split(df.FullName, " ").alias('cn'))  -->> multi char "[-, ]", -->> size, [0] or .getItem(0), explode, array_contains
	==>> result_df = df.select( col("EmployeeID"), ltrim(col("Name")), lpad(col("Name"), 10, "X"))  -->> rtrim / trim / rpad
	==>> create date, timestamp columns in df, add/subtract days, 
	     df = spark.range(10).withColumn('today', current_date())
	     df = spark.range(10).withColumn('now', current_timestamp())
	     date_sub(col("today"), 5) / date_add(col("today"), 5) / datediff(col("week_ago"), col("today")/ 
	     months_between(to_date(lit("2016-01-01")), to_date(lit("2017-01-01")) / to_date / to_timestamp / 
	==>>    df.dropna("All" subset=['cn1", "cn2"]) 
		df.fillna("N/A" subset=['cn1", "cn2"]) / df.na.fill({'cn1':'val1', 'cn2': 'val2'}) / is null('CN') 
		df_replaced_multiple = df.replace(to_replace=["banana", "cherry"], value=["orange", "grape"], subset="fruit")
	==>> coalesce('cn1', 'cn2','cn3') - return first non-null value from list of columns
	     df.gruopBy("region").agg(coalesce(mean("units_sold), lit(0))
	==>> agg functions --> df.select(sum('CN')).show()  ------> note: ignore null values
	     grouping and agg -----> df.groupBy('CN').agg(sum('CN')).show()
		sum , sumDistinct, min, max, avg, count, countDistanct, collect_list, collect_set first, last, stddev, variance 
	==>> when&otherwise -->> df = ( df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult")).withColumn("income_bracket", when(df.salary < 4000, "Low") 					.when((df.salary >= 4000) & (df.salary <= 4500), "Medium") .otherwise("High")) )
	==>>cast data types -->> df = df.withColumn("column_name", col("column_name").cast("target_data_type"))
			    -->> cast_expr = [col("column1_name").cast("target_data_type1"), col("column2_name").cast("target_data_type2")] / df = df.select(*cast_expr)
	==>> window functions -->> window_spec = Window.partitionBy("category", "sub_category").orderBy(asc("timestamp"), col("score").desc()) \
                    				.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
				   window_spec = Window.partitionBy("category", "sub_category").orderBy(col("timestamp"), col("score")).rowsBetween(-2, 3)
				   df = df.withColumn("row_number", row_number().over(window_spec)) 
   		max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) /row_number() / rank() / dense_rank() / lag(cn) / lead(cn) / 
		first_value(cn) / last_value(cn) / nth_value(cn, n) /
	==>> explode/explode_outer: exploded_df = df.select("Name", explode("Subjects").alias("Subject"))  -->> explode_outer
		Drops rows with null or empty arrays / Keeps rows with null or empty arrays, filling with null
	==>> 


===========>
pyspark
or 
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Load CSV to BigQuery").getOrCreate()

>>> df = spark.read.option("header", "True").option("inferSchema", "True").csv("gs://surdatabuk/data/mark.csv")

>>> df1=df.filter(col("age")>=30)
or
>>> df.createOrReplaceTempView("bigquery_table")
>>> df1 = spark.sql("SELECT * FROM bigquery_table WHERE age > 30")

>>> df1.write.mode("overwrite").format("bigquery").option("temporaryGcsBucket", "surdatabuk").option("table", "elegant-circle-454709-h5.surtest11.surtt3").save()

exit()

============>
spark-shell
or
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._ / import org.apache.spark.sql.functions.{sum, round, col}
val spark = SparkSession.builder.appName("Load CSV to BigQuery with Transformation").getOrCreate()

scala> val df = spark.read.option("header", "true").option("inferSchema", "true").csv("gs://surdatabuk/data/mark.csv")

scala> val df1=df.filter(col("age")>=30)
scala> df1.show / df1.show()
or
scala> df.createOrReplaceTempView("bigquery_table")
scala> val df1=spark.sql("select * from bq_table where age>32")

scala> df1.write.mode("append").format("bigquery").option("temporaryGcsBucket", "surdatabuk").option("table", "elegant-circle-454709-h5.surtest11.surtt4").save()

scala> :quit

===========>
spark-sql

spark-sql (default)> create or replace temporary view orders(name string, age int, city string) using csv options(path='gs://surdatabuk/data/mark.csv');
spark-sql (default)> select * from orders where age > 30;
spark-sql (default)> describe table orders;

spark-sql (default)> exit;

=============>
use val for scala code
>>> df=spark.read.format("bigquery").option("temporaryGcsBucket", "surdatabuk").option("table", "elegant-circle-454709-h5.surtest11.surtt4").load()
>>> df.write.partitionBy("age").mode("overwrite").csv("gs://surdatabuk/part_folder")  - create partition folders as per key 
>>> df.repartition(2).write.mode("overwrite").csv("gs://surdatabuk/part_file")   - create two files as per partition no



==================================================================================================================================

Pyspark Interview QA:

pyspark QA:

>> what is spark, features, architecture

>> Haddop Yarn Cluster 
	-- master node (Yarn RM)
   	-- worker node --> App Master container (pyspark driver --> py4j --> JVM driver), executors (each 4cpu 16 gb ram - means 4 slots and memory common for 4 slots)
   	-- Script or spark app submit --> spark launches driver app --> driver app create spark session/conset --> spark context  request executor resources from YARN rm -->
	   in parallel, driver app divide spark app into small tasks (app -> jobs based on action --> stages based on wide trans --> shuffle/sort --> 
	   tasks (these tasks run on executor slots)

   -- Dynamic Partition pruning in spark3.0:

1. What is the difference between cache() and persist() in PySpark?
	when df is frequently used, we do caching df for better performance
	cache(): cache() uses the default storage level, which is MEMORY_ONLY.
	df.cache()
	persist(): persist() allows you to specify different storage levels, such as MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.
	from pyspark import StorageLevel
	df.persist(StorageLevel.MEMORY_AND_DISK)

2. How does Lazy Evaluation work in PySpark?
	the execution of transformations on RDDs (Resilient Distributed Datasets) or DataFrames is deferred until an action is performed. 
	This allows PySpark to optimize the execution plan and improve performance.

3. What are wide and narrow transformations in PySpark?
	Narrow transformations are those where each input partition contributes to only one output partition. Examples filter, union
	Wide transformations involve shuffling data across the network, as input partitions contribute to multiple output partitions. join, distinct, groupBY

4. Explain shuffle operations in PySpark and their impact on performance.
	redistributing data across different nodes in a cluster, which is necessary for certain transformations like groupBy, join, and distinct.
	suppose if have suffle partions lesser than cluster capacity --> slow completion time, underutilization of cluster
	if data per shuffle partion is too large --> tune by increasing num of partitions required
	if data per shuffle partition is very less --> tune by decewasing no of SP, find size of SP based on available cores.
	default shuffile partitions is 200 / data size per shuffle partition b/w 1-200 mb
	# Set the number of shuffle partitions --> spark.conf.set("spark.sql.shuffle.partitions", 100)
	# Verify the setting --> print(spark.conf.get("spark.sql.shuffle.partitions"))

5. what is data skewness and salting technique? (after data shuffling, most of data goes to few partitions as reference to the keys, which leads to data skewness)
   skewed partition : uneven data distribution
	job is taking time, unutilization of resources, oom or data spill (write results to disk -- very costly)
	fix --> repartition, salting, AQE, broadcasting
   df.repartition(10)
   df.rdd.getNumPartitions()
   df.select(spark_partition_id().alias("partid")).groupBy("partid").count()

6. What is broadcast join, and when should we use it?
	type of join operation where a small dataset is broadcasted to all worker nodes in the cluster. 
	This allows the join to be performed locally on each node, significantly improving performance by reducing the amount of data shuffled across the network.

7.      select: Ideal for column selection.
	selectExpr: Best for applying SQL expressions and transformations.

>> Key Differences
   Memory Management: On-heap memory (data in deserilization -ready for eat) is managed by the JVM and subject to GC, while off-heap memory (data in serilization form - raw format) is managed outside the JVM and not subject to GC.
   Performance: Off-heap memory can reduce GC overhead and improve performance for large datasets, but may introduce serialization/deserialization overhead12.

>> What is PySpark (python api let u use spark with python), and how is it different from Apache Spark (big data engine).

>> How do you initialize a SparkSession in PySpark?

>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
    similarities in 3 apis : foult tolerant, distributed, in-memory computation, immutable, lazy evaluation, internally processing as rdd for all 3 codes
    RDD: low-level api / no optimizer / oops style api / complie time error (strong type safety) / 4 lang / no schema /
    DF: high-level api / catalyst optimizer / sql style api (user friendly) / run time error (less type safety) / 4 lang / schema structured /
    DS: high-level api / optimizer (best plan execution)/ oops / complie time error (strong type safety) / 2 lang / schema structured / 

>> What is the difference between map() and flatMap() inPySpark?
    map() when you want to apply a function that returns a single value for each input element
    flatMap() when the function can return multiple values for each input element

>> How do you handle missing values in PySpark?
	df_replaced = df.replace(to_replace=4, value=0)
	fillna / dropna --> see above
	    
>> select(): Simple column selection. --> df2 = df.select("name")
   selectExpr(): Column selection with transformations. --> df2 = df.selectExpr("id as identifier", "name")

>> What are transformations (select, filter, groupBy, ..) and actions (collect, show, count,...) in PySpark? Giveexamples.

>> What is the significance of partitioning and bucketing in PySpark, and how does it affect the performance of data processing tasks?
	Partitioning: Divides data based on column values, creating separate directories for each partition. Best for filtering queries.
	df.write.partitionBy("country", "year").parquet("output/partitioned_sales")
	Bucketing: Divides data into a fixed number of buckets based on the hash of a column. Best for join queries.
	df.write.bucketBy(4, "user_id").saveAsTable("bucketed_sales")

>> How does .ðœð¨ð¥ð¥ðžðœð­() work, and when should it be avoided (memory constraints, performance issues) ?
	When you call .collect(), PySpark gathers all the partitions of the DataFrame or RDD and brings them to the driver node. This means that the data is no longer 		distributed across the cluster but is instead stored in the memory of the driver program

>> What is the difference between createOrReplaceTempView and createGlobalTempView in DataFrames?
   Scope:
   createOrReplaceTempView creates a view local to the SparkSession.
   createGlobalTempView creates a view accessible across all SparkSessions within the same application.
   Lifetime:
   createOrReplaceTempView views are dropped when the SparkSession ends.
   createGlobalTempView views persist until the Spark application terminates.

>> How to set the partitions in pyspark?
   Reading Data: The option("numPartitions", 10) sets the number of partitions when loading the data.
   repartition Method: This method increases or decreases the number of partitions and shuffles the data.
   df1 = df.repartition(3)
   print(f"Number of partitions  repartitioning: {df1.rdd.getNumPartitions()}")
   coalesce Method: This method reduces the number of partitions without a full shuffle, which is more efficient for reducing partitions.
   df_coalesced = df.coalesce(2)
   print(f"Number of partitions after coalescing: {df_coalesced.rdd.getNumPartitions()}")

>> Default file format in Spark (is Parquet), Why Parquet?
      Efficient Storage: Columnar format allows for better compression and encoding.
      Performance: Optimized for query performance, especially for read-heavy operations.
      Compatibility: Widely supported across various big data tools and frameworks.

>> what are the optimization techniques in pyspark?
	Use DataFrame/Dataset over RDD
	use col format files 
	filter data erly
	Cache Data or persist data
	use broadcast to Optimize Joins
	Optimize shuffle Partitions: Repartition / Coalesce
	enable AQE, salting
	partitioning & bucketing for frequently accessed based on specific column
	Avoid User Defined Functions (UDFs)

>> Difference between sortby and orderby?
   sortBy is used with RDDs.
   orderBy is used with DataFrames.

>> what is the default block size (128 MB) in hdfs and how to change it?
   Edit the hdfs-site.xml File:
   <property>
       <name>dfs.blocksize</name>
       <value>268435456</value> <!-- 256 MB in bytes -->
   </property>

>> what is data skewness 
   Sol: salting technique, AQE, broadcastjoin
 
>> Explain about out of memory issue in spark?
   Causes of Out of Memory Issues: 
   Driver Memory:
   Collect Operations: Using collect() to gather large datasets to the driver can cause memory overflow. The driver tries to merge all results into a single object, which      might be too large to fit into the driver's memory1.
   Large Broadcast Variables: Broadcasting large variables can consume significant memory on the driver.
   Executor Memory:
   Task Execution: Executors run tasks and store intermediate data. If the tasks require more memory than allocated, it can lead to out of memory errors1.
   Shuffling Data: During operations like groupBy or join, data shuffling can cause memory issues if the data size exceeds the executor's memory capacity2.
   Solutions to Out of Memory Issues
   Increase Driver Memory: Adjust the driver memory settings using spark.driver.memory to allocate more memory to the driver2.
   Limit Result Size: Use spark.driver.maxResultSize to limit the size of results collected to the driver1.
   Increase Executor Memory: Configure executor memory using spark.executor.memory to allocate sufficient memory for task execution2.
   Memory Overhead: Set spark.executor.memoryOverhead to account for additional memory required for JVM overhead3.
   Repartition Data: Repartition large datasets to reduce the size of data processed by each executor1.

1. Find the top 3 highest-paid employees from each department.
   data = [(1, "Amit", "IT", 90000),

2. Write a PySpark code to remove duplicate records based on a specific column.
   data = [(101, "Mumbai", "Maharashtra"), ]

3. Write a PySpark query to calculate the moving average of sales over the last 3 months.
    window_spec = Window.orderBy("Month").rowsBetween(-2, 0)
    df_with_moving_avg = df.withColumn("Moving_Avg", avg(col("Sales")).over(window_spec))

4. What is SparkSQL, and how do you perform SQL operations on DataFrames?
	SparkSQL is a module in Apache Spark that allows you to run SQL queries on large datasets.
   	Registering the DataFrame as a Temporary View: df.createOrReplaceTempView("people")
   	Running SQL Queries: spark.sql("SELECT Name, Age FROM people WHERE Age > 30").show()

5. What is difference between spark session and spark context?
	SparkContext: Basic functionality for RDD operations. Requires separate contexts for SQL, streaming, etc. (used <2.0 spark version)
	SparkSession: Provides a unified interface for DataFrame and Dataset APIs, SQL queries, and more. (used >2.0 spark version)

6. what is collect list and collect set and when do we use it?
   result = df.groupBy("name").agg(collect_list("value").alias("values_list")) - allows duplicates in list
   result = df.groupBy("name").agg(collect_set("value").alias("values_set")) - no duplicates in list

7. Given a dataset of Indian cities with their respective populations, write a PySpark code snippet to find the top 5 most populous cities.
     +-------------+----------+
     | City    | Population|
     +-------------+----------+
    top_5_cities = df.orderBy(col("Population").desc()).limit(5)

8. Given a DataFrame containing employee details, write a PySpark code snippet to group employees by their department and calculate the average salary for each department.
    avg_salary_by_dept = df.groupBy("Department").agg(avg("Salary").alias("AverageSalary"))

9. Write a PySpark code snippet to remove duplicate records from a DataFrame based on a composite key consisting of 'customer_id' and 'transaction_date'.
     +------------+----------------+-------------------+
     | customer_id| transaction_id | transaction_date |
     +------------+----------------+-------------------+
    df_no_duplicates = df.dropDuplicates(["customer_id", "transaction_date"])
10. input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")]
    output 
   +---+---------------+----------+---------+
   | ID| Name|First_Name|Last_Name|
   +---+---------------+----------+---------+
   | 1|Sagar-Prajapati| Sagar|Prajapati|
   | 2| Alex-John| Alex| John|
   | 3| John Cena| John| Cena|
   | 4| Kim Joe| Kim| Joe|
   +---+---------------+----------+---------+
	df = df.withColumn("fn", split(df.Name, "[- ]").getItem(0)).withColumn("ln", split(df.Name, "[- ]")[1])

11. Given a DataFrame, split the data into two columns (Even, Odd) where:
    Ans. df2 = df1.withColumn('even', when(col('id') % 2 == 0, col('id'))).withColumn('odd', when(col('id')%2!=0, col('id')))

12. Create a DataFrame with two columns:
    Column 1: Default String
    Column 2: Default Integer

13. Word count program in pyspark
    df1 = df.select(explode(split(df["name"], " ")).alias("word")).groupBy("word").agg(count("*").alias("count"))

14. Write a PySpark query to count the number of null values in each column of a DataFrame.
	df1.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()

15. Write a PySpark query to replace null values in a specific column with the previous non-null value.
	window_spec = Window.orderBy("id").rowsBetween(Window.unboundedPreceding, Window.currentRow)
	df.withColumn("filled_col", last('value', ignorenulls=True).over(window_spec)).show()

16. problems that partition solves?
	fast/esay access, parallesium & resource utilization

17. how to decide partiTIon on which column --> based on column cordinality/unique (not high & low / rec low to med) & frequent filter criteria 

18. partitioning
folder: df.write.partitionBy("date").mode("overwrite").parquet("/tmp/folder_partitioned_data")
file: repartitioned_df.write.mode("overwrite").parquet("/tmp/file_partitioned_data")
combine: repartitioned_df = df.repartition(3).write.partitionBy("date").mode("overwrite").parquet("/tmp/combined_partitioned_data")

>>
df = spark.read.option("recursiveFileLookup", "true").parquet("hdfs:///path/to/your/directory")
df = spark.read.option("compression", "gzip").csv("/tmp/compressed_csv_data", sep="|", linesep="/r or /n")
df.write.option("compression", "gzip").mode("overwrite").parquet("/tmp/compressed_data", sep="|", linesep="/r or /n")

df = spark.read.option("mode", "PERMISSIVE").schema(cust_schema).option("columnNameOfCorruptRecord", "_corrupt_record").csv("/path/to/csv")
df = spark.read.option("mode", DROPMALFORMED/FAILFAST").schema(cust_schema).csv("/path/to/csv")

>>
Dynamic partition pruning (DPP) in PySpark is a feature that optimizes query performance by pruning partitions dynamically based on runtime data. This is particularly useful for queries involving joins where one table is significantly smaller than the other. DPP helps reduce the amount of data scanned, thereby improving query efficiency.


>> data cashing in spark
	cache()
	persist(storageLevel=StorageLevel.MEMORY_ONLY) --> DISK_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, OFF_HEAP, MEMORY_ONLY_2
	persist(StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)) --> 	persist(StorageLevel(True, True, False, False, 1)) 
	for disk only ser, for memory ser (compact in size, less memory, more cpu for access) or deser (consumes more memory but fast access)
	how to uncahe --> df.unpersist()
	when cache req: access large df multiple times across spark actions
	when not cache: not fit in memory, not reusing frequently, df is small

>> repartition and coalesce
	repartition(num - uniform size, *cols - not ensure in uniform size)  --> hash based partitioning,
	repartitionByRange(num, *cols)  --> range of values based partitioning
	wide trans, cause shuffle sort
	config("spark.sql.shuffle.partitions", 100) --> can override with num partitions argument
	when: df reuse and repeated col filter, df partition are not well distributed on nodes, large df or skewed partitions
	coalesce - to reduced no of partitions - combine local partitions, no shuffeling, may cause skewed partitions,

.config("spark.sql.adaptive.skewJoin.enabled", "true")  # Handle skewed joins

>> sPARK query plan:
	code syntax is correct or not --> unresolved logical plan --> catalog --> logical plan --> optimized logical plan (filter pushdown & selct req col)--> 
	converted several pysical plans --> cost model --> best pysical --> run on cluster.
	df1.explain(True)

=============

1. create spark session with config optimization
spark = SparkSession.builder \
.appName("olist config optimization") \
.config('spark.executor.memory', '6g') \
.config('spark.executor.cores', '4') \
.config('spark.executor.instances', '2') \
.config('spark.driver.memory', '4g') \
.config('spark.driver.maxResultSize', '2g') \
.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores
.config('spark.default.parallelism', '64') \		# It usually defaults to the number of cores on all executor nodes.
.config('spark.sql.adaptive.enabled', 'true') \
.config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \
.config('spark.sql.autoBroadcastJoinThreshold', 24*1024*1024) \
.config('spark.sql.files.maxPartitionBytes', '64MB') \
.config('spark.sql.files.openCostInBytes', '2MB') \
.config('spark.memory.fraction', 0.8) \
.config('spark.memory.storageFraction', 0.2) \
.getOrCreate()

-- config('spark.sql.shuffle.partitions', '64') - 
-- cluster capacity required to process 25 gb file
	how many cpu cores are required 25GB,  (1-200mb rec, in hdfs-128mb block size), 25*1024*1024/128 = 200 cores
	how many executors requires (2-5 cores rec per executor) - 200/4 = 50 executor instances
	how much each executor memory req  - 1.5 * reserved memory(300mb) or min 4*(partition size) = 512 mb/core = 2 gb/executor
	total memory required to process file = 50*2 = 100 gb
-- Spark memory management
	submit spark app in yarn cluster --> yarn rm allocates app container and starts driver JVM 
	spark.driver.memory --> JVM memory 
	spark.driver.memoryOverhead --> 10% 0r 384mb max one --> used for container processes, pyspark app
	spark.executor.memory --> JVM heap memory (8 gb)
		reserved memory 300 mb --> fixed reserve for spark engine
		spark memory --> spark.memory.fraction=0.6 (default, u can change)--> used for DF operation & caching
			storage memory pool --> 0.5 default, u can change --> spark.memory.storageFraction=0.5 --
			executor memory pool --> 
		user memory -->0.4 --> used for spark internal metada, udf, rdd operations
	spark.executor.memoryOverhead  --> 10% 0r 384mb max one --> overhead memory used for container processes, n/w transfer, read shuffle, python worker 
	spark.memory.offHeap.enabled  --> spark.memory.offHeap.size --> off heap memory --> default = 0
	spark.executor.pyspark.memory --> pyspark memory --> default zero (pyspark is non jvm. so it will take from overhead memory)
	spark.executor.cores --> rec 2 to 5
	
	yarn.scheduler.maximum-allocation-mb --> pysical memory limit at the worker node
	yarn.nodemanager.resource.memory-mb 


	

Project: divide into project modules
review data using colab, then create dev cluster to handle sample datasets, the create cluster in prod to handle live/big data

1. Data Ingetion and exploration: setup env, import csv into hdfs, load data into spark dataframe, examine schema, data types, perform EDA
	==> schema and datatypes:
	==> data leakage or drop:  print(f'customer_df has {customer_df.count()} rows')
	==> Null values: customer_df.select([count(when(col(c).isNull(), 1)).alias(c) for c in customer_df.columns]).show()
	==> duplicate values:	customer_df.groupBy('customer_id').count().filter('count>1').show(10)
	==> data distribution as per state, avg time taken to deliver item: customer_df.groupBy('customer_state').count().orderBy('count', ascending=False).show(10)
	
2. data cleaning (remove duplicates, handle null, ..) and trans (agg, pivoting,..) :
	=> identify issues: missing values, duplicate, invalid date
		#finding missing values
		def missing_values(df, df_name):
		    print(f'missing values in {df_name}:')
		    df.select([count(when(col(c).isNull(), 1)).alias(c) for c in df.columns]).show(5)
	=> handle missing values: drop (for non-critical columns) or fill null values (for numerical columns) or impute values (for contionus data)
		df.na.drop(subset=['name', 'age']).show()
		df.fillna({'name':'unknown', 'age':1000}).show()
	=> standadize format: extract date from timestamp, replace payment_type col using when/otherwise
	=> data type corretion: change pincode from int to string
	=> dedplication: reomve duplicte records
		df.dropDuplicates(['id']).show()
	=> data transformation: apply feature engineering
		dfj = ordd_df.join(ordi_df, 'order_id', 'left').join(ordp_df, 'order_id', 'left')
		groupBy & agg
 	=> store in cleaned data (HDFS in parquet - good for querying)
		>> cust_df.write.mode('overwrite').format('parquet').save('/data/olist_output/cust_df.parquet') - data in tb this may take hours/days
		>> other: use terminal, spark-sql - create external hive table, select data
		CREATE EXTERNAL TABLE IF NOT EXISTS orders (
		Â  customer_id string,
		Â  customer_unique_id string,
		Â  customer_zip_code_prefix integer,
		Â  customer_city string,
		Â  customer_state string,
		)
		STORED AS PARQUET
		LOCATION '/data/olist_output/cust_df.parquet';
		>> select * from orders limit 5;


3. data integration (combine data from multiple tables) and aggregation (to compute metrics): 
	==> join all datasets -	cache frequently used datasets for better performance -	when we use inner join / left join?
	==> optimizing joins - broadcast small datasets
	==> focus on aggregations & window function
	==> caching & optimizing queries for performance

4. performance optimization: enhance data processing task - partitioning, cache/persist, spark config
5. data serving: make processed data available in bq


terms:
anomalies

