========================================================================================================================================================================
links:
https://github.com/afaqueahmad7117/spark-experiments/tree/main/spark
https://allthingdata.substack.com/p/essential-linux-commands-for-data         

========================================================================================================================================================================
===========================================> hdfs commands:

>> linux system / hdfs system
hdfs -help
hdfs dfs -help
hdfs dfs -help ls

==========================================> low-level api


>> rdd = spark.sparkContext.parallelize(data)  /  rdd = spark.sparkContext.textFile(hdfs_gcs_path) 
>> rdd1 = rdd.filter(lambda row: 'Frieza' in row) / rdd1 = rdd.filter(lambda row : row!=header)  - header = rdd.first()
   rdd1 = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)   ---> word count programme
   rdd1 = rdd.map(lambda row:row.split(',')).map(lambda row:(row[2],1)).groupByKey().map(lambda row:(row[0],len(row[1]))).collect()
   rdd1 = rdd.map(parse_row) / def parse_row(row):
>> rdd.first() / rdd.collect() / rdd.take(3) / rdd1.countByValue()
   
=============================>> high-level api

>> custom_schema = StructType([StructField("id", IntegerType(), True), ....]) or ''' ID Integer, Name String '''
   df = spark.read.option("header", True).option("inferSchema", True)/schema(cust_schema).option("mode", "PERMISSIVE").json("file:///path_to_file/data_files/")
   df.write.format('parquet').mode('overwrite').save('gs://surdatabuk/data/olist_op/')
   df.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket', 'gcs_path').option('table', 'p.d.t').save()
>> join_df = df1.join(df2, df1.id == df2.id/on='cn', how='inner/left/right/outer/left_semi/left_anti') / df1.crossjoin(df2)  / df1.join(broadcast(df2), "id", "inner")
>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  &|   /
>> df.select("customer_id", col("Name"), df.email, df["city"])    /    df.selectExpr("id*2 as nid", "name as nname").show()
>> sorted_df = df.orderBy(asc('department')) / col("country).desc())  / asc_nulls_last('value') /
>> df.show(n=3, truncate=25, vertical=True) / df.collect() / df.count() / df.printSchema() / df.columns / df.limit(10)
>> union tables: union/subtract/intersect/   -->> / col name, position, schema match /
   union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col name / position, schema can differ / missing col handle with null values
>> add new column, drop columns, column name change, column type change 
>> terms - initcap / lower / trim / lpad(col("Name"), 10, "X") / coalesce('cn1', 'cn2','cn3')
>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName)) / concat_ws(' | ', col("Region"), col("Country"))
>> df.dropna(how="all/any", subset=["age"]).show()
   df.fillna({'name': 'suresh', 'age': 40}).show()   /   avg_value = df.select(avg("age")).collect()[0][0]
   df.replace(to_replace=['Alice', 'Bob'], value=['suresh', 'unknown']).show()
>> when&otherwise -->> df = df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult"))
>> agg 	 	   ----> df.select(sum('CN')).show()  		   ------> note: ignore null values ----> sumDistinct, countDistanct
   grouping and agg----> df.groupBy('CN').agg(sum('CN')).show()   	
   window functions----> win_spe=Window.partitionBy("category").orderBy(asc("timestamp")).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) / (-2, 3)
				df = df.withColumn("row_number", row_number().over(window_spec)) 
   	max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) / collect_list('cn') / collect_set('cn') / first_value('cn') / last_value(cn) / nth_value(cn, n)  /
	/ row_number() / rank() / dense_rank() / lag(cn) / lead(cn, 2) / ntile(n) / cume_dist() / percent_rank()
>> splitng column, array_length (size), word in array (array_contains), explode/explode_outer, indexing: [0] or .getItem(0)
>> Pivot: df.groupBy('Month').pivot('product').sum('sales').show()
   unpivot: unpivot_exp = "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (Product, Sales)"  ------> df1.selectExpr("Month", unpivot_exp).show()
>> Dates

================================================> spark-sql 

>> spark temp table (exists only for SS) / global temp table (exists all SS within same app)
   spark.sql('show databases') --> spark.sql('use db_name') --> spark.sql('show tables') --> spark.sql('show tables in global_temp')
   df.createOrReplaceTempView('customers') -->	df.createOrReplaceGlobalTempView('gcustomers')
   spark.sql('select * from customers limit 5') or spark.table('customers') --> spark.sql('describe extended customers') --> spark.sql('drop table customers') 		
>> Spark Persistant: Managed table (spark owns both metadata and data)
   spark.sql('''CREATE TABLE custm (customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) USING CSV''')
   df.write.mode('overwrite').saveAsTable('default.customers_sur')
   data: hdfs://sur-dp-clu11-m/user/hive/warehouse/customers 
   metadata: vi etc/hive/conf/hive-site.xml -> connect mysql -u hive -p  -> Pw1+fgC9gJI+ -> show databases; use metastore; show tables; select * from TBLS; exit
>> Spark Persistant: external table (spark owns metadata and gcs/hdfs owns data)
   spark.sql(''' create external table custe (customer_id STRING, ...) using csv location 'gs://surdatabuk/data/olist/olist_customers_dataset.csv' ''')

=========================================>>  HiveQL: 

>> sql like interface (HQL) --> translator ---> MR/spark/tej code (is abstraction for java or MR programe) >> cli: set hive.execution.engine=tez/spark/mr;
   table --> data (hdfs, s3, gcs) + metadata (hive stores metastore in it). 
>>  Hive external table (data file in GCS)
   create external table custe (customer_id string, ...) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location 'gs://surdatabuk/data/olist/cust_exe/'
>> Hive managed table (data in hdfs --> hdfs dfs -ls /user/hive/warehouse/  --> set hive.metastore.warehouse.dir; )
   create table custm (	order_id string, ....) STORED AS TEXTFILE;
>> connect: 1.hive & 2.beeline : prod rec --> !connect jdbc:hive2://<hostname>:<port>/<database> / !connect jdbc:hive2://localhost:10000/default (hive & Pw1+fgC9gJI+)

-- hive is database like mysql?		its not a database. but, its DWH tool built on top of Hadoop to query large datasets from HDFS storage
-- hive is replacement for Hadoop? 	No, buit on top of hadoop core components (hdfs, yarn, mr/spark/tej)
-- hive queries like normal sql queries in db?  	HQL queries --> translatior --> mr/spark/tej code which runs on distributed system
-- hive can perform row-level transaction like mysql? 	yes, but, hive is not designed for it (if u change one row- it will recreate entire partition file)
-- hive can works with only HDFS?  			no. HDFS/s3/gcs/data lake
-- hive tables work like noraml database tables? 	hive is metastore and not storing any tables

==========================================================================================================================================================================

>> Spark performance tuning

1. master in reading query plan
	syntax check ---> unresolved logical plan --> catalyst optimizer --> optimal logical plan (filter push down & req col) --> several pysical plan -->
	based on cost model select best pysical plan  --> executes and create RDD/DF
	df.explain(True)

2. master in reading spark dags

3. Tune no of shuffle partitions (optimal SP size b/w 1 to 200 mb )
	* if data per SP is large - cores = 20, SP = 200, data = 300 gb - data/sp = 1.5gb - no of SP = 300*1000/200 = 1500 SP
	* if data per SP is large - cores = 12, SP = 200, data = 50 mb - data/sp = 0.25 mb - no of SP=50/5=10 or data/SP = 50/12 = 4.5 mb

4. Data Skew (job taking time, uneven resource utilization, OOM/data spill)
	AQE(>3.0) - uses runtime statistics - choose most efficient query plan
		-> tuning no of SP - 15 keys - coalesce(15) - spark.sql.adaptive.enabled, spark.sql.adaptive.coalescePartitions.enabled
		-> optimize joins - SMJ to BJ - df1.repartiton(4), df2 broadcasted on all executors - spark.sql.autoBroadcastJoinThreshold
		-> optimize Skew joins - breaks larger part into smaller part - spark.sql.adaptive.skewJoin.enabled
	Salt - adding randomness (key) to distribute uneven data evenly.
		-> add salt columnn - hash(key, salt)% SP

5. Partitioning: create folders for each country within files are equal to no of partitions - Filtering	(Adv: fast access, parallesium/resource utilization)
   df.write.mode('overwrite').partitionBy('country').parquet('/content/ecommer/') >> high cordinality - small file problem - buckting is rec
   Bucketing: Joins and aggregations, filter - hash(product_id)%4  
   df.write.mode('overwrite').bucketBy(2, 'product_id').sortBy("product_id").format("parquet").saveAsTable('bucketed_table')  - 2buc*2part=4files
   Q/A - optimal number of buckets = size of dataset / optimal bucket size (128-200mb)  
   Q/A - once df is bucketed, no shuffle in groupby & join, scan one buckets when you filter instead of all 

6. executor tuning: 
   Rules:leave 1C1gbM per Node for hadoop/yarn/os - leave 1Exe or 1C1gbM for app master container at cluster level - 3 t0 5 cores/exe - overhead mem max(384 or 10% exe mem)
   12c48gb/node - 11c47gb/node-55c235gb-54c234gb-total exe=54/5=10 & mem/exe=234/10=23.4-actual memory/exe=23-2.3=20gb

7.Spark memory management
	submit spark app in yarn cluster --> yarn rm allocates app container and starts driver JVM 
	spark.driver.memory --> JVM memory 
	spark.driver.memoryOverhead --> 10% 0r 384mb max one --> used for container processes, pyspark app
	>> Spark executor container: sum of below 3 are the total memory of executor container
	1. on-heap memory managed by JVM (8 gb) [spark.executor.memory ] - when on-heap memory is full, operations paused and do gcc then resume operation - perfor reduce
		reserved memory 300 mb --> fixed reserve for spark engine
		spark memory (unified memory) [spark.memory.fraction] (0.6 default, u can change)--> used DF operation & caching
			storage memory pool [spark.memory.storageFraction], (0.5 default, u can change)
			executor memory pool --> 
		user memory --> used for spark internal metada, udf, rdd operations, variables, objects
	2. overhead [spark.executor.memoryOverhead  --> 10% of spark executor memory 0r 384mb max one], for container processes, n/w transfer, read shuffle, python worker 
	   spark.executor.pyspark.memory --> pyspark memory --> default zero (pyspark is non jvm. so it will take from overhead memory)
	3. off-heap mwmory: managed by OS [spark.memory.offHeap.enabled, spark.memory.offHeap.size - default 0, 10to20% of on-heap memory].
	   Off-heap memory can reduce garbage collection overhead, leading to better performance and lower latency

	yarn.scheduler.maximum-allocation-mb --> pysical memory limit at the worker node
	yarn.nodemanager.resource.memory-mb 

8. static partition pruning: partition on listen date data on disk - this scans only listen date that user eants
   dynamic partition pruning: send partition that is supossed to be scanned
	release date table --> filter to get requ data which comes to know during run time --> spark uses results to scan on other dataset based on listen date
	Adv: reduce time to scan/process)
	limits: one of dataset must be partitioned and based on column that is filered from other datase	
	SS: .config("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true") \

9. cache/persist, Use DataFrame/Dataset over RDD, Avoid User Defined Functions (UDFs)

=====================================================================================================================================================

>> 
spark = SparkSession.builder.appName('surapp11').config('spark.sql.files.maxPartitionBytes', '3mb').getOrCreate()
spark.sparkContext.applicationId, spark.stop(), spark application -list, spark application -kill app_id
df.explain(True)
spark_new_app_session = spark.newSession()
df.rdd.getNumPartitions()
spark.conf.get('spark.sql.files.maxPartitionBytes') / rdd.getNumPartitions() / spark.sparkContext.defaultMinPartitions 

gcloud: --properties=spark:spark.executor.cores=4,spark:spark.executor.memory=4g,spark:spark.executor.instances=2
.config('spark.driver.memory', '4g')  \ .config('spark.driver.maxResultSize', '2g') \
.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores
.config('spark.memory.fraction', 0.8) \ .config('spark.memory.storageFraction', 0.2) \
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10 * 1024 * 1024)  # 10 MB

>> dataproc cluster: stagging bucket --> cluster metainfo, notebooks	temp bucket --> MR/spark job history, YARN logs
>> validate pyspark exit() / spark-sql exit; / 
>> execute gcs script: master node: spark-submit gs://buk/pysp1.py   >> submit pyspark job using UI >> gcloud dataproc jobs submit pyspark pysp1.py --cluster --region

==========================================================================================================================================================================

>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
   what is transformation and action?      
	How does Lazy Evaluation work in PySpark?      
	What are wide and narrow transformations in PySpark?
   	reduceByKey vs groupByKey?		
	increase and decrease no of partions?		
	spark read data and partitioning?
	spark jobs (no of actions), stages (no of wide trans + 1), tasks (no of partitions used) 
>> key-benfits compared to low-level api  
	reading data in spark is action or transformation?
	read modes: option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record") // option("mode", DROPMALFORMED/FAILFAST")
>> df.cache()  # Stores in memory, spills to disk if needed  -->> df.persist(StorageLevel.MEMORY_AND_DISK_SER_3)   (6)
>> shuffle operations - redistributing data across different nodes in a cluster, which is necessary for certain transformations like groupBy, join, and distinct.
>> broadcast join - small dataset is broadcasted to all worker nodes - allows to perform join locally - reduced data shuffling across the network - improved performance
>> map() returns a single value for each input element / flatMap() return multiple values for each input element
>> .ðœð¨ð¥ð¥ðžðœð­() work?  --> gathers all partitions of RDD and brings them to the driver node. - data is no longer distributed, instead stored in the memory of the driver program
>> Difference between sortby and orderby? - sortBy is used with RDDs and orderBy is used with DataFrames.
>> what is the default block size (128 MB) in hdfs and how to change it?:   Edit the hdfs-site.xml File
>> SparkContext: entry point for RDD operations/functionality. SparkSession: Provides a unified entry point for DataFrame and Dataset APIs, SQL queries, and more. 
>> What is SparkSQL, and how do you perform SQL operations on DataFrames? --> is a module in Apache Spark that allows you to run SQL queries on large datasets.

======================================================================================================================================================================

year(col("timestamp_str"))  -  month / day / hour / minute / second 
date_trunc("month", "date_col") 
date_diff("end_date", "start_date") / round((unix_timestamp("end_ts") - unix_timestamp("start_ts")) / 3600, 2)
date_sub("start_date", 10) / date_add("start_date", 10) / expr("ts_col + interval -2 year")

>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> What are the different ways to remove duplicate records ---> distinct() / dropDuplicates(["name", 'id']) / row_number() / groupBy /
>> find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 
>> split the data into two columns (Even, Odd) 
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> Retrieve employees who joined in the last 6 months.
>> Identify employees whose salary is greater than the average salary of their department.
>> Get the first and last transaction date for each customer.
>> Replace null values in a column with the last non-null value in that partition.

>> Categorize employees based on years of experience into Junior, Mid, and Senior levels.
>> query to replace null values in a specific column with the previous non-null value. -----> last('value', ignorenulls=True).over(window_spec)
>> Fill the mean salary value in Null. ---> [("Alice", 50000), ..] --> avg_sal = df.select(mean(col('sal'))).collect()[0][0] --> df.fillna({'salary': avg_salary}).show()
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]
Replace all NULL values in the Quantity column with 0 / Price column with the average price of the existing data / Fill missing Sales_Date with '2025-01-01'.
Drop rows where the Product column is NULL / Drop rows where all columns are NULL.


>> Word count program in pyspark rdd & DF
>> count the number of null values in each column of df. 	df.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()
>> display only last 4 characters of card number. data = [(1,'Rahul',1234567891234567),(2,'Raj',1234567892345678)] schema = "id int, name string, card_no long"
	maskfun_udf = udf(maskfun, StringType()) 	 /	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))
>> extract filename: df = df.withColumn("state", element_at(split(input_file_name(), "/"), -1)).withColumn("state", split("state", "\\.").getItem(0))



>> find max id excluding duplicates | 2, 6, 5, 6, 9, 9, 8: with unique_ids AS (SELECT id FROM id_list GROUP BY id HAVING COUNT(*) = 1) | SELECT MAX(id) FROM unique_ids
>> find department names where no employees are found: emp -->> employee_id, employee_name, dept_id | Department Table -->> dept_id, dept_name
>> Given two tables, find the count of records for Left Outer Join and Inner Join | Table A  & Table B | 1 1 1 1  &  1 1 1 | Ans. 12 & 12
>> table1: 1 2 3 null ""  & table2: 2 2  3 null null "" : inner join - 2 2, 2 2, 3 3, empty empty |  Null doesnt include matching, empty string includes matching


>> column Country: India Australia Pakistan | Output: India vs Australia India vs Pakistan Australia vs Pakistan
    SELECT a.country || ' vs ' || b.country as country FROM country a  join country b on a.country > b.country;  
>> number of patients per doctor including unassigned patients: docter_id | pationt_id, doctor_id
   SELECT d.doctor_id, COUNT(p.patient_id) AS total_patients FROM doctors d  LEFT JOIN patients p ON  d.doctor_id = p.doctor_id GROUP BY d.doctor_id
>> Handling NULL values using the average salary | emp_id, salary: IFNULL(cn, rep_val) COALESCE(cn, rep_val1, rep_val2, ...) 
   SELECT employee_id,  COALESCE(salary, (SELECT avg(salary) FROM table)) AS adjusted_salary  FROM table;
>> Src, beforeTgt, tgt after src ing: MERGE INTO target_table AS T  USING source_table AS S  ON T.id = S.id	
   WHEN MATCHED THEN UPDATE SET T.name = S.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);
>> ORDER_DAY, ORDER_ID, PRODUCT_ID, QUANTITY, PRICE | Get me all products that got sold both the days and the number of times the product is sold.  
   SELECT PRODUCT_ID, COUNT(*) AS COUNT FROM orders WHERE ORDER_DAY IN ('01-JUL-11', '02-JUL-11') GROUP BY PRODUCT_ID HAVING COUNT(DISTINCT ORDER_DAY) = 2;
>> i/p - customer_id, region, sales_amount | o/p - customer_id,	north_sales, south_sales, East sales, West Sales
   SELECT customer_id, SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales, .... FROM orders GROUP BY customer_id;
   select * from (select customer_id, region, sales_amount from orders) pivot(sum(sales_amount) for region in ('north', 'south', 'east', 'west'));
>> employees sal > managers | EMPID, EMPNAME, MANAGERID, SLARY: 
   SELECT  e.employee_id,  e.name,  e.salary FROM  employees e JOIN  employees m ON e.manager_id = m.employee_id WHERE  e.salary > m.salary ORDER BY  e.salary DESC;
>> employees who hired in last 90 days: SELECT * FROM  employee_data` WHERE hire_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);
>> Extract the Domain from the Email column:	substring(email, strpos(email, '@')+1, length-opt) 
>> Find all employees whose names contain the letters "a" exactly twice: SELECT * FROM employees WHERE LENGTH(name) - LENGTH(REPLACE(LOWER(name),'a','')) = 2
>> Write a query to count how many employees share the same salary: SELECT salary, COUNT(*) AS employee_count FROM employees GROUP BY salary HAVING COUNT(*) > 1
>> emp earning more than avg sal in their department: SELECT * FROM employees e WHERE salary > (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id))
>> records where date is within last 7 days from today: SELECT * FROM `table` WHERE date_column BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) AND CURRENT_DATE()
>> tran_id, sale_date, amount | o/p- year, Jan, Feb, Mar: WITH sales_data AS (SELECT EXTRACT(YEAR FROM sale_date) AS year, FORMAT_DATE('%b', sale_date) AS month, amount        FROM sales) | SELECT * FROM sales_data PIVOT (SUM(amount) FOR month IN ('Jan', 'Feb', 'Mar'))	ORDER BY year;
>> WITH CustomOrder AS (SELECT emp_name, emp_dep, CASE WHEN emp_dep = 'it' THEN 1 WHEN emp_dep = 'hr' THEN 2 WHEN emp_dep = 'sales' THEN 3 end as emp_dept_sort,     
   ROW_NUMBER() OVER(PARTITION BY emp_dep ORDER BY emp_name) AS row_numÂ  FROM table) | SELECT * FROM CustomOrder ORDER BYÂ emp_dept_sort, row_num;
>> employees same sal as another emp in same dept | dept_id, emp_name, salary: WITH es AS (SELECT dept, sal, COUNT(*) FROM employees GROUP BY dept, sal HAVING COUNT(*) > 1)    | SELECT * FROM employees e JOIN es es ON  e.dept_id = es.dept_id AND e.salary = es.salary;
>> find customers who made transactions in every month of the year | cust_id, date, purchage_amount
   EXTRACT(MONTH FROM transaction_date) | GROUP BY customer_id, month | customer_id, COUNT(month) | WHERE months_count = 12;
>> find the first and last occurrence of each event per user | UserID, EventID, EventDate,
   ROW_NUMBER() for rn_first |  ROW_NUMBER() for rn_last | MIN(CASE WHEN rn_first = 1 THEN event_time END) AS first_occurrence
>> Write a query to detect a sudden drop (>30%) in daily revenue for any restaurant | restaurant_id, revenue_date, daily_revenue: lag to previous_revenue, sudden_drop > 30
>> Detect duplicated orders â€” same items, user, and restaurant within 5 minutes | order_id, user_id, restaurant_id, item_id, order_time:
   LAG(order_time) AS previous_order_time | TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) AS minutes_diff | minutes_diff <= 5
>> Find cities where lunch orders (12â€“3 PM) have higher revenue than dinner orders (7â€“10 PM)| order_id, user_id, city, order_time, order_amount
   order_hour |	CASE WHEN order_hour BETWEEN 12 AND 14 THEN 'lunch'  WHEN order_hour BETWEEN 19 AND 21 THEN 'dinner' ELSE NULL | pivot-city, meal-period, revenue	
>> Find restaurants whose best-selling dish makes up over 60% of their revenue | restaurant_id, dish_id,  dish_name, quantity, price
   restaurant_total_revenue, best_selling_dish,  best_dish_revenue / restaurant_revenue AS revenue_share, revenue_share > 0.6;
>> Identify consecutive login streaks for users where they logged in for at least three consecutive days | user_id, login_date
>> Identify users who have placed an order in two consecutive months but not in the third month | user_id, order_date
>> pivot & unpivot data: cricket_data = [("Virat Kohli", 'Match1', 75), ..........], col = ["Player", "Match", "score"]









