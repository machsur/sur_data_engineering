========================================================================================================================================================================
links:
https://github.com/afaqueahmad7117/spark-experiments/tree/main/spark
https://allthingdata.substack.com/p/essential-linux-commands-for-data         

========================================================================================================================================================================
>> steps to follow as data engineer:
1. Data Ingetion: load data into spark dataframe 
2  exploration: schema and datatypes, data leakage or drop, Null values, duplicate values, data distribution as per state, avg time taken to deliver item	
3. data cleaning: handle missing values, duplicate, invalid date (standadize format), data type corretion
4. data integration (combine data from multiple tables) and aggregation (to compute metrics): 
5. performance optimization: enhance data processing task - partitioning, cache/persist, spark config
6. data serving: make processed data available in bq

==========================================================================================================================================================================

===========================================> hdfs commands:

>> linux system / hdfs system
hdfs -help
hdfs dfs -help
hdfs dfs -help ls

==========================================> low-level api

>> spark.conf.get('spark.sql.files.maxPartitionBytes') / rdd.getNumPartitions() / spark.sparkContext.defaultMinPartitions 
   spark.sparkContext.defaultParallelism  (for local data)
   # 544mb => 5 blocks => 5 partition   
   #1 mb => 1 block --> 1 partition ( This is wrong)  => 2 ---> because spark.sparkContext.defaultMinPartitionsspark application -list
   spark.sparkContext.applicationId, spark application -list, spark application -kill app_id
   spark_new_app_session = spark.newSession()
>> rdd = spark.sparkContext.parallelize(data)  /  rdd = spark.sparkContext.textFile(hdfs_gcs_path) 
>> rdd1 = rdd.filter(lambda row: 'Frieza' in row) / rdd1 = rdd.filter(lambda row : row!=header)  - header = rdd.first()
   rdd1 = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)   ---> word count programme
   rdd1 = rdd.map(lambda row:row.split(',')).map(lambda row:(row[2],1)).groupByKey().map(lambda row:(row[0],len(row[1]))).collect()
   rdd1 = rdd.map(parse_row) / def parse_row(row):
>> rdd.first() / rdd.collect() / rdd.take(3) / rdd1.countByValue()
   
=============================>> high-level api

>> create spark session with config optimization
spark = SparkSession.builder \
.appName("olist config optimization") \
.config('spark.executor.memory', '6g') \
.config('spark.executor.cores', '4') \
.config('spark.executor.instances', '2') \
.config('spark.driver.memory', '4g') \
.config('spark.driver.maxResultSize', '2g') \
.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores
.config('spark.default.parallelism', '64') \		# It usually defaults to the number of cores on all executor nodes.
.config('spark.sql.adaptive.enabled', 'true') \
.config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \
.config('spark.sql.autoBroadcastJoinThreshold', 24*1024*1024) \
.config('spark.sql.adaptive.skewJoin.enabled', 'true') \
.config('spark.sql.files.maxPartitionBytes', '64MB') \
.config('spark.sql.files.openCostInBytes', '2MB') \
.config('spark.memory.fraction', 0.8) \
.config('spark.memory.storageFraction', 0.2) \
.getOrCreate()

/etc/hadoop/conf/core-site.xml

>> df=spark.range(0,10) or rdd=spark.sparkContext.rrange(0,10) / df_rdd = rdd.toDF(["customer_id", "name"]) or df_rdd = spark.createDataFrame(rdd, schema)
   custom_schema = StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("age", IntegerType(), True)])
   custom_schema = ''' ID Integer, Name String, Age Integer, Salary Double '''
   df = spark.read.option("header", True).option("inferSchema", True)/schema(cust_schema).option("mode", "PERMISSIVE").json("file:///path_to_file/data_files/")
   df.write.format('parquet').mode('overwrite').save('gs://surdatabuk/data/olist_op/')
   df.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket', 'gcs_path').option('table', 'p.d.t').save()
>> join_df = df1.join(df2, df1.department_id == df2.id/on='cn, how='inner/left/right/outer/left_semi/left_anti') 
   join_df = df1.crossjoin(df2)  /  broadcast_join = df1.join(broadcast(df2), on="id", how="inner")
>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  &|   /
>> grouped_df = df.groupBy('department').agg(max("sal"))   		
>> having_df = df.filter(grouped_df['count'] > 1)
>> df.select("customer_id", col("Name").alias('EmployeeName'), column("last_name"), df.email, df["city"] )
   df.selectExpr("id*2 as nid", "name as nname").show()
>> sorted_df = df.orderBy(asc('department')) / col("country).desc())  / asc_nulls_last('value') /
>> df_limit = df.limit(10)
>> df.show(n=3, truncate=25, vertical=True) / df.collect() / df.count() / df.printSchema() / df.columns 
>> union tables: union/subtract/intersect/   -->> / col name, position, schema match /
>> union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col name / position, schema can differ / missing col handle with null values
>> add new column, drop columns, column name change, column type change 
>> terms - initcap / lower / trim / lpad(col("Name"), 10, "X") / coalesce('cn1', 'cn2','cn3')
>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName)) / concat_ws(' | ', col("Region"), col("Country"))
>> work with dates   
>> df.dropna() /  df.fillna() / df.replace(to_replace=["banana", "cherry"], value=["orange", "grape"], subset=["fruit"])
>> when&otherwise -->> df = df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult"))
>> 
agg 	 		-------> df.select(sum('CN')).show()  		   ------> note: ignore null values ----> sumDistinct, countDistanct
grouping and agg 	-------> df.groupBy('CN').agg(sum('CN')).show()   	
window functions 	-------> window_spec = Window.partitionBy("category", "sub_category").orderBy(asc("timestamp"))
                    				     .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) / .rowsBetween(-2, 3)
				df = df.withColumn("row_number", row_number().over(window_spec)) 
   				max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) / collect_list('cn') / collect_set('cn') 
				/ first_value('cn') / last_value(cn) / nth_value(cn, n)  /
				/ row_number() / rank() / dense_rank() / lag(cn) / lead(cn, 2) / ntile(n) / cume_dist() / percent_rank()
>> splitng column, array_length (size), word in array (array_contains), explode/explode_outer, indexing: [0] or .getItem(0)
>> Pivot: df.groupBy('Month').pivot('product').sum('sales').show()
   unpivot: unpivot_exp = "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (Product, Sales)"  ------> df1.selectExpr("Month", unpivot_exp).show()

================================================> spark-sql 

>> spark temp table (exists only for SS) / global temp table (exists all SS within same app)
   spark.sql('show databases') --> spark.sql('use db_name') --> spark.sql('show tables') --> spark.sql('show tables in global_temp')
   df.createOrReplaceTempView('customers') -->	df.createOrReplaceGlobalTempView('gcustomers')
   spark.sql('select * from customers limit 5') or spark.table('customers') --> spark.sql('describe extended customers') --> spark.sql('drop table customers') 		
>> Spark Persistant: Managed table (spark owns both metadata and data)
   spark.sql('''CREATE TABLE custm (customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) USING CSV''')
   df.write.mode('overwrite').saveAsTable('default.customers_sur')
   data: hdfs://sur-dp-clu11-m/user/hive/warehouse/customers 
   metadata: vi etc/hive/conf/hive-site.xml -> connect mysql -u hive -p  -> Pw1+fgC9gJI+ -> show databases; use metastore; show tables; select * from TBLS; exit
>> Spark Persistant: external table (spark owns metadata and gcs/hdfs owns data)
   spark.sql(''' create external table custe (customer_id STRING, ...) using csv location 'gs://surdatabuk/data/olist/olist_customers_dataset.csv' ''')

=========================================>>  HiveQL: 

>> sql like interface (HQL) --> translator ---> MR/spark/tej code (is abstraction for java or MR programe) >> cli: set hive.execution.engine=tez/spark/mr;
   table --> data (hdfs, s3, gcs) + metadata (hive stores metastore in it). 
>>  Hive external table (data file in GCS)
   create external table custe (customer_id string, ...) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location 'gs://surdatabuk/data/olist/cust_exe/'
>> Hive managed table (data in hdfs --> hdfs dfs -ls /user/hive/warehouse/  --> set hive.metastore.warehouse.dir; )
   create table custm (	order_id string, ....) STORED AS TEXTFILE;
>> connect: 1.hive & 2.beeline : prod rec --> !connect jdbc:hive2://<hostname>:<port>/<database> / !connect jdbc:hive2://localhost:10000/default (hive & Pw1+fgC9gJI+)

-- hive is database like mysql?		its not a database. but, its DWH tool built on top of Hadoop to query large datasets from HDFS storage
-- hive is replacement for Hadoop? 	No, buit on top of hadoop core components (hdfs, yarn, mr/spark/tej)
-- hive queries like normal sql queries in db?  	HQL queries --> translatior --> mr/spark/tej code which runs on distributed system
-- hive can perform row-level transaction like mysql? 	yes, but, hive is not designed for it (if u change one row- it will recreate entire partition file)
-- hive can works with only HDFS?  			no. HDFS/s3/gcs/data lake
-- hive tables work like noraml database tables? 	hive is metastore and not storing any tables

==========================================================================================================================================================================

>> dataproc cluster: stagging bucket --> cluster metainfo, notebooks	temp bucket --> MR/spark job history, YARN logs
>> validate pyspark exit() / spark-sql exit; / 
>> execute gcs script: master node: spark-submit gs://buk/pysp1.py   >> submit pyspark job using UI >> gcloud dataproc jobs submit pyspark pysp1.py --cluster --region
>> sPARK query plan: code syntax -> unresolved logical plan --> catalog --> optimized logical plan -> convert pysical plans --> cost --> best pysical (df.explain(True))
>> Spark memory management
	submit spark app in yarn cluster --> yarn rm allocates app container and starts driver JVM 
	spark.driver.memory --> JVM memory 
	spark.driver.memoryOverhead --> 10% 0r 384mb max one --> used for container processes, pyspark app
	>> Spark executor container: sum of below 3 are the total memory of executor container
	1. on-heap memory managed by JVM (8 gb) [spark.executor.memory ] - when on-heap memory is full, operations paused and do gcc then resume operation - perfor reduce
		reserved memory 300 mb --> fixed reserve for spark engine
		spark memory (unified memory) [spark.memory.fraction] (0.6 default, u can change)--> used DF operation & caching
			storage memory pool [spark.memory.storageFraction], (0.5 default, u can change)
			executor memory pool --> 
		user memory --> used for spark internal metada, udf, rdd operations, variables, objects
	2. overhead [spark.executor.memoryOverhead  --> 10% of spark executor memory 0r 384mb max one], for container processes, n/w transfer, read shuffle, python worker 
	   spark.executor.pyspark.memory --> pyspark memory --> default zero (pyspark is non jvm. so it will take from overhead memory)
	3. off-heap mwmory: managed by OS [spark.memory.offHeap.enabled, spark.memory.offHeap.size - default 0, 10to20% of on-heap memory].
	   Off-heap memory can reduce garbage collection overhead, leading to better performance and lower latency

	yarn.scheduler.maximum-allocation-mb --> pysical memory limit at the worker node
	yarn.nodemanager.resource.memory-mb 

>> executor tuning: thin exe-45, fat exe-3, optSizedExe- leave 1C1gbM per Node for hadoop/yarn/os - leave 1Exe or 1C1gbM for app master container at cluster level 
   3N16C48gbM, totC=16-1(os)*3=45, totM=48-1(os)*3=141gb, exe=45/4~11, M/exe=141/11~12gb, for app driver 1exe (exe=10, cores=4, memory = 12gb)
   file size=25GB, cores=25*1024*1024/128 =200, executor=200/4=50, exeMemory=1.5*300(RM)or4*128(PZ)=512mb/core=2gb/exe, totMemoryReq=50*2=100gb
>> shuffling: happenned when the wide transformation is performed and partitions after shufflling known as Shuffle Partition
   if data per sp is large - Increase no of shuffle partitions --> if data per sp is small - decrease no of shuffle partitions
>> Partitioning: Divides based on column values - Separate folders - Filtering	(Adv: fast access, parallesium/resource utilization)
   df.write.mode('overwrite').partitionBy('listen_date').parquet('/content/ecommer/') >> high cordinality - small file problem - buckting is rec
   Bucketing: Divides based on hash of column  - Fixed number of files (buckets) - Joins and aggregations.
   df.write.mode('overwrite').bucketBy(2, 'order_status').saveAsTable('bucketed_table')   --->  6 partitions * 2 buckets = 12 files
   Q/A - optimal number of buckets = size of dataset / optimal bucket size (128-200mb)   dataset size,mb = (no_records * no_col * avg width of var in bytes) / (1024 * 1024)
   Q/A - once df is bucketed, no shuffle in groupby & join, scan one buckets when you filter instead of all
>> what is data skewness? most of data goes to few partitions (based on keys) after data shuffling. job is taking time, unutilization of resources, oom or data spill 
   repartition:
   salt: decide salt number : [0, 3) --> ds1: add salt column to key column -->	ds2: add salt column (having array [0, 1, 2] - explode ) to key column
   AQE (>3.0): based on runtime stat select best query plan: tuning shuffle part (200 default- 15 keys-red 15) --> converts SMJ to BJ --> opti skew joins by splitting  
>> 
static partition pruning: partition on listen date data on disk - this scans only listen date that user eants
dynamic partition pruning: send partition that is supossed to be scanned
	release date table --> filter to get requ data which comes to know during run time --> spark uses results to scan on other dataset based on listen date
	Adv: reduce time to scan/process)
	limits: one of dataset must be partitioned and based on column that is filered from other datase	
	SS: .config("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true") \

==========================================================================================================================================================================

>> why bigdata - growth of internet, social media & IOT devices
>> (1.1) desiging good bigdata system, onpremise vs cloud, DB vs DWH vs DL, ETL vs ELT
>> Hadoop Ecosystem (2)
>> Map Reduce and YARN (3)
>> limitations of MR, Spark EcoSystem (4)
	What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
	what is transformation and action
	How does Lazy Evaluation work in PySpark?
	What are wide and narrow transformations in PySpark?
	reduceByKey vs groupByKey
	increase and decrease no of partions
	spark read data and partitioning
	spark jobs (no of actions), stages (no of wide trans + 1), tasks (no of partitions used) 
>> Spark higher level api (DF, DS, spark-sql) (5)
	key-benfits compared to low-level api
	reading data in spark is action or transformation?
	read modes: "PERMISSIVE" - Puts corrupt records in a special column (e.g., _corrupt_record), "DROPMALFORMED": Drops rows, "FAILFAST": Fails immediately
	option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record") // option("mode", DROPMALFORMED/FAILFAST")
>> cache() - when default memory-disk behaviour is sufficient (6)
   persist - when we need to control how data is stored - MEMORY_AND_DISK_SER_3
>> shuffle operations - redistributing data across different nodes in a cluster, which is necessary for certain transformations like groupBy, join, and distinct.
>> broadcast join - small dataset is broadcasted to all worker nodes - allows to perform join locally - reduced data shuffling across the network - improved performance
>> map() returns a single value for each input element / flatMap() return multiple values for each input element
>> .ðœð¨ð¥ð¥ðžðœð­() work?  --> gathers all partitions of RDD and brings them to the driver node. - data is no longer distributed, instead stored in the memory of the driver program
>> Difference between sortby and orderby? -    sortBy is used with RDDs and orderBy is used with DataFrames.
>> what is the default block size (128 MB) in hdfs and how to change it?:   Edit the hdfs-site.xml File
>> SparkContext: entry point for RDD operations/functionality. SparkSession: Provides a unified entry point for DataFrame and Dataset APIs, SQL queries, and more. 
>> What is SparkSQL, and how do you perform SQL operations on DataFrames? --> is a module in Apache Spark that allows you to run SQL queries on large datasets.
>> what is collect list and collect set and when do we use it?
>> what are the optimization techniques in pyspark?
	use col format files, Use DataFrame/Dataset over RDD, filter data erly, Cache/persist data, use broadcast Joins
	Optimize shuffle Partitions (Repartition / Coalesce enable AQE, salting)
	partitioning & bucketing for frequently accessed based on specific column, Avoid User Defined Functions (UDFs)

======================================================================================================================================================================

>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> Write a PySpark code to remove duplicate records based on a specific column.   data = [(101, "Mumbai", "Maharashtra"), ]
>> Write a PySpark query to calculate the moving average of sales over the last 3 months:   order_id, date, sale_amount
>> pivot & unpivot data: cricket_data = [("Virat Kohli", 'Match1', 75), ..........], col = ["Player", "Match", "score"]
>> write a PySpark code snippet to find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> remove duplicate records from a df based on a composite key consisting of 'customer_id' and 'transaction_date'. | customer_id| transaction_id | transaction_date |
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 	-----> output:   | 1|Sagar-Prajapati| Sagar|Prajapati|
>> Given a DataFrame, split the data into two columns (Even, Odd) 
>> Create a DataFrame with two columns: Column 1: Default String Column 2: Default Integer
>> query to replace null values in a specific column with the previous non-null value. -----> last('value', ignorenulls=True).over(window_spec)
>> reading file into df / write df into target
>> rank students within each state based on their scores in descending order.   student_id, student_name, state, score
>> Fill the mean salary value in Null. ---> [("Alice", 50000), ..] --> avg_sal = df.select(mean(col('sal'))).collect()[0][0] --> df.fillna({'salary': avg_salary}).show()
>> What are the different ways to remove duplicate records in PySpark? ---> distinct() / dropDuplicates(["name", 'id']) / row_number() / groupBy("name") /
>> Explain `groupBy()`, `agg()`, and `pivot()` functions with an example.
>> remove duplicate rows based on the composite key (cust_id, order_id) & retain row with latest order_date for each combination: cust_id, order_id, order_date, amount
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> find the total salary paid in each department.  --->  emp_id`, `name`, `department`, `salary`, and `city`.  
>>  (1, 'Laptop', 800)  ---->  count number of products in each category / based on its price into three categories below. 
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]
Replace all NULL values in the Quantity column with 0 / Price column with the average price of the existing data / Fill missing Sales_Date with '2025-01-01'.
Drop rows where the Product column is NULL / Drop rows where all columns are NULL.
>> ["order_id", "state", "city", "product_id", "category", "units", "unit_price", "order_date"]
	1. Calculate the total revenue per state and category. (Revenue = units Ã— unit_price)
	2. For each state, identify the category that has the highest total revenue.
	3. For each state-category pair, calculate the average units sold per order.
	4. From the results, list the states along with the category that generated the highest revenue, and the average units sold per order for that category.
	5. Sort the final output by total revenue in descending order.
>> Word count program in pyspark rdd & DF
>> count the number of null values in each column of df. 	df.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()
>> display only last 4 characters of card number. data = [(1,'Rahul',1234567891234567),(2,'Raj',1234567892345678)] schema = "id int, name string, card_no long"
	maskfun_udf = udf(maskfun, StringType()) 	 /	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))
>> extract filename: df = df.withColumn("state", element_at(split(input_file_name(), "/"), -1)).withColumn("state", split("state", "\\.").getItem(0))



