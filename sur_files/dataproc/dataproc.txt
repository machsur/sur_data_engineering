========================================================================================================================================================================
links:
https://github.com/afaqueahmad7117/spark-experiments/tree/main/spark
https://allthingdata.substack.com/p/essential-linux-commands-for-data         

==========================================================================================================================================================================
commands:

hdfs dfs -ls -R /public/retail_db
hdfs dfs -mkdir /path/to/new_directory
hdfs dfs -copyFromLocal /local/path/to/file /hdfs/path/to/destination 		or
hdfs dfs -copyToLocal /hdfs/path/to/file /local/path/to/destination      or
hdfs dfs -put data/retail_db /public (local to hdfs -put)
hdfs dfs -cp gs://surretail/retail_db /public/retail_db (hdfs to hdfs, gcs to hdfs, hdfs to gcs, gcs to gcs -cp)
hdfs dfs -mv /path/to/source /path/to/destination
hdfs dfs -rm -R -skipTrash /public/retail_db
hdfs dfs -cat /path/to/file
hdfs dfsadmin -report
hdfs dfs -df -h
hdfs dfs -du -s -h /path/to/directory
hdfs dfs -chmod 755 /path/to/file_or_directory


==> low-level api
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('WordCountDemo').master('yarn').getOrCreate()
rdd = spark.sparkContext.parallelize(data)
rdd = spark.sparkContext.textFile(hdfs_path)   ---> hdfs_path = '/tmp/inputhdfsdbz.txt'
rdd_hdfs_mapped = rdd_hdfs.map(lambda line:line.split(" "))
rdd1 = rdd.map(lambda line: line.split(','))
rdd1 = rdd.flatMap(lambda line: line.split(','))
rdd1 = rdd.filter(lambda row: 'Frieza' in row) / rdd1 = rdd.filter(lambda row : row!=header)  - header = rdd.first()
rdd1 = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)   ---> word count programme
rdd1 = rdd.map(lambda row:row.split(',')).map(lambda row:(row[2],1)).groupByKey().map(lambda row:(row[0],len(row[1]))).collect()

rdd1 = rdd.map(parse_row)
def parse_row(row):
    fields = row.split(',')
    return (int(fields[0]), fields[1], fields[2], fields[3], fields[6]=='True')

rdd.first()
rdd.collect()
rdd.take(3)
rdd1.countByValue()

>>
# spark.conf.get('spark.sql.files.maxPartitionBytes')
# 544mb => 5 blocks => 5 partition  ----> big_rdd_from_hdfs.getNumPartitions() 
#1 mb => 1 block --> 1 partition ( This is wrong)  ---> small_rdd_from_hdfs.getNumPartitions()  
	--> because spark.sparkContext.defaultMinPartitions  / spark.sparkContext.defaultParallelism

>> Q/A- # In this case we will have to start from Scratch only
rdd1 = rdd_big_file.flatMap(lambda line:line.split(' '))
rdd1 = rdd1.map(lambda word:(word,1))
rdd1 = rdd1.reduceByKey(lambda a,b : a+b) 
   
==>> high-level api
    df = spark.createDataFrame(data, columns)
    df_from_pandas = spark.createDataFrame(pandas_df/data_dict)
    df_empty = spark.createDataFrame([], columns)
    df_csv = spark.read.csv("/path/to/file.csv", header=True, inferSchema=True)
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
    custom_schema = StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("age", IntegerType(), True)])
    custom_schema = ''' ID Integer, Name String, Age Integer, Salary Double '''
    df = spark.read.csv("your_file.csv", schema=custom_schema, header=True)
    file_paths = ["file1.csv", "file2.csv", "file3.csv"] 
    df = spark.read.csv(file_paths, header=True, inferSchema=True, sep=',')
    df = spark.read.option("modifiedBefore", "2022-02-27T05:30:00").json("file:///path_to_file/data_files/")
    ==>> join_df = df1.join(df2, df1.department_id == df2.id/on='cn, how='inner/left/right/outer/left_semi/left_anti') 
         join_df = df1.crossjoin(df2)
         broadcast_join = df1.join(broadcast(df2), on="id", how="inner")
    ==>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  &|   /
    ==>> grouped_df = df.groupBy('department').agg(max("sal"))   		
    ==>> having_df = df.filter(grouped_df['count'] > 1)
    ==>> selected_df = df.select("customer_id", col("Name").alias('EmployeeName'), column("last_name"), df.email, df["city"] )
                    = df.selectExpr("id*2 as nid", "name as nname").show()
    ==>> sorted_df = df.orderBy(asc('department'), col("country).desc())  / df11.orderBy(asc_nulls_last('value')).show() /
    ==>> df_limit = df.limit(10)
    ==>> df.show(n=3, truncate=25, vertical=True) / print(df.collect()) / print(df.count()) / df.printSchema() / display(df) / print(df1.columns) 


	==>> union tables: union_distinct = df1.union(df2).distinct()  /union/subtract/intersect/   -->> positional col match / schema must be identical
	     union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col match by name / schema can differ / missing col handle with null values
	     distanct values/drop duplicates ----> df.select('cn1', 'cn2').distinct() ----> df.dropDuplicates(['cn1', 'cn2'])
	==>> 
	add new column ---> newdf = df.withColumn("NewColumn", lit(1))
	drop columns -->> df2 = df.drop("Country", "Region")
	column name change -->> new_df = df.withColumnRenamed("oldColumnName", "newColumnName")
	column type change --> df = df.withColumn("Phone", col("Phone").cast("string"))
	==>> case sensitive - df.select(initcap(col("country")))  -->> initcap / lower / upper
	==>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName))
			  --> df.select(concat_ws(' | ', col("Region"), col("Country")))
	==>> result_df = df.select( col("EmployeeID"), ltrim(col("Name")), lpad(col("Name"), 10, "X"))  -->> rtrim / trim / rpad
	==>> create date, timestamp columns in df, add/subtract days, 
		withColumn("date", to_date(df["date_string"], "yyyy-MM-dd"))
		withColumn("month", month(df["date"]))
		withColumn("truncated_timestamp", date_trunc("hour", df["timestamp"]))
		datediff(df["end_date"], df["start_date"])
		withColumn("months_diff", months_between(df["end_date"], df["start_date"]))  
		withColumn("timestamp_diff", unix_timestamp(df["end_timestamp"]) - unix_timestamp(df["start_timestamp"]))      
	==>>    df.dropna("All" subset=['cn1", "cn2"]) 
		df.fillna("N/A" subset=['cn1", "cn2"]) / df.na.fill({'cn1':'val1', 'cn2': 'val2'}) / is null('CN') 
		df_replaced_multiple = df.replace(to_replace=["banana", "cherry"], value=["orange", "grape"], subset="fruit")
	==>> coalesce('cn1', 'cn2','cn3') - return first non-null value from list of columns
	     df.gruopBy("region").agg(coalesce(mean("units_sold), lit(0))) 
	==>> when&otherwise -->> df = ( df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult")).withColumn("income_bracket", when(df.salary < 4000, "Low") 					.when((df.salary >= 4000) & (df.salary <= 4500), "Medium") .otherwise("High")) )
	==>>cast data types -->> df = df.withColumn("column_name", col("column_name").cast("target_data_type"))
			    -->> cast_expr = [col("column1_name").cast("target_data_type1"), col("column2_name").cast("target_data_type2")] / df = df.select(*cast_expr)
	==>> agg functions 
		agg ------->  df.select(sum('CN')).show()  ------> note: ignore null values
		grouping and agg -----> df.groupBy('CN').agg(sum('CN')).show()
		sum , sumDistinct, min, max, avg, count, countDistanct, collect_list, collect_set first, last, stddev, variance
	==>> window functions -->> window_spec = Window.partitionBy("category", "sub_category").orderBy(asc("timestamp"), col("score").desc()) \
                    				.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
				   window_spec = Window.partitionBy("category", "sub_category").orderBy(col("timestamp"), col("score")).rowsBetween(-2, 3)
				   df = df.withColumn("row_number", row_number().over(window_spec)) 
   		max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) / collect_list / collect_set / first / last /
		row_number() / rank() / dense_rank() / 
		lag(cn) / lead(cn, 2) / 
		first_value(cn) / last_value(cn) / nth_value(cn, n) /
	==>> splitng column -->> df.withColumn('nc', split(col('name'), '[ ,]'))   -->> [0] or .getItem(0), explode, 
	     Array length: df.withColumn('nc', size(split(col('name'), '[ ,]')))
	     word in array or not: df.withColumn('nc', array_contains(split(col('name'), '[ ,]'), 'macha'))
	     indesxing: df.withColumn('nc', split(col('name'), '[ ,]')[0])  /  df.withColumn('nc', split(col('name'), '[ ,]').getItem(0)).show()
	     explode/explode_outer: df.withColumn('nc', explode(split(col('name'), '[ ,]')))  /  df.withColumn('nc', explode_outer(split(col('name'), '[ ,]')))
	     Drops rows with null or empty arrays / Keeps rows with null or empty arrays, filling with null
	==>> Pivot: df.groupBy('Month').pivot('product').sum('sales').show()
	     unpivot: unpivot_exp = "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (Product, Sales)"
 		      df1.selectExpr("Month", unpivot_exp).show()
	==> 
		regexp_extract: df.withColumn("file_name", regexp_extract("file_path", r'^(.*)_\d{14}\.csv$', 1))
		regexp_replace: df.withColumn("new_file_path", regexp_replace("file_path", r'_\d{14}\.csv$', '_TIMESTAMP.csv'))
		regexp_contain: df.withColumn("contains_digits", when(regexp_extract(col("text"), r'\d', 0) != "", True).otherwise(False))
		regexp_substr: df = df.withColumn("first_digits", regexp_extract("text", r'\d+', 0))

==> spark-sql 
spark = SparkSession.builder.appName('sur rdd1').getOrCreate()
spark_new_app_session = spark.newSession()
spark.sparkContext.applicationId
>
>> spark temp table / global temp table
df.createOrReplaceTempView('customers') 		/ 	df.createOrReplaceGlobalTempView('gcustomers')
spark.sql('show databases').show()
spark.sql('use db_name')
spark.sql('show tables').show()   			/   	spark.sql('show tables in global_temp').show()
spark.sql('select * from customers limit 5').show()   	/    	spark.sql('select * from global_temp.gcustomers limit 5').show()
spark.sql('describe customers').show()     		/       spark.sql('describe global_temp.gcustomers').show()
spark.sql('describe extended customers').show()  	/ 	spark.sql('describe extended global_temp.gcustomers').show()
spark.sql('drop table customers') 			/ 	spark.sql('drop table global_temp.gcustomers')

>> Spark Persistant: Managed table
spark.sql('''
CREATE TABLE IF NOT EXISTS customers (
customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) 
USING CSV
''')
df.write.mode('overwrite').saveAsTable('default.customers_sur')
spark.sql('describe extended customers_sur').show(truncate=False)
spark.sql('select * from customers_sur limit 6').show()

>> Spark Persistant: external table
spark.sql('''
create external table external_customers_sur (
customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) 
using csv
location 'gs://surdatabuk/data/olist/olist_customers_dataset.csv'
''')
spark.sql('describe extended external_customers_sur').show(truncate=False)
spark.sql('select * from external_customers_sur limit 8').show()

>> create DF in spark
df_list= spark.createDataFrame(data,columns)
df_csv = spark.read.format('csv').option('header','true').load('/data/customers_100.csv')
df_sql = spark.sql('select * from external_customers_2 where is_active=True')
df_table = spark.table('external_customers_2')
df_range = spark.range(0,10)
df_rdd = rdd.toDF(["customer_id", "name"])

====>>  HiveQL: 
-- sql like interface (HQL) --> translator ---> MR/spark/tej code (is abstraction for java or MR programe)
-- table --> data (hdfs, s3, gcs) + metadata (hive stores metastore in it). 
-- hive is database like mysql?
its not a database. but, its DWH tool built on top of Hadoop to query large datasets from HDFS storage
	storage			query processing		schema 			transaction type	speed
mysql   stru data in tables	execute in-memory		schema-on-write		oltp			fast for small queries, real-time updates
Hive -   HDFS, s3		MR or Spark or Tej 		schema-on-read		olap			opti for large scale batch processing
-- hive is replacement for Hadoop?
No, buit on top of hadoop core components (hdfs, yarn, mr/spark/tej)
-- hive queries like normal sql queries in db?
HQL queries --> translatior --> mr/spark/tej code which runs on distributed system
-- hive can perform row-level transaction like mysql?
yes, it can. but, hive is not designed for row-level operations (if u change one row- it will recreate entire partition file)
-- hive can works with only HDFS?
no. HDFS/s3/gcs/data lake
-- hive tables work like noraml database tables?
hive is metastore and not storing any tables

>> connect with hive clients
1. hive: direct connection / no auth / single sessions/ read-heavy for query performance / production not rec /
or
beeline : uses jdbc for remote connection / support auth / multiple concurrant sessions/ opti for query performance / production rec /
	!connect jdbc:hive2://<hostname>:<port>/<database> / !connect jdbc:hive2://localhost:10000/default
	vi etc/hive/conf/hive-site.xml (for hive properties, we got username: hive and password: Pw1+86Xg4CCq from her as well)
	
>> (inside terminal)
	set hive.execution.engine; / set hive.execution.engine=tez/spark/mr;
	ctrl + l  --> to clear

>>  Hive external table (data file in GCS)
create external table h_suret2 (customer_id string, customer_unique_id string, customer_zip_code_prefix string, customer_city string, customer_state string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
location 'gs://surdatabuk/data/olist/cust_exe/'

>> Hive managed table (data in hdfs)
create table orders22 (	order_id string, ....) STORED AS TEXTFILE;
load data inpath '/data/olist/olist_customers_hive.csv' into table orders22;
set hive.metastore.warehouse.dir;  (hive data files stroed in hdfs) / hdfs dfs -ls /user/hive/warehouse/  

>> hive metastore (connect on vm node): mysql db (see conf xml file), default derby db, might be diff db configured in your companey
-- connect to mysql db 
	mysql -u hive -p 
	Pw1+86Xg4CCq
-- show databases; use metastore; show tables; select * from TBLS; exit

==========================================================================================================================================================================

Pyspark Interview QA:

>> Spark Architecture:

>> sPARK query plan:
	code syntax is correct or not --> unresolved logical plan --> catalog --> logical plan --> optimized logical plan (filter pushdown & selct req col)--> 
	converted several pysical plans --> cost model --> best pysical --> run on cluster.
	df1.explain(True)

>> Spark memory management
	submit spark app in yarn cluster --> yarn rm allocates app container and starts driver JVM 
	spark.driver.memory --> JVM memory 
	spark.driver.memoryOverhead --> 10% 0r 384mb max one --> used for container processes, pyspark app
	>> Spark executor container: sum of below 3 are the total memory of executor container
	1. on-heap memory managed by JVM (8 gb) [spark.executor.memory ] - when on-heap memory is full, operations paused and do gcc then resume operation - perfor reduce
		reserved memory 300 mb --> fixed reserve for spark engine
		spark memory (unified memory) [spark.memory.fraction] (0.6 default, u can change)--> used DF operation & caching
			storage memory pool [spark.memory.storageFraction], (0.5 default, u can change)
			executor memory pool --> 
		user memory --> used for spark internal metada, udf, rdd operations, variables, objects
	2. overhead [spark.executor.memoryOverhead  --> 10% of spark executor memory 0r 384mb max one], for container processes, n/w transfer, read shuffle, python worker 
	   spark.executor.pyspark.memory --> pyspark memory --> default zero (pyspark is non jvm. so it will take from overhead memory)
	3. off-heap mwmory: managed by OS [spark.memory.offHeap.enabled, spark.memory.offHeap.size - default 0, 10to20% of on-heap memory].
	   Off-heap memory can reduce garbage collection overhead, leading to better performance and lower latency

	yarn.scheduler.maximum-allocation-mb --> pysical memory limit at the worker node
	yarn.nodemanager.resource.memory-mb 

>> apache spark executor tuning: 
	-- thin executors (1 node 12-1 cpu, 48-1 gb memory --> 1 executor has 1 cpu & ~4gb memory) 55 executors
	-- fat executors (1 node 12-1 cpu, 48-1 gb memory --> 1 executor has 11 cpu & ~47gb memory) 5 executors
	-- optimal sized executors: 
		per node leave out 1 core 1 gb ram for hadoop/yarn/os
		app master container at cluster level - (1 executor or 1cpu1gbram)
	============================================================================================
	3node 16 core 48gb ram
	total cores = 16 -1 (os) * 3 = 45
	total memory = 48 - 1 (os) * 3 = 141
	executors = 45/4 ~ 11 		/ memory per executor = 141/11 ~ 12gb 		/cores per executor = 4
	overhead memory = max(384 or 1.2gb)
	actuval memory = 12 -1 = 11 gb 
	for app driver 1 executor / num of executors = 10, cores = 4, memory = 11gb
	===================================================================================================
	how many cpu cores are required 25GB,  (1-200mb rec, in hdfs-128mb block size), 25*1024*1024/128 = 200 cores
	how many executors requires (2-5 cores rec per executor) - 200/4 = 50 executor instances
	how much each executor memory req  - 1.5 * reserved memory(300mb) or min 4*(partition size) = 512 mb/core = 2 gb/executor
	total memory required to process file = 50*2 = 100 gb

>> 
shuffling: happenned when the wide transformation is performed / intend to bring all releted data together
Shuffle Partition: partitions after shufflling of data across the nodes based on key column
1000 core cluster vs 200 sp (default) -- under utilization / slow completion time
-- if data per sp is large
	ex: cores= 20, sp=200, data size=300gb, size of sp = 1.5 gb 
	rec: 1 to 200mb | sol is tune no of shuffle partitions | no of sp = 300*1000/200=1500sp
-- if data per sp is small
	ex: cores= 12, sp=200, data size=50mb, size of sp = 50/200=0.25mb 
	rec: 1 to 200mb | sol is tune no of shuffle partitions | no of sp = 50/choose 10mb=5sp or 50mb/12 cores ~ 4mb

>> 
partitioning & bucketing:
	Partitioning: Divides data based on column values, creating separate directories for each partition. Adv: esay/fast access, parallesium/resource utilization
	df.write.mode('overwrite').partitionBy('listen_date').parquet('/content/ecommer/') --> folder based on order_status & inside num of parti files	
	df.reparttion(3).write.mode('overwrite').partitionBy('listen_date').parquet('/content/ecommer/') 
	column should be medium cardinality & filter column
	suppose filter column has high cordinality - end up with small file problem. - buckting is rec

	Bucketing: Divides data into a fixed number of buckets based on the hash of a column  - filter, groupby, join queries.
	h(product_id)%no of buckets 
	during bucketing shuffeling involves.
	proucts, orders table - bucket 0 from both tables - placed in same executor 
	DS1		DS2		Performance
	B, X		B, X		no shuffle
	B, X 		B, Y		one of the dataset is shuffled
	B, X		B, X		bucketing x column, but join on Y column - involves full shuffle
 	df.write.mode('overwrite').bucketBy(2, 'order_status').saveAsTable('bucketed_table')   --->  6 partitions * 2 buckets = 12 files
	df_bucketed = spark.table('bucketed_table')

-- Q/A - how to decide optimal number of buckets = size of dataset / optimal bucket size (128-200mb)
   size of dataset in mb = (no of records * no of var/col * avg width of var in bytes) / (1024 * 1024)
-- once df is bucketed, no shuffle in groupby & join, scan one buckets when you filter instead of all

>> what is data skewness? (after data shuffling, most of data goes to few partitions as reference to the keys, which leads to data skewness)
	job is taking time, unutilization of resources, oom or data spill (write results to disk -- very costly)
	fix --> repartition, salting, AQE

>> salt: 
	decide salt number : [0, 3)
	ds1 - add salt column to key column 
	ds2 - add salt column (having array [0, 1, 2] - explode ) to key column

>> AQE (>spark 3.0): based on runtime statastics select best query plan - aqe gives: 
	tuning shuffle partitions- 200 shuffle parti default, when join it has 15 didtinct keys, aqe coalsese to 15 partitions
	optimizing joins - converts SMJ to BJ,   --> .config('spark.sql.autoBroadcastJoinThreshold', 24*1024*1024)
	optimizing skew joins: split skewed joins into smaller partitions --> .config('spark.sql.adaptive.skewJoin.enabled', 'true') 

	sort merge join --> shuffle, sort, merge --> hash(key1, key2..) - return int value % num of shuffle partitions
	BJ --> when one dataset is smaller
>> 
static partition pruning: partition on listen date data on disk - this scans only listen date that user eants
dynamic partition pruning: send partition that is supossed to be scanned
	release date table --> filter to get requ data which comes to know during run time --> spark uses results to scan on other dataset based on listen date
	Adv: reduce time to scan/process)
	limits: one of dataset must be partitioned and based on column that is filered from other datase	
	SS: .config("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true") \




>> create spark session with config optimization
spark = SparkSession.builder \
.appName("olist config optimization") \
.config('spark.executor.memory', '6g') \
.config('spark.executor.cores', '4') \
.config('spark.executor.instances', '2') \
.config('spark.driver.memory', '4g') \
.config('spark.driver.maxResultSize', '2g') \
.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores
.config('spark.default.parallelism', '64') \		# It usually defaults to the number of cores on all executor nodes.
.config('spark.sql.adaptive.enabled', 'true') \
.config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \
.config('spark.sql.autoBroadcastJoinThreshold', 24*1024*1024) \
.config('spark.sql.adaptive.skewJoin.enabled', 'true') \
.config('spark.sql.files.maxPartitionBytes', '64MB') \
.config('spark.sql.files.openCostInBytes', '2MB') \
.config('spark.memory.fraction', 0.8) \
.config('spark.memory.storageFraction', 0.2) \
.getOrCreate()

/etc/hadoop/conf/core-site.xml


>> steps to follow as data engineer:
1. Data Ingetion: load data into spark dataframe 
2   exploration: schema and datatypes, data leakage or drop, Null values, duplicate values, data distribution as per state, avg time taken to deliver item	
3. data cleaning: handle missing values, duplicate, invalid date (standadize format), data type corretion
4. data integration (combine data from multiple tables) and aggregation (to compute metrics): 
5. performance optimization: enhance data processing task - partitioning, cache/persist, spark config
6. data serving: make processed data available in bq



>> validate pyspark exit() / spark-shell (spark scala) :quit / spark-sql exit; / CLI in dataproc cluster.

>> execute gcs script 
   ---> run pyspark script on master node of VM 
	gcloud compute ssh --zone "us-central1-c" "cluster-1c1f-m" --project "elegant-circle-454709-h5"
	spark-submit gs://surdatabuk/scripts/pysp1.py
   ---> submit pyspark job using UI 
   ---> gcloud dataproc jobs submit pyspark gs://surdatabuk/scripts/pysp1.py --cluster=cluster-1c1f --region=us-central1 

>> 
pyspark script:
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder.appName('surapp').getOrCreate()
dfo=spark.read.csv('gs://surdatabuk/data/olist/olist_orders_dataset.csv', header=True, inferSchema=True)
dfc = spark.read.csv('gs://surdatabuk/data/olist/olist_customers_dataset.csv', header=True, inferSchema=True)
dfj = dfc.join(dfo, dfc['customer_id']==dfo['customer_id'], 'inner').drop(dfo['customer_id'])
dfjf = dfj.select('customer_id', 'customer_unique_id', col('customer_zip_code_prefix').cast('string'), 'customer_city', 'customer_state', 'order_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date')
dfjf.write.format('parquet').mode('overwrite').save('gs://surdatabuk/data/olist_op/')
dfjf.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket', 'gs://surdatabuk/data/olist_op/csv/').option('table', 'elegant-circle-454709-h5.surtest11.customers_orders').save()


==========================================================================================================================================================================



===========================================================================================================================================================================

>> How does Lazy Evaluation work in PySpark?
>> What are wide and narrow transformations in PySpark?
>> Explain shuffle operations in PySpark and their impact on performance.
	redistributing data across different nodes in a cluster, which is necessary for certain transformations like groupBy, join, and distinct.
>> What is broadcast join, and when should we use it?
	small dataset is broadcasted to all worker nodes in the cluster - allows to perform join locally on each node - reducing the amount of data shuffled across the 	network - improved performance
>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
>> What is the difference between map() and flatMap() inPySpark?
    returns a single value for each input element / return multiple values for each input element
>> How does .𝐜𝐨𝐥𝐥𝐞𝐜𝐭() work, and when should it be avoided (memory constraints, performance issues) ?
	gathers all the partitions of the df or RDD and brings them to the driver node. - data is no longer distributed, instead stored in the memory of the driver program
>> Difference between sortby and orderby? -    sortBy is used with RDDs and orderBy is used with DataFrames.
>> what is the default block size (128 MB) in hdfs and how to change it?:   Edit the hdfs-site.xml File:
>> What is difference between spark session and spark context?
	SparkContext: entry point for RDD operations/functionality. (used <2.0 spark version)
	SparkSession: Provides a unified entry point for DataFrame and Dataset APIs, SQL queries, and more. (used >2.0 spark version)
>> what are the optimization techniques in pyspark?
	use col format files, Use DataFrame/Dataset over RDD, filter data erly, Cache/persist data, use broadcast Joins
	Optimize shuffle Partitions (Repartition / Coalesce enable AQE, salting)
	partitioning & bucketing for frequently accessed based on specific column, Avoid User Defined Functions (UDFs)

======================================================================================================================================================================
>> Find the top 3 highest-paid employees from each department.
   data = [(1, "Amit", "IT", 90000),

>> Write a PySpark code to remove duplicate records based on a specific column.
   data = [(101, "Mumbai", "Maharashtra"), ]

>> Write a PySpark query to calculate the moving average of sales over the last 3 months:   order_id, date, sale_amount
    window_spec = Window.orderBy("Month").rowsBetween(-2, 0)
    df_with_moving_avg = df.withColumn("Moving_Avg", avg(col("Sales")).over(window_spec))

>> What is SparkSQL, and how do you perform SQL operations on DataFrames?
	SparkSQL is a module in Apache Spark that allows you to run SQL queries on large datasets.

>> pivot & unpivot data: cricket_data = [("Virat Kohli", 'Match1', 75), ..........], col = ["Player", "Match", "score"]
	pivot_df = df.groupBy('Player').pivot('Match').agg(sum('score'))
	unpivoted_df = df.selectExpr("Name", "stack(3, 'Match1', Match1, 'Match2', Match2, 'Match3', Match3) as (Product, Sales)")

>> what is collect list and collect set and when do we use it?
   result = df.groupBy("name").agg(collect_list("value").alias("values_list")) - allows duplicates in list
   result = df.groupBy("name").agg(collect_set("value").alias("values_set")) - no duplicates in list

>> Given a dataset of Indian cities with their respective populations, write a PySpark code snippet to find the top 5 most populous cities.
     | City    | Population|
    top_5_cities = df.orderBy(col("Population").desc()).limit(5)

>> write a PySpark code snippet to group employees by their department and calculate the average salary for each department.  emp_name, dept, salary
    avg_salary_by_dept = df.groupBy("Department").agg(avg("Salary").alias("AverageSalary"))

>> Write a PySpark code snippet to remove duplicate records from a DataFrame based on a composite key consisting of 'customer_id' and 'transaction_date'.
     | customer_id| transaction_id | transaction_date |

>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")]
    output 
   +---+---------------+----------+---------+
   | ID| Name|First_Name|Last_Name|
   +---+---------------+----------+---------+
   | 1|Sagar-Prajapati| Sagar|Prajapati|
   | 3| John Cena| John| Cena|
   +---+---------------+----------+---------+
	df = df.withColumn("fn", split(df.Name, "[- ]").getItem(0)).withColumn("ln", split(df.Name, "[- ]")[1])

>> Given a DataFrame, split the data into two columns (Even, Odd) where:
    Ans. df2 = df1.withColumn('even', when(col('id') % 2 == 0, col('id'))).withColumn('odd', when(col('id')%2!=0, col('id')))

>> Create a DataFrame with two columns:
    Column 1: Default String
    Column 2: Default Integer

>> Word count program in pyspark rdd & DF
    df1 = df.select(explode(split(df["name"], " ")).alias("word")).groupBy("word").agg(count("*").alias("count"))

>> Write a PySpark query to count the number of null values in each column of a DataFrame.
	df1.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()

>> Write a PySpark query to replace null values in a specific column with the previous non-null value.
	window_spec = Window.orderBy("id").rowsBetween(Window.unboundedPreceding, Window.currentRow)
	df.withColumn("filled_col", last('value', ignorenulls=True).over(window_spec)).show()

>> reading file into df / write df into target
option("recursiveFileLookup", "true")
option("compression", "gzip")
option("modifiedBefore", "2025-04-19T20:58:57") / option("modifiedAfter", "2025-04-19T20:58:57")
option('header', True)
option("mode", "PERMISSIVE").schema(cust_schema).option("columnNameOfCorruptRecord", "_corrupt_record")  - return null / 
option("mode", DROPMALFORMED/FAILFAST").schema(cust_schema) - remove wrong records / throw error if any incorrect records are there

df = spark.read.csv("/tmp/compressed_csv_data", sep="|", linesep="/r or /n")
df.write.mode("overwrite").parquet("/tmp/compressed_data", sep="|", linesep="/r or /n")
df1.write.mode("overwrite").format("bigquery").option("temporaryGcsBucket", "surdatabuk").option("table", "elegant-circle-454709-h5.surtest11.surtt3").save()

>>
Given two datasets, products and sales, write a PySpark program to identify products that have never been sold. Assume the schema of products is (product_id, product_name) and the schema of sales is (sale_id, product_id, sale_date).
products = spark.createDataFrame([ (1, "Laptop"), (2, "Tablet"), (3, "Smartphone"), (4, "Monitor"), (5, "Keyboard") ], ["product_id", "product_name"]) 
sales = spark.createDataFrame([ (101, 1, "2025-01-01"), (102, 3, "2025-01-02"), (103, 5, "2025-01-03") ], ["sale_id", "product_id", "sale_date"]) 

	dfj = products.join(sales, on='product_id', how='left')
	dfj.filter(dfj.sale_id.isNull()).select('product_id', 'product_name').show()

>>
You are given a dataset of employees in an Indian company with columns: `emp_id`, `name`, `department`, `salary`, and `city`. Write a PySpark program to find the total salary paid in each department.
Sample Data:
emp_id, name, department, salary, city
101, Rajesh, IT, 75000, Bangalore
Expected Output:
IT, 233000

>> Write a pyspark program to count number of products in each category 
# based on its price into three categories below. 
# Display the output in descending order of no of products.
# 1- "Low Price" for products with a price less than 100
# 2- "Medium Price" for products with a price between 100 and 500 (inclusive)
# 3- "High Price" for products with a price greater than 500.

products_data = [
 (1, 'Laptop', 800),
 (2, 'Smartphone', 600)]


>> Handling Null Values
You are given a dataset containing customer transactions in an Indian e-commerce platform with columns: `cust_id`, `cust_name`, `city`, `purchase_amount`, and `product_category`. Some records have missing `purchase_amount`. Write a PySpark program to fill missing `purchase_amount` values with the average purchase amount of that product category.

Sample Data:
cust_id, cust_name, city, purchase_amount, product_category
201, Aman, Delhi, 1500, Electronics
202, Kiran, Mumbai, , Fashion

	df_avg_pa = df.groupBy('product_category').agg(avg('purchase_amount').alias('avg_purchase_amount'))
	df_join = df.join(df_avg_pa, on='product_category', how='left')
	df_join.withColumn('purchase_amount', when(col('purchase_amount').isNull(), col('avg_purchase_amount')).otherwise(col('purchase_amount')))
	df_final = df_join.drop('purchase_amount').show()

>> 
You have a dataset of students from different Indian states with columns: `student_id`, `student_name`, `state`, `score`. 
Write a PySpark program to rank students within each state based on their scores in descending order.
student_id, student_name, state, score
301, Rohit, Maharashtra, 85
Expected Output:
Maharashtra, Amit, 90, Rank 1
	window_spec = Window.partitionBy("state").orderBy(desc("score"))
	ranked_df = df.withColumn("rank", rank().over(window_spec))

>> Fill the mean salary value where salary is Null.
data = [("Alice", 50000), ("Bob", None), ("Charlie", 60000), ("David", None)]
df = spark.createDataFrame(data, ["name", "salary"])

	avg_salary = df.select(mean(col('salary'))).collect()[0][0]
	df.fillna({'salary': avg_salary}).show()

>> What are the different ways to remove duplicate records in PySpark?
	1. distinct_df = df.distinct()
	2. drop_duplicates_df = df.dropDuplicates(["name", 'id'])
	3. window_spec = Window.partitionBy("name").orderBy("id")
	   df_with_row_num = df.withColumn("row_num", row_number().over(window_spec))
	   filtered_df = df_with_row_num.filter(col("row_num") == 1).drop("row_num")
	4. grouped_df = df.groupBy("name").agg(max("id").alias("max_id"))

>> Explain `groupBy()`, `agg()`, and `pivot()` functions with an example.
	pivot_df = df.groupBy("name").pivot("department").sum("salary")

>>
𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧
You need to remove the duplicate rows based on the composite key (customer_id, order_id) and retain only the row with the latest order_date for each combination of customer_id and order_id.
𝐬𝐜𝐡𝐞𝐦𝐚 
data = [ (101, 1001, "2025-01-15", 500.00), (102, 1002, "2025-01-14", 300.00), (101, 1001, "2025-01-17", 550.00), (103, 1003, "2025-01-16", 450.00), 
(102, 1002, "2025-01-18", 320.00), (103, 1003, "2025-01-19", 460.00) ] 
schema = ["customer_id", "order_id", "order_date", "total_amount"] 

	window_spec = Window.partitionBy("customer_id", "order_id").orderBy(desc("order_date"))
	df = df.withColumn("row_number", row_number().over(window_spec))
	df.filter(col("row_number") == 1).drop("row_number").orderBy("customer_id").show()

>>
Hide Credit card number: Accept 16 digit credit card number from user and display only last 4 characters of card number.
data = [(1,'Rahul',1234567891234567),(2,'Raj',1234567892345678),(3,'Priya',1234567893456789),(3,'Murti',1234567890123456)]
schema = "id int, name string, card_no long"

	from pyspark.sql.functions import udf, StringType
	
	def maskfun(num):
	  return '************' + str(num)[-4:]
	
	maskfun_udf = udf(maskfun, StringType())
	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))

>> 𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧
You are working as a Data Engineer for a company. The sales team has provided you with a dataset containing sales information. However, the data has some missing values that need to be addressed before processing. You are required to perform the following tasks:
1. Load the following sample dataset into a PySpark DataFrame:
2. Perform the following operations:
a. Replace all NULL values in the Quantity column with 0.
b. Replace all NULL values in the Price column with the average price of the existing data.
c. Drop rows where the Product column is NULL.
d. Fill missing Sales_Date with a default value of '2025-01-01'.
e. Drop rows where all columns are NULL.
𝐬𝐜𝐡𝐞𝐦𝐚 
data = [ (1, "Laptop", 10, 50000, "North", "2025-01-01"), (2, "Mobile", None, 15000, "South", None), (3, "Tablet", 20, None, "West", "2025-01-03"), (4, "Desktop", 15, 30000, None, "2025-01-04"), (5, None, None, None, "East", "2025-01-05") ] columns = ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]

	from pyspark.sql.functions import *
	avg_value = df.select(avg('Price')).collect()[0][0]
	df = df.na.fill({'Quantity':0, 'Price':avg_value, 'Sales_Date':'2025-01-01'})
	df = df.na.drop(subset=['Product'])
	df = df.na.drop(how='all')
	df.show()

>>
𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧:
You have a PySpark DataFrame df containing detailed sales data from various states in India for an e-commerce company. The DataFrame schema is:

You need to perform the following complex analysis using PySpark:
1. Calculate the total revenue per state and category.
 (Revenue = units × unit_price)
2. For each state, identify the category that has the highest total revenue.
3. For each state-category pair, calculate the average units sold per order.
4. From the results, list the states along with the category that generated the highest revenue, and the average units sold per order for that category.
5. Sort the final output by total revenue in descending order.
 
𝐒𝐚𝐦𝐩𝐥𝐞 𝐃𝐚𝐭𝐚 :
data = [
 ("O1001", "Maharashtra", "Mumbai", "P100", "Electronics", 3, 15000.0, "2025-05-10"),
 ("O1002", "Maharashtra", "Pune", "P101", "Clothing", 5, 1200.0, "2025-05-11"),
 ("O1003", "Karnataka", "Bengaluru", "P102", "Electronics", 2, 18000.0, "2025-05-10"),
 ("O1004", "Karnataka", "Mysore", "P103", "Grocery", 10, 200.0, "2025-05-12"),
 ("O1005", "Tamil Nadu", "Chennai", "P104", "Electronics", 1, 16000.0, "2025-05-11"),
 ("O1006", "Tamil Nadu", "Coimbatore", "P105", "Clothing", 8, 1100.0, "2025-05-13"),
 ("O1007", "Maharashtra", "Nagpur", "P106", "Grocery", 20, 150.0, "2025-05-12"),
 ("O1008", "Karnataka", "Bengaluru", "P107", "Clothing", 6, 1300.0, "2025-05-14"),
 ("O1009", "Tamil Nadu", "Chennai", "P108", "Grocery", 15, 180.0, "2025-05-14"),
]

columns = ["order_id", "state", "city", "product_id", "category", "units", "unit_price", "order_date"]

𝐄𝐱𝐩𝐞𝐜𝐭𝐞𝐝 𝐎𝐮𝐭𝐩𝐮𝐭:
+-----------+-----------+-------------+-------------------+
|state |category |total_revenue|avg_units_per_order |
+-----------+-----------+-------------+-------------------+
|Maharashtra|Electronics|45000.0 |3.0 |
|Karnataka |Electronics|36000.0 |2.0 |
|Tamil Nadu |Clothing |8800.0 |8.0 |
+-----------+-----------+-------------+-------------------+



