>> Apache Spark - multi-language engine (single-node machines or clusters) suitable for large-scale data workloads, and can be --         deployed on-premises or in the cloud.
   Google Cloud Dataproc is a managed service that runs Apache Spark (and other frameworks),

>> Haddop Yarn Cluster -- master node (Yarn RM), worker node
   worker node --> App Master container (pyspark driver --> py4j --> JVM driver), executors (each 4cpu 16 gb ram - means 4 slots and memory common for 4 slots)
   Script or spark app --> divide into jobs based on action --> job divide into stages based wide trans --> shuffle/sort --> tasks (these tasks run slots)

>> enable API, create dataproc cluster on vm/gke.
   -- cluster type: single node 1m0w, standard for dev team 1m Nw, high availability for prod 3m Nw
   -- allocate static ip to it >> vm instance - external ip - view detais - n/w page ip addresses - reseve to static 

>> validate ssh connectivity to master node vm of dataproc cluster: gcloud ssh cammand helps to copy key on vm and able to ssh vm (gcloud compute ssh --zone "us-central1-a"    "dataprocclu1-m" --project "possible-dream-433814-b4")
   -- establish an SSH connection to a remote server VM instance: ssh -i C:\Users\surmacha\.ssh\google_compute_engine surmacha@35.184.234.232
   -- setup vs code remote window for dataproc vm of master node - create below config file in .ssh - select remote window button on vs code - host in search 
   Host Sur-DP-Cluster-VSC
    HostName 35.238.68.160
    IdentityFile C:\Users\surmacha\.ssh\google_compute_engine
    User surmacha
   -- copy material to the VM master node using git clone command.

>> copy local files in master node of vm to hdfs in dataproc/ gcs blobs into hdfs on dataproc.
   hdfs dfs -ls /  ----    hdfs dfs -ls -R /public/retail_db
   hdfs dfs -mkdir /public
   hdfs dfs -put data/retail_db /public (local to hdfs -put)
   hdfs dfs -cp gs://surretail/retail_db /public/retail_db (hdfs to hdfs, gcs to hdfs, hdfs to gcs, gcs to gcs -cp)
   hdfs dfs -rm -R -skipTrash /public/retail_db

6. validate pyspark / spark-shell (spark scala) / spark-sql CLI in dataproc cluster.
7. submit dataproc job using spark sql in UI
8. create dataproc workflow using jobs
   cleanup --> convert json file to parquet for orders/order_items --> compute daily product revenue and save back to gcs.
   >> review the scripts.
   >> apply unit tests/validation in local vs code environment configured with dataproc cluster master node.
   >> use scripts in gcs rather then hdfs (temproray based on cluster)
   >> copy spark-sql scripts to gcs location.
   >> run and validate spark-sql scripts placed in gcs using vs code configured with dataproc master node
   >> limitations of running spark sql scripts using dataproc jobs
         submtting jobs by defining query file in gcs via ui is not working (gcloud cli is working).
         submit jobs using gcloud from external cli but not from vs code dataproc master node
   >> submit spark sql jobs: list spark-sql scripts in gcs and review content then submit job using dataproc gcloud command.
   >> dataproc workflow templates: orchestrated pipeline within gcp dataproc
         UI- create template using jobs defined by query text (query file not working in job) - not use in practical
         gcloud commands (more flexibility than ui):
               1. create dataproc workflow template (gcloud dataproc workflow-templates create ----)
               2. attach workflow template with existing cluster (gcloud dataproc workflow-templates add-cluster-selector---)
               3. add jobs to workflow template (gcloud dataproc workflow-templates add-job ----)
               4. instantiate workflow template (gcloud dataproc workflow-templates instantiate ----)
   >> stop / delete cluster using ui, gcloud
----------------
>> place script in gcs location: gs://surretail/scripts/daily_product_revenue/gcsToBq.py (or) gcsToBq.sql (or) gcsToBq.scala
>> execute gcs script 
   ---> run pyspark script on master node of VM (ex: spark-submit -f gs://surretail/scripts/daily_product_revenue/gcsToBq.py)
   ---> submit pyspark job using UI 
   ---> submit spark-sql job using gcloud (gcloud dataproc jobs submit spark-sql --cluster=cluster-a695 -f          
        gs://surretail/scripts/daily_product_revenue/compute_daily_product_revenue.sql --params=bucket_name=surretail)
-------------------------------
>> Execution Flow in PySpark
   Start SparkSession → spark = SparkSession.builder.appName("App").getOrCreate()
   Load Data (from a list, CSV, Parquet, etc.)
   Apply Transformations (e.g., filter(), select(), groupBy())
   Trigger an Action (e.g., show(), collect(), count())
   Stop SparkSession → spark.stop()
------------------------------
