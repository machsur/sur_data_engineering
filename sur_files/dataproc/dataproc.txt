========================================================================================================================================================================
links:
https://github.com/afaqueahmad7117/spark-experiments/tree/main/spark
https://allthingdata.substack.com/p/essential-linux-commands-for-data         

========================================================================================================================================================================
===========================================> hdfs commands:

>> linux system / hdfs system
hdfs -help
hdfs dfs -help
hdfs dfs -help ls

==========================================> low-level api


>> rdd = spark.sparkContext.parallelize(data)  /  rdd = spark.sparkContext.textFile(hdfs_gcs_path) 
>> rdd1 = rdd.filter(lambda row: 'Frieza' in row) / rdd1 = rdd.filter(lambda row : row!=header)  - header = rdd.first()
   rdd1 = rdd.flatMap(lambda line:line.split(' ')).map(lambda word:(word,1)).reduceByKey(lambda a,b : a+b)   ---> word count programme
   rdd1 = rdd.map(lambda row:row.split(',')).map(lambda row:(row[2],1)).groupByKey().map(lambda row:(row[0],len(row[1]))).collect()
   rdd1 = rdd.map(parse_row) / def parse_row(row):
>> rdd.first() / rdd.collect() / rdd.take(3) / rdd1.countByValue()
   
=============================>> high-level api

>> custom_schema = StructType([StructField("id", IntegerType(), True), ....]) or ''' ID Integer, Name String '''
   df = spark.read.option("header", True).option("inferSchema", True)/schema(cust_schema).option("mode", "PERMISSIVE").json("file:///path_to_file/data_files/")
   df.write.format('parquet').mode('overwrite').save('gs://surdatabuk/data/olist_op/')
   df.write.format('bigquery').mode('overwrite').option('temporaryGcsBucket', 'gcs_path').option('table', 'p.d.t').save()
>> join_df = df1.join(df2, df1.id == df2.id/on='cn', how='inner/left/right/outer/left_semi/left_anti') / df1.crossjoin(df2)  / df1.join(broadcast(df2), "id", "inner")
>> filtered_df = df.filter(df['cn'] > 10) / .isNull() / .isNotNull() /  isIn(['India', 'USA'])  /  &|   /
>> df.select("customer_id", col("Name"), df.email, df["city"])    /    df.selectExpr("id*2 as nid", "name as nname").show()
>> sorted_df = df.orderBy(asc('department')) / col("country).desc())  / asc_nulls_last('value') /
>> df.show(n=3, truncate=25, vertical=True) / df.collect() / df.count() / df.printSchema() / df.columns / df.limit(10)
>> union tables: union/subtract/intersect/   -->> / col name, position, schema match /
   union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col name / position, schema can differ / missing col handle with null values
>> add new column, drop columns, column name change, column type change 
>> terms - initcap / lower / trim / lpad(col("Name"), 10, "X") / coalesce('cn1', 'cn2','cn3')
>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName)) / concat_ws(' | ', col("Region"), col("Country"))
>> df.dropna(how="all/any", subset=["age"]).show()
   df.fillna({'name': 'suresh', 'age': 40}).show()   /   avg_value = df.select(avg("age")).collect()[0][0]
   df.replace(to_replace=['Alice', 'Bob'], value=['suresh', 'unknown']).show()
>> when&otherwise -->> df = df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult"))
>> agg 	 	   ----> df.select(sum('CN')).show()  		   ------> note: ignore null values ----> sumDistinct, countDistanct
   grouping and agg----> df.groupBy('CN').agg(sum('CN')).show()   	
   window functions----> win_spe=Window.partitionBy("category").orderBy(asc("timestamp")).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing) / (-2, 3)
				df = df.withColumn("row_number", row_number().over(window_spec)) 
   	max(cn) / min(cn) / count(cn) / sum(cn) / avg(cn) / collect_list('cn') / collect_set('cn') / first_value('cn') / last_value(cn) / nth_value(cn, n)  /
	/ row_number() / rank() / dense_rank() / lag(cn) / lead(cn, 2) / ntile(n) / cume_dist() / percent_rank()
>> splitng column, array_length (size), word in array (array_contains), explode/explode_outer, indexing: [0] or .getItem(0)
>> Pivot: df.groupBy('Month').pivot('product').sum('sales').show()
   unpivot: unpivot_exp = "stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (Product, Sales)"  ------> df1.selectExpr("Month", unpivot_exp).show()
>> Dates

================================================> spark-sql 

>> spark temp table (exists only for SS) / global temp table (exists all SS within same app)
   spark.sql('show databases') --> spark.sql('use db_name') --> spark.sql('show tables') --> spark.sql('show tables in global_temp')
   df.createOrReplaceTempView('customers') -->	df.createOrReplaceGlobalTempView('gcustomers')
   spark.sql('select * from customers limit 5') or spark.table('customers') --> spark.sql('describe extended customers') --> spark.sql('drop table customers') 		
>> Spark Persistant: Managed table (spark owns both metadata and data)
   spark.sql('''CREATE TABLE custm (customer_id STRING, customer_unique_id STRING, customer_zip_code_prefix INT, customer_city STRING, customer_state STRING) USING CSV''')
   df.write.mode('overwrite').saveAsTable('default.customers_sur')
   data: hdfs://sur-dp-clu11-m/user/hive/warehouse/customers 
   metadata: vi etc/hive/conf/hive-site.xml -> connect mysql -u hive -p  -> Pw1+fgC9gJI+ -> show databases; use metastore; show tables; select * from TBLS; exit
>> Spark Persistant: external table (spark owns metadata and gcs/hdfs owns data)
   spark.sql(''' create external table custe (customer_id STRING, ...) using csv location 'gs://surdatabuk/data/olist/olist_customers_dataset.csv' ''')

=========================================>>  HiveQL: 

>> sql like interface (HQL) --> translator ---> MR/spark/tej code (is abstraction for java or MR programe) >> cli: set hive.execution.engine=tez/spark/mr;
   table --> data (hdfs, s3, gcs) + metadata (hive stores metastore in it). 
>>  Hive external table (data file in GCS)
   create external table custe (customer_id string, ...) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location 'gs://surdatabuk/data/olist/cust_exe/'
>> Hive managed table (data in hdfs --> hdfs dfs -ls /user/hive/warehouse/  --> set hive.metastore.warehouse.dir; )
   create table custm (	order_id string, ....) STORED AS TEXTFILE;
>> connect: 1.hive & 2.beeline : prod rec --> !connect jdbc:hive2://<hostname>:<port>/<database> / !connect jdbc:hive2://localhost:10000/default (hive & Pw1+fgC9gJI+)

-- hive is database like mysql?		its not a database. but, its DWH tool built on top of Hadoop to query large datasets from HDFS storage
-- hive is replacement for Hadoop? 	No, buit on top of hadoop core components (hdfs, yarn, mr/spark/tej)
-- hive queries like normal sql queries in db?  	HQL queries --> translatior --> mr/spark/tej code which runs on distributed system
-- hive can perform row-level transaction like mysql? 	yes, but, hive is not designed for it (if u change one row- it will recreate entire partition file)
-- hive can works with only HDFS?  			no. HDFS/s3/gcs/data lake
-- hive tables work like noraml database tables? 	hive is metastore and not storing any tables

==========================================================================================================================================================================

>> Spark performance tuning

1. master in reading query plan
	syntax check ---> unresolved logical plan --> catalyst optimizer --> optimal logical plan (filter push down & req col) --> several pysical plan -->
	based on cost model select best pysical plan  --> executes and create RDD/DF
	df.explain(True)

2. master in reading spark dags

3. Tune no of shuffle partitions (optimal SP size b/w 1 to 200 mb )
	* if data per SP is large - cores = 20, SP = 200, data = 300 gb - data/sp = 1.5gb - no of SP = 300*1000/200 = 1500 SP
	* if data per SP is large - cores = 12, SP = 200, data = 50 mb - data/sp = 0.25 mb - no of SP=50/5=10 or data/SP = 50/12 = 4.5 mb

4. Data Skew (job taking time, uneven resource utilization, OOM/data spill)
	AQE(>3.0) - uses runtime statistics - choose most efficient query plan
		-> tuning no of SP - 15 keys - coalesce(15) - spark.sql.adaptive.enabled, spark.sql.adaptive.coalescePartitions.enabled
		-> optimize joins - SMJ to BJ - df1.repartiton(4), df2 broadcasted on all executors - spark.sql.autoBroadcastJoinThreshold
		-> optimize Skew joins - breaks larger part into smaller part - spark.sql.adaptive.skewJoin.enabled
	Salt - adding randomness (key) to distribute uneven data evenly.
		-> add salt columnn - hash(key, salt)% SP

5. Partitioning: create folders for each country within files are equal to no of partitions - Filtering	(Adv: fast access, parallesium/resource utilization)
   df.write.mode('overwrite').partitionBy('country').parquet('/content/ecommer/') >> high cordinality - small file problem - buckting is rec
   Bucketing: Joins and aggregations, filter - hash(product_id)%4  
   df.write.mode('overwrite').bucketBy(2, 'product_id').sortBy("product_id").format("parquet").saveAsTable('bucketed_table')  - 2buc*2part=4files
   Q/A - optimal number of buckets = size of dataset / optimal bucket size (128-200mb)  
   Q/A - once df is bucketed, no shuffle in groupby & join, scan one buckets when you filter instead of all 

6. executor tuning: 
   Rules:leave 1C1gbM per Node for hadoop/yarn/os - leave 1Exe or 1C1gbM for app master container at cluster level - 3 t0 5 cores/exe - overhead mem max(384 or 10% exe mem)
   12c48gb/node - 11c47gb/node-55c235gb-54c234gb-total exe=54/5=10 & mem/exe=234/10=23.4-actual memory/exe=23-2.3=20gb

7.Spark memory management
	submit spark app in yarn cluster --> yarn rm allocates app container and starts driver JVM 
	spark.driver.memory --> JVM memory 
	spark.driver.memoryOverhead --> 10% 0r 384mb max one --> used for container processes, pyspark app
	>> Spark executor container: sum of below 3 are the total memory of executor container
	1. on-heap memory managed by JVM (8 gb) [spark.executor.memory ] - when on-heap memory is full, operations paused and do gcc then resume operation - perfor reduce
		reserved memory 300 mb --> fixed reserve for spark engine
		spark memory (unified memory) [spark.memory.fraction] (0.6 default, u can change)--> used DF operation & caching
			storage memory pool [spark.memory.storageFraction], (0.5 default, u can change)
			executor memory pool --> 
		user memory --> used for spark internal metada, udf, rdd operations, variables, objects
	2. overhead [spark.executor.memoryOverhead  --> 10% of spark executor memory 0r 384mb max one], for container processes, n/w transfer, read shuffle, python worker 
	   spark.executor.pyspark.memory --> pyspark memory --> default zero (pyspark is non jvm. so it will take from overhead memory)
	3. off-heap mwmory: managed by OS [spark.memory.offHeap.enabled, spark.memory.offHeap.size - default 0, 10to20% of on-heap memory].
	   Off-heap memory can reduce garbage collection overhead, leading to better performance and lower latency

	yarn.scheduler.maximum-allocation-mb --> pysical memory limit at the worker node
	yarn.nodemanager.resource.memory-mb 

8. static partition pruning: partition on listen date data on disk - this scans only listen date that user eants
   dynamic partition pruning: send partition that is supossed to be scanned
	release date table --> filter to get requ data which comes to know during run time --> spark uses results to scan on other dataset based on listen date
	Adv: reduce time to scan/process)
	limits: one of dataset must be partitioned and based on column that is filered from other datase	
	SS: .config("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true") \

9. cache/persist, Use DataFrame/Dataset over RDD, Avoid User Defined Functions (UDFs)

=====================================================================================================================================================

>> 
spark = SparkSession.builder.appName('surapp11').config('spark.sql.files.maxPartitionBytes', '3mb').getOrCreate()
spark.sparkContext.applicationId, spark.stop(), spark application -list, spark application -kill app_id
df.explain(True)
spark_new_app_session = spark.newSession()
df.rdd.getNumPartitions()
spark.conf.get('spark.sql.files.maxPartitionBytes') / rdd.getNumPartitions() / spark.sparkContext.defaultMinPartitions 

gcloud: --properties=spark:spark.executor.cores=4,spark:spark.executor.memory=4g,spark:spark.executor.instances=2
.config('spark.driver.memory', '4g')  \ .config('spark.driver.maxResultSize', '2g') \
.config('spark.sql.shuffle.partitions', '64') \   	# default 200 - usevally set 2 to 3 times number of cores
.config('spark.memory.fraction', 0.8) \ .config('spark.memory.storageFraction', 0.2) \
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10 * 1024 * 1024)  # 10 MB

>> dataproc cluster: stagging bucket --> cluster metainfo, notebooks	temp bucket --> MR/spark job history, YARN logs
>> validate pyspark exit() / spark-sql exit; / 
>> execute gcs script: master node: spark-submit gs://buk/pysp1.py   >> submit pyspark job using UI >> gcloud dataproc jobs submit pyspark pysp1.py --cluster --region

==========================================================================================================================================================================

>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
   what is transformation and action?      
	How does Lazy Evaluation work in PySpark?      
	What are wide and narrow transformations in PySpark?
   	reduceByKey vs groupByKey?		
	increase and decrease no of partions?		
	spark read data and partitioning?
	spark jobs (no of actions), stages (no of wide trans + 1), tasks (no of partitions used) 
>> key-benfits compared to low-level api  
	reading data in spark is action or transformation?
	read modes: option("mode", "PERMISSIVE").option("columnNameOfCorruptRecord", "_corrupt_record") // option("mode", DROPMALFORMED/FAILFAST")
>> df.cache()  # Stores in memory, spills to disk if needed  -->> df.persist(StorageLevel.MEMORY_AND_DISK_SER_3)   (6)
>> shuffle operations - redistributing data across different nodes in a cluster, which is necessary for certain transformations like groupBy, join, and distinct.
>> broadcast join - small dataset is broadcasted to all worker nodes - allows to perform join locally - reduced data shuffling across the network - improved performance
>> map() returns a single value for each input element / flatMap() return multiple values for each input element
>> .ðœð¨ð¥ð¥ðžðœð­() work?  --> gathers all partitions of RDD and brings them to the driver node. - data is no longer distributed, instead stored in the memory of the driver program
>> Difference between sortby and orderby? - sortBy is used with RDDs and orderBy is used with DataFrames.
>> what is the default block size (128 MB) in hdfs and how to change it?:   Edit the hdfs-site.xml File
>> SparkContext: entry point for RDD operations/functionality. SparkSession: Provides a unified entry point for DataFrame and Dataset APIs, SQL queries, and more. 
>> What is SparkSQL, and how do you perform SQL operations on DataFrames? --> is a module in Apache Spark that allows you to run SQL queries on large datasets.

======================================================================================================================================================================

>> Find the top 3 highest-paid employees from each department.   data = [(1, "Amit", "IT", 90000),
>> Write a PySpark query to calculate the moving average of sales over the last 3 months:   order_id, date, sale_amount
>> rank students within each state based on their scores in descending order.   student_id, student_name, state, score
>> What are the different ways to remove duplicate records in PySpark? ---> distinct() / dropDuplicates(["name", 'id']) / row_number() 
>> Calculate the percentage change in salary compared to the previous year for each employee.
>> Calculate the number of days between consecutive orders placed by each customer. 
>> Find the top 2 highest-paid employees in each department using window functions.
>> Calculate the cumulative sum of sales for each product ordered by date.
>> find the top 5 most populous cities.    | City    | Population|
>> calculate the average salary for each department.  emp_name, dept, salary
>> remove duplicate records based on a composite key consisting of 'customer_id' and 'transaction_date'. | customer_id| transaction_id | transaction_date |
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")] 
>> split the data into two columns (Even, Odd) 
>> Create a DataFrame with two columns: Column 1: Default String Column 2: Default Integer

>> Retrieve employees who joined in the last 6 months.
>> Find and display duplicate records based on specific columns
>> Identify employees whose salary is greater than the average salary of their department.
>> Get the first and last transaction date for each customer.
>> Replace null values in a column with the last non-null value in that partition.
>> Categorize employees based on years of experience into Junior, Mid, and Senior levels.

>> query to replace null values in a specific column with the previous non-null value. -----> last('value', ignorenulls=True).over(window_spec)
>> Fill the mean salary value in Null. ---> [("Alice", 50000), ..] --> avg_sal = df.select(mean(col('sal'))).collect()[0][0] --> df.fillna({'salary': avg_salary}).show()
>> Explain `groupBy()`, `agg()`, and `pivot()` functions with an example.
>> remove duplicate rows based on the composite key (cust_id, order_id) & retain row with latest order_date for each combination: cust_id, order_id, order_date, amount
>> identify products that have never been sold. product_id, product_name / sale_id, product_id, sale_date. --> left join / filter null
>> find the total salary paid in each department.  --->  emp_id`, `name`, `department`, `salary`, and `city`.  
>>  (1, 'Laptop', 800)  ---->  count number of products in each category / based on its price into three categories below. 
>> fill missing `purchase_amount` values with the average purchase amount of that product category. -->`cust_id`, `cust_name`, `city`, `pur_amount`, `product_category`. 
>> ["Sales_ID", "Product", "Quantity", "Price", "Region", "Sales_Date"]
Replace all NULL values in the Quantity column with 0 / Price column with the average price of the existing data / Fill missing Sales_Date with '2025-01-01'.
Drop rows where the Product column is NULL / Drop rows where all columns are NULL.
>> ["order_id", "state", "city", "product_id", "category", "units", "unit_price", "order_date"]
	1. Calculate the total revenue per state and category. (Revenue = units Ã— unit_price)
	2. For each state, identify the category that has the highest total revenue.
	3. For each state-category pair, calculate the average units sold per order.
	4. From the results, list the states along with the category that generated the highest revenue, and the average units sold per order for that category.
	5. Sort the final output by total revenue in descending order.
>> Word count program in pyspark rdd & DF
>> count the number of null values in each column of df. 	df.select([count(when(col(c).isNull(), lit(1))) for c in df1.columns]).show()
>> display only last 4 characters of card number. data = [(1,'Rahul',1234567891234567),(2,'Raj',1234567892345678)] schema = "id int, name string, card_no long"
	maskfun_udf = udf(maskfun, StringType()) 	 /	df_final = df.withColumn('ncn', maskfun_udf(col('card_no')))
>> extract filename: df = df.withColumn("state", element_at(split(input_file_name(), "/"), -1)).withColumn("state", split("state", "\\.").getItem(0))




>> find max id excluding duplicates | 2, 6, 5, 6, 9, 9, 8: with unique_ids AS (SELECT id FROM id_list GROUP BY id HAVING COUNT(*) = 1) | SELECT MAX(id) FROM unique_ids
>> find department names where no employees are found: emp -->> employee_id, employee_name, dept_id | Department Table -->> dept_id, dept_name
   SELECT d.dept_name FROM dept d LEFT JOIN employee e ON d.dept_id = e.dept_id WHERE e.dept_id IS NULL;
>> Given two tables, find the count of records for Left Outer Join and Inner Join | Table A  & Table B | 1 1 1 1  &  1 1 1 | Ans. 12 & 12
>> table1: 1 2 3 null ""  & table2: 2 2  3 null null "" : inner join - 2 2, 2 2, 3 3, empty empty |  Null doesnt include matching, empty string includes matching
>> Give the output for DENSE_RANK() and RANK() functions for the below dataset: Nums: 85 85 80 75 75 70
>> column Country: India Australia Pakistan | Output: India vs Australia India vs Pakistan Australia vs Pakistan
    SELECT a.country || ' vs ' || b.country as country FROM country a  join country b on a.country > b.country;  
>> Given two tables, output the result of INNER, LEFT, RIGHT, FULL JOINS | Table1-  1 1 & Table2- b a 1
>> Find the 777th highest salary:  SELECT salary FROM (SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num FROM `your_table`) WHERE row_num = 777
>> number of patients per doctor including unassigned patients: docter_id | pationt_id, doctor_id
   SELECT d.doctor_id, COUNT(p.patient_id) AS total_patients FROM doctors d  LEFT JOIN patients p ON  d.doctor_id = p.doctor_id GROUP BY d.doctor_id
>> Handling NULL values using the average salary | emp_id, salary: IFNULL(cn, rep_val) COALESCE(cn, rep_val1, rep_val2, ...) 
   SELECT employee_id,  COALESCE(salary, (SELECT avg(salary) FROM table)) AS adjusted_salary  FROM table;
>> Src, beforeTgt, tgt after src ing: MERGE INTO target_table AS T  USING source_table AS S  ON T.id = S.id	
   WHEN MATCHED THEN UPDATE SET T.name = S.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (S.id, S.name);
>> ORDER_DAY, ORDER_ID, PRODUCT_ID, QUANTITY, PRICE | Get me all products that got sold both the days and the number of times the product is sold.  
   SELECT PRODUCT_ID, COUNT(*) AS COUNT FROM orders WHERE ORDER_DAY IN ('01-JUL-11', '02-JUL-11') GROUP BY PRODUCT_ID HAVING COUNT(DISTINCT ORDER_DAY) = 2;
>> i/p - customer_id, region, sales_amount | o/p - customer_id,	north_sales, south_sales, East sales, West Sales
   SELECT customer_id, SUM(CASE WHEN region = 'North' THEN sales_amount ELSE 0 END) AS north_sales, .... FROM orders GROUP BY customer_id;
   select * from (select customer_id, region, sales_amount from orders) pivot(sum(sales_amount) for region in ('north', 'south', 'east', 'west'));
>> 2 highest sal without LIMIT&OFFSET | emp, sal: WITH cte AS (SELECT sal, dense_rank() OVER(ORDER BY sal DESC) AS rk FROM employees) |select sal FROM cte WHERE rk = 2;
>> running total of revenue for each day| order_date, revenue:	SELECT  order_date, revenue, SUM(revenue) OVER (ORDER BY order_date) AS running_total FROM  orders
>> employees sal > managers | EMPID, EMPNAME, MANAGERID, SLARY: 
   SELECT  e.employee_id,  e.name,  e.salary FROM  employees e JOIN  employees m ON e.manager_id = m.employee_id WHERE  e.salary > m.salary ORDER BY  e.salary DESC;
>> department with highest avg sal | dept, salary: SELECT dept,  AVG(salary) AS average_salary FROM `table` group by  dept ORDER BY average_salary DESC LIMIT 1;
>> top 3 frequently sold products| product_name , quantity_sold: SELECT PN,  SUM(quantity_sold) AS tqs FROM  `sales_data` GROUP BY PN ORDER BY  tqs DESC LIMIT 3;
>> employees who hired in last 90 days: SELECT * FROM  employee_data` WHERE hire_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);
>> Extract the Domain from the Email column:	substring(email, strpos(email, '@')+1, length-opt) 
>> Find the 2nd Highest Salary: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees)
>> Count Of Employees in Each Department: SELECT department, COUNT(*) AS employee_count FROM employees GROUP BY department
>> duplicate records based on column: select * from `table` where email in (SELECT email FROM `table` group by email having count(*)>1);
>> Duplicate Records based on column: SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name HAVING COUNT(*)>1  /  row_number()
>> delete duplicate records: WITH CTE AS (SELECT column_name, ROW_NUMBER() OVER (PARTITION BY CN ORDER BY CN) AS rn FROM table_name) | DELETE FROM CTE WHERE rn > 1
>> Find Employees Who joined in 2024: SELECT * FROM employees WHERE EXTRACT(YEAR FROM join_date) = 2024;
>> Find Customers Without Orders: SELECT c.customer_id, c.name FROM customers c  LEFT JOIN orders o ON c.customer_id = o.customer_id WHERE o.customer_id IS NULL
>> Get Total Salary by Department: SELECT department, SUM(salary) AS total_salary FROM employees GROUP BY department
>> Find Nth Highest Salary (Nth = 3): SELECT DISTINCT salary FROM employees ORDER BY salary DESC LIMIT 1 OFFSET 2
>> Find all employees whose names contain the letters "a" exactly twice: SELECT * FROM employees WHERE LENGTH(name) - LENGTH(REPLACE(LOWER(name),'a','')) = 2
>> Running total of sales by date:  SELECT date, sales, SUM(sales) OVER (ORDER BY date) AS RunningTotal FROM sales_data
>> Write a query to count how many employees share the same salary: SELECT salary, COUNT(*) AS employee_count FROM employees GROUP BY salary HAVING COUNT(*) > 1
>> find the most frequently occurring value in a column: SELECT column_name, COUNT(*) AS freq FROM table_name GROUP BY column_name ORDER BY freq DESC LIMIT 1
>> How to get the common records from two tables: SELECT * FROM table1 INTERSECT SELECT * FROM table2
>> How to retrieve the last 10 records from a table:  SELECT * FROM employees ORDER BY employee_id DESC LIMIT 10
>> emp earning more than avg sal in their department: SELECT * FROM employees e WHERE salary > (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id))
>> records where date is within last 7 days from today: SELECT * FROM `table` WHERE date_column BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) AND CURRENT_DATE()

>> Find the top N customers who made the highest purchases, ensuring no duplicates if customers have the same purchase amount|   CustomerID, PurchaseAmount:
   WITH rp AS (SELECT *, DENSE_RANK() OVER (ORDER BY PurchaseAmount DESC) AS Rank FROM table) |	SELECT * FROM rp WHERE Rank <= N;
>> Write an efficient query to detect duplicate records in a table and delete only the extra duplicates, keeping one copy.
   WITH rr AS (SELECT *, ROW_NUMBER() OVER (PARTITION BY column1, .. ORDER BY column1) AS rn FROM table) | DELETE FROM ranked_records WHERE row_num > 1;
>> Retrieve 1 order for each customer, ensuring that ties (customers with multiple 1 orders on the same date) are handled correctly.  customer_id | order_id | order_date
   WITH ro AS (SELECT *, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date, order_id) AS rn  FROM orders) | SELECT * FROM ranked_orders WHERE rn = 1;
>> Find products that were never purchased by any customer.    t1: product_id, product_name;  t2: product_id, order_id
   SELECT p.product_id, p.product_name FROM products p LEFT JOIN orders o ON p.product_id = o.product_id WHERE o.product_id IS NULL;
>> dept with highest total salary paid| dept, sal:  SELECT dept, SUM(Sal) AS ts FROM table GROUP BY dept ORDER BY ts DESC LIMIT 1;
>> find top 3 orders per each customer| cust, OrderID, OrderVal: WITH ro AS (SELECT *, RANK() OVER (PARTITION BY cust ORDER BY OrderVal DESC) AS rk FROM table`)
>> tran_id, sale_date, amount | o/p- year, Jan, Feb, Mar: WITH sales_data AS (SELECT EXTRACT(YEAR FROM sale_date) AS year, FORMAT_DATE('%b', sale_date) AS month, amount        FROM sales) | SELECT * FROM sales_data PIVOT (SUM(amount) FOR month IN ('Jan', 'Feb', 'Mar'))	ORDER BY year;
>> Find the moving average of sales for the last 7 days for each product in a sales table | product_id, sale_amount, sale_date:
   WITH ma AS (SELECT *, AVG(SalesAmount) OVER (PARTITION BY ProductID ORDER BY SaleDate ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS avg FROM sales) | SELECT * FROM ma;
>> moving avg for each resta over a 7-day window | restarant, date, revenue: AVG(revenue) OVER (PARTITION BY resta ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) 
>> Write a query to generate a sequential ranking of products based on total sales, but reset the ranking for each year | prodct_id, sale_amount, sale_date
   SELECT *, RANK() OVER(PARTITION BY year ORDER BY total_sales DESC) AS rank FROM (SELECT EXTRACT(YEAR FROM sd) AS year, pi, SUM(sa) AS ts FROM table GROUP BY year, pi);
>> WITH CustomOrder AS (SELECT emp_name, emp_dep, CASE WHEN emp_dep = 'it' THEN 1 WHEN emp_dep = 'hr' THEN 2 WHEN emp_dep = 'sales' THEN 3 end as emp_dept_sort,     
   ROW_NUMBER() OVER(PARTITION BY emp_dep ORDER BY emp_name) AS row_numÂ  FROM table) | SELECT * FROM CustomOrder ORDER BYÂ emp_dept_sort, row_num;
>> Determine which users are likely bots based on their ordering patterns (e.g., 50+ orders/day). (user, order_timestamp, order_id)
   SELECT user_id, DATE(order_timestamp) AS order_day, COUNT(order_id) AS orders_per_day FROM orders GROUP BYÂ  user_id,Â  order_day HAVING orders_per_day >= 50;
>> Build a time-series summary table that aggregates revenue per day, per restaurant, for dashboarding.     (restaurant_id, order_day, revenue)
   SELECT  restaurant_id, order_day, SUM(revenue) AS daily_revenue FROM  orders GROUP BY  restaurant_id,  order_day;
>> Rank users based on orders and spend, giving more weight to spend. -- (0.7 * total_spend + 0.3 * total_orders) AS weighted_score
>> employees same sal as another emp in same dept | dept_id, emp_name, salary: WITH es AS (SELECT dept, sal, COUNT(*) FROM employees GROUP BY dept, sal HAVING COUNT(*) > 1)    | SELECT * FROM employees e JOIN es es ON  e.dept_id = es.dept_id AND e.salary = es.salary;
>> find customers who made transactions in every month of the year | cust_id, date, purchage_amount
   EXTRACT(MONTH FROM transaction_date) | GROUP BY customer_id, month | customer_id, COUNT(month) | WHERE months_count = 12;
>> find the first and last occurrence of each event per user | UserID, EventID, EventDate,
   ROW_NUMBER() for rn_first |  ROW_NUMBER() for rn_last | MIN(CASE WHEN rn_first = 1 THEN event_time END) AS first_occurrence
>> Find the most frequently purchased product category by each user over the past year | user, product_category, order_id, sale_date:
   user_id,  product_category, COUNT(*) & sale_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR) | RANK() | WHERE rank = 1;
   
>> cust who orders in consecutive months | cust, order_date, order: DATE_TRUNC(order_date, MONTH) | LEAD(order_month) | WHERE DATE_DIFF(next_month, order_month, MONTH) = 1
>> Write a query to detect a sudden drop (>30%) in daily revenue for any restaurant | restaurant_id, revenue_date, daily_revenue: lag to previous_revenue, sudden_drop > 30
>> Detect duplicated orders â€” same items, user, and restaurant within 5 minutes | order_id, user_id, restaurant_id, item_id, order_time:
   LAG(order_time) AS previous_order_time | TIMESTAMP_DIFF(order_time, previous_order_time, MINUTE) AS minutes_diff | minutes_diff <= 5
>> For each cuisine, identify the most loyal customer (most repeat orders)| order_id, user_id, cuisine, order_date:  ROW_NUMBER() | WHERE rank = 1;
>> return the top 5% of users by revenue contribution | user_id,  revenue: PERCENT_RANK() OVER (ORDER BY revenue DESC) AS revenue_percentile |revenue_percentile <= 0.05
>> Calculate % of abandoned carts (added to cart but not ordered) | user- user_id, item_id, event_type, event_time & order - user_id, item_id, order_id, order_time
   ROUND(100 * abandoned_count / total_added, 2) 
>> Find cities where lunch orders (12â€“3 PM) have higher revenue than dinner orders (7â€“10 PM)| order_id, user_id, city, order_time, order_amount
   order_hour |	CASE WHEN order_hour BETWEEN 12 AND 14 THEN 'lunch'  WHEN order_hour BETWEEN 19 AND 21 THEN 'dinner' ELSE NULL | pivot-city, meal-period, revenue	
>> Find restaurants whose best-selling dish makes up over 60% of their revenue | restaurant_id, dish_id,  dish_name, quantity, price
   restaurant_total_revenue, best_selling_dish,  best_dish_revenue / restaurant_revenue AS revenue_share, revenue_share > 0.6;
>> Identify consecutive login streaks for users where they logged in for at least three consecutive days | user_id, login_date
>> Identify users who have placed an order in two consecutive months but not in the third month | user_id, order_date
>> pivot & unpivot data: cricket_data = [("Virat Kohli", 'Match1', 75), ..........], col = ["Player", "Match", "score"]

date_trunc("month", col("start_date"))
datediff(col("transaction_date"), col("previous_transaction_date"))
date_sub(col("start_date"), 10)
date_add(col("start_date"), 10)







