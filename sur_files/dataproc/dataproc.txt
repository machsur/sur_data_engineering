>> Apache Spark - multi-language engine (single-node machines or clusters) suitable for large-scale data workloads, and can be --         deployed on-premises or in the cloud.
   Google Cloud Dataproc is a managed service that runs Apache Spark (and other frameworks),

>> Haddop Yarn Cluster -- master node (Yarn RM), worker node
   worker node --> App Master container (pyspark driver --> py4j --> JVM driver), executors (each 4cpu 16 gb ram - means 4 slots and memory common for 4 slots)
   Script or spark app --> divide into jobs based on action --> job divide into stages based wide trans --> shuffle/sort --> tasks (these tasks run slots)

   -- Dynamic Partition pruning in spark3.0:

>> enable API, create dataproc cluster on vm/gke.
   -- cluster type: single node 1m0w, standard for dev team 1m Nw, high availability for prod 3m Nw
   -- allocate static ip to it >> vm instance - external ip - view detais - n/w page ip addresses - reseve to static 

>> validate ssh connectivity to master node vm of dataproc cluster: gcloud ssh cammand helps to copy key on vm and able to ssh vm (gcloud compute ssh --zone "us-central1-a"    "dataprocclu1-m" --project "possible-dream-433814-b4")
   -- establish an SSH connection to a remote server VM instance: ssh -i C:\Users\surmacha\.ssh\google_compute_engine surmacha@35.184.234.232
   -- setup vs code remote window for dataproc vm of master node - create below config file in .ssh - select remote window button on vs code - host in search 
   Host Sur-DP-Cluster-VSC
    HostName 35.238.68.160
    IdentityFile C:\Users\surmacha\.ssh\google_compute_engine
    User surmacha
   -- copy material to the VM master node using git clone command.

>> copy local files in master node of vm to hdfs in dataproc/ gcs blobs into hdfs on dataproc.
   hdfs dfs -ls /  ----    hdfs dfs -ls -R /public/retail_db
   hdfs dfs -mkdir /public
   hdfs dfs -put data/retail_db /public (local to hdfs -put)
   hdfs dfs -cp gs://surretail/retail_db /public/retail_db (hdfs to hdfs, gcs to hdfs, hdfs to gcs, gcs to gcs -cp)
   hdfs dfs -rm -R -skipTrash /public/retail_db

6. validate pyspark / spark-shell (spark scala) / spark-sql CLI in dataproc cluster.
7. submit dataproc job using spark sql in UI
8. create dataproc workflow using jobs
   cleanup --> convert json file to parquet for orders/order_items --> compute daily product revenue and save back to gcs.
   >> review the scripts.
   >> apply unit tests/validation in local vs code environment configured with dataproc cluster master node.
   >> use scripts in gcs rather then hdfs (temproray based on cluster)
   >> copy spark-sql scripts to gcs location.
   >> run and validate spark-sql scripts placed in gcs using vs code configured with dataproc master node
   >> limitations of running spark sql scripts using dataproc jobs
         submtting jobs by defining query file in gcs via ui is not working (gcloud cli is working).
         submit jobs using gcloud from external cli but not from vs code dataproc master node
   >> submit spark sql jobs: list spark-sql scripts in gcs and review content then submit job using dataproc gcloud command.
   >> dataproc workflow templates: orchestrated pipeline within gcp dataproc
         UI- create template using jobs defined by query text (query file not working in job) - not use in practical
         gcloud commands (more flexibility than ui):
               1. create dataproc workflow template (gcloud dataproc workflow-templates create ----)
               2. attach workflow template with existing cluster (gcloud dataproc workflow-templates add-cluster-selector---)
               3. add jobs to workflow template (gcloud dataproc workflow-templates add-job ----)
               4. instantiate workflow template (gcloud dataproc workflow-templates instantiate ----)
   >> stop / delete cluster using ui, gcloud
----------------
>> place script in gcs location: gs://surretail/scripts/daily_product_revenue/gcsToBq.py (or) gcsToBq.sql (or) gcsToBq.scala
>> execute gcs script 
   ---> run pyspark script on master node of VM (ex: spark-submit -f gs://surretail/scripts/daily_product_revenue/gcsToBq.py)
   ---> submit pyspark job using UI 
   ---> submit spark-sql job using gcloud (gcloud dataproc jobs submit spark-sql --cluster=cluster-a695 -f          
        gs://surretail/scripts/daily_product_revenue/compute_daily_product_revenue.sql --params=bucket_name=surretail)
-------------------------------
>> Execution Flow in PySpark
   Start SparkSession → spark = SparkSession.builder.appName("App").getOrCreate()
   Load Data (from a list, CSV, Parquet, etc.)
   Apply Transformations (e.g., filter(), select(), groupBy())
   Trigger an Action (e.g., show(), collect(), count())
   Stop SparkSession → spark.stop()
------------------------------


===========================================================================================================================
PySpark Syntax: read : Join : filter : groupBy: agg : filter : select : orderBy : Limit: 
    ==>>
    df = spark.createDataFrame(data, columns)
    df_from_pandas = spark.createDataFrame(pandas_df/data_dict)
    df_empty = spark.createDataFrame([], columns)
    df_csv = spark.read.csv("/path/to/file.csv", header=True, inferSchema=True)
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
    custom_schema = StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("age", IntegerType(), True)])
    custom_schema = ''' ID Integer, Name String, Age Integer, Salary Double '''
    df = spark.read.csv("your_file.csv", schema=custom_schema, header=True)
    file_paths = ["file1.csv", "file2.csv", "file3.csv"] 
    df = spark.read.csv(file_paths, header=True, inferSchema=True, sep=',')
    df = spark.read.option("modifiedBefore", "2022-02-27T05:30:00").json("file:///path_to_file/data_files/")
    ==>> join_df = df1.join(df2, df1.department_id == df2.id/on='cn, how='inner/left/right/outer/left_semi/left_anti') 
         join_df = df1.crossjoin(df2)
         broadcast_join = df1.join(broadcast(df2), on="id", how="inner")
    ==>> filtered_df = df.filter(df['cn'] > 10 / .isNull() / .isNotNull())  or df.filter((df["Age"] > 30) &| (df["Department"] == "IT"))
    ==>> grouped_df = df.groupBy('department').agg(max("sal"))   -->> count() / agg(max("sal")) or agg(sum('Age'), sum('sal')) or max('age', 'sal) -- min avg sum /
    ==>> having_df = df.filter(grouped_df['count'] > 1)
    ==>> selected_df = df.select("customer_id", col("Name").alias('EmployeeName'), column("last_name"), df.email, df["city"] )
                    = df.selectExpr("Name as EmployeeName", "Salary as EmployeeSalary", "Department").show()
    ==>> sorted_df = df.orderBy(asc('department'), col("country).desc(), nulls_last=True)
    ==>> result = df.limit(10)
    ==>> df.show(n=3, truncate=25, vertical=True) / df.printSchema() / display(df) / print(df1.columns)


	==>> union tables: union_distinct = df1.union(df2).distinct()  /union/subtract/intersect/   -->> positional col match / schema must be identical
	     union by name -->> uni_df = df3.unionByName(df4, allowMissingColumns=True)   -->> col match by name / schema can differ / missing col handle with null values
	     distanct values/drop duplicates ----> df.select('cn1', 'cn2').distinct() ----> df.dropDuplicates(['cn1', 'cn2'])
	==>> 
	add new column ---> newdf = df.withColumn("NewColumn", lit(1))
	drop columns -->> df2 = df.drop("Country", "Region")
	column name change -->> new_df = df.withColumnRenamed("oldColumnName", "newColumnName")
	column type change --> df = df.withColumn("Phone", col("Phone").cast("string"))
	==>> df.select(initcap(col("country")))  -->> initcap / lower / upper
	==>> concatnation --> df = df.withColumn("FullName", concat(df.FirstName, lit(" "), df.LastName))
			  --> df.select(concat_ws(' | ', col("Region"), col("Country")))
	==>> splitng column -->> df = df.select(split(df.FullName, " ").alias('cn'))  -->> size, [0], explode, array_contains
	==>> result_df = df.select( col("EmployeeID"), ltrim(col("Name")), lpad(col("Name"), 10, "X"))  -->> rtrim / trim / rpad
	==>> create date, timestamp columns in df, add/subtract days, 
	     df = spark.range(10).withColumn('today', current_date())
	     df = spark.range(10).withColumn('now', current_timestamp())
	     date_sub(col("today"), 5) / date_add(col("today"), 5) / datediff(col("week_ago"), col("today")/ 
	     months_between(to_date(lit("2016-01-01")), to_date(lit("2017-01-01")) / to_date / to_timestamp / 
	==>> df.dropna("All" subset=['cn1", "cn2"]) / df.fillna("N/A" subset=['cn1", "cn2"]) / df.na.fill({'cn1':'val1', 'cn2': 'val2'}) / is null('CN') 
	==>> coalesce('cn1', 'cn2','cn3') - return first non-null value from list of columns
	     df.gruopBy("region").agg(coalesce(mean("units_sold), lit(0))
	==>> agg functions --> df.select(sum('CN')).show() -----> max, min, avg, count, countDistanct, stddev, variance, sumDistinct  ------> note: ignore null values
	     grouping and agg -----> df.groupBy('CN').agg(sum('CN')).show() -------> min, max, avg, count, concat_ws(', ', collect_list('cn')), first, last
	==>> when&otherwise -->> df = ( df.withColumn("status", when(df.age < 30, "Young").otherwise("Adult")).withColumn("income_bracket", when(df.salary < 4000, "Low") 					.when((df.salary >= 4000) & (df.salary <= 4500), "Medium") .otherwise("High")) )
	==>>cast data types -->> df = df.withColumn("column_name", col("column_name").cast("target_data_type"))
			    -->> cast_expr = [col("column1_name").cast("target_data_type1"), col("column2_name").cast("target_data_type2")] / df = df.select(*cast_expr)
	==>> window functions -->> window_spec = Window.partitionBy("category", "sub_category").orderBy(asc("timestamp"), col("score").desc()) \
                    				.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
				   window_spec = Window.partitionBy("category", "sub_category").orderBy(col("timestamp"), col("score")).rowsBetween(-2, 3)
				   df = df.withColumn("row_number", F.row_number().over(window_spec))  -->> rank(), dense_rank(), lead('cn'), lag('cn')
				   df = df.withColumn("avg_value", F.avg("value").over(window_spec))   -->> sum / min /max /
	==>> explode/explode_outer: exploded_df = df.select("Name", explode("Subjects").alias("Subject"))  -->> explode_outer
		Drops rows with null or empty arrays / Keeps rows with null or empty arrays, filling with null
	==>> print(set(df1.schema)-set(df2.schema))
	==>> df2=df1.groupBy(df1.EmpName).agg(collect_list(df1.Skill).alias('Skill'))
	     df3=df2.select(df2.EmpName,concat_ws(',',df2.Skill).alias('Skills'))
	==>>
	DataFrames are immutable in Spark—transformations result in new DataFrames, leaving the original unchanged.


==================================================================================================================================

Pyspark Interview QA:

pyspark QA:

>> What is the difference between cache() and persist() in PySpark?
   cache(): Default Storage Level: cache() uses the default storage level, which is MEMORY_ONLY. 
   persist(): Flexible Storage Levels: persist() allows you to specify different storage levels, such as MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.
>> How does Lazy Evaluation work in PySpark?
   the execution of transformations on RDDs (Resilient Distributed Datasets) or DataFrames is deferred until an action is performed. This allows PySpark to optimize the 
   execution plan and improve performance.
>> What are wide and narrow transformations in PySpark?
   Narrow transformations are those where each input partition contributes to only one output partition. Examples include: map() / filter() / flatMap()
   Wide transformations involve shuffling data across the network, as input partitions contribute to multiple output partitions. 
   groupByKey(): Groups elements with the same key. / reduceByKey(): Combines values with the same key using a specified function. /join(): Joins two datasets 
5. Explain shuffle operations in PySpark and their impact on performance.
>> What are the different persistence levels available in PySpark?
>> How does PySpark handle schema evolution in DataFrames?
>> What is broadcast join, and when should we use it?
>> Explain the difference between groupBy() and reduceByKey() in PySpark.
>> What is the use of explode() function in PySpark?

>> Key Differences
   Memory Management: On-heap memory (data in deserilization -ready for eat) is managed by the JVM and subject to GC, while off-heap memory (data in serilization form - raw format) is managed outside the JVM and not subject to GC.
   Performance: Off-heap memory can reduce GC overhead and improve performance for large datasets, but may introduce serialization/deserialization overhead12.



>> What is PySpark (python api let u use spark with python), and how is it different from Apache Spark (big data engine).
>> How do you initialize a SparkSession in PySpark?
>> How do you perform basic operations on DataFrames in PySpark?
>> What is SparkSQL, and how do you perform SQL operations on DataFrames?
   Registering the DataFrame as a Temporary View: df.createOrReplaceTempView("people")
   Running SQL Queries: spark.sql("SELECT Name, Age FROM people WHERE Age > 30").show()
>> How do you read and write data in different formats (CSV, JSON, Parquet) using PySpark?
>> What is lazy evaluation in PySpark, and why is it important?
>> Explain the role of caching and persistence in PySpark.
>> Given a DataFrame, split the data into two columns (Even, Odd) where:
    Ans. df2 = df1.withColumn('even', when(col('id') % 2 == 0, col('id'))).withColumn('odd', when(col('id')%2!=0, col('id')))
>> Read a CSV file and create a DataFrame with properties.
>> Create a DataFrame with two columns:
    Column 1: Default String
    Column 2: Default Integer
>> Explain the key differences between Apache Spark's DataFrame and RDD APIs. In which scenarios would you prefer one over the other?
    esay of use, performance, schema
>> Describe the concept of lazy evaluation in PySpark. How does it impact the execution of Spark jobs?
    Lazy evaluation allows Spark to optimize the execution plan.
>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
    similarities in 3 apis : foult tolerant, distributed, in-memory computation, immutable, lazy evaluation, internally processing as rdd for all 3 codes
    RDD: low-level api / no optimizer / oops style api / complie time error (strong type safety) / 4 lang / no schema /
    DF: high-level api / catalyst optimizer / sql style api (user friendly) / run time error (less type safety) / 4 lang / schema structured /
    DS: high-level api / optimizer (best plan execution)/ oops / complie time error (strong type safety) / 2 lang / schema structured / 
>> What is the difference between map() and flatMap() inPySpark?
    map() when you want to apply a function that returns a single value for each input element
    flatMap() when the function can return multiple values for each input element
>> How do you handle missing values in PySpark?
    select(): Use when you need to select columns without transformations.
    selectExpr(): Use when you need to apply SQL expressions or transformations while selecting columns.
>> What are transformations and actions in PySpark? Giveexamples.


3. What are broadcast variables in PySpark, and how do they optimize join operations? Provide an example scenario where a broadcast join would be beneficial.

5. How does PySpark handle data skew, and what strategies can be employed to mitigate its effects in a large dataset?

6. Explain the role of the Catalyst optimizer in PySpark. How does it improve the performance of Spark SQL queries?

>> What is the significance of partitioning in PySpark, and how does it affect the performance of data processing tasks?

>> How would you implement window functions in PySpark to calculate a moving average over a specific time window?

10. Explain the concept of checkpointing in PySpark. Why is it important in streaming applications?






2️⃣ How does .𝐜𝐨𝐥𝐥𝐞𝐜𝐭() work, and when should it be avoided?
3️⃣ Explain .countByKey() with an example.
4️⃣ What are some use cases for .take() and .takeOrdered()?
5️⃣ How does .saveAsTextFile() handle data partitioning during output?
6️⃣ What is the role of .foreach() in Spark?
7️⃣ What is the difference between .countByValue() and .countByKey()


>> What is the difference between createOrReplaceTempView and createGlobalTempView in DataFrames?
   Scope:
   createOrReplaceTempView creates a view local to the SparkSession.
   createGlobalTempView creates a view accessible across all SparkSessions within the same application.
   Lifetime:
   createOrReplaceTempView views are dropped when the SparkSession ends.
   createGlobalTempView views persist until the Spark application terminates.
18)Explain the integration of DataFrames with Spark SQL.
19)How do you optimize DataFrame operations for better performance?
20)Can you explain the difference between DataFrame.cache() and DataFrame.persist()?


26)xplain the concept of Encoders in Spark Datasets.

>> How to set the partitions in pyspark?
   Reading Data: The option("numPartitions", 10) sets the number of partitions when loading the data.
   repartition Method: This method increases or decreases the number of partitions and shuffles the data.
   df1 = df.repartition(3)
   print(f"Number of partitions  repartitioning: {df1.rdd.getNumPartitions()}")
   coalesce Method: This method reduces the number of partitions without a full shuffle, which is more efficient for reducing partitions.
   df_coalesced = df.coalesce(2)
   print(f"Number of partitions after coalescing: {df_coalesced.rdd.getNumPartitions()}")

>> Default file format in Spark (is Parquet), Why Parquet?
      Efficient Storage: Columnar format allows for better compression and encoding.
      Performance: Optimized for query performance, especially for read-heavy operations.
      Compatibility: Widely supported across various big data tools and frameworks.

>> what are the optimization techniques in pyspark?
1. Use DataFrame/Dataset over RDD
DataFrames and Datasets are optimized for performance and provide a higher-level API compared to RDDs. They leverage Spark's Catalyst optimizer and Tungsten execution engine for efficient query planning and execution12.
2. Avoid User Defined Functions (UDFs)
UDFs can be a performance bottleneck because they are not optimized by Spark's Catalyst optimizer. Instead, use built-in functions whenever possible2.
3. Optimize Partitions
Repartition: Use repartition() to increase the number of partitions for better parallelism.
Coalesce: Use coalesce() to reduce the number of partitions without a full shuffle, which is more efficient12.
4. Cache Data
Caching intermediate DataFrames can save time on repeated computations. Use df.cache() to store DataFrames in memory2.
5. Optimize Joins
Broadcast Joins: Use broadcast() for small DataFrames to avoid shuffling large datasets.
Sort-Merge Joins: Ensure data is partitioned and sorted on join keys to optimize join operations1.
6. Use Efficient Serialization Formats
Kryo Serialization: Use Kryo serialization for faster and more compact serialization compared to Java serialization3.
7. Reduce Shuffle Operations
Minimize expensive shuffle operations by optimizing data partitioning and using operations like mapPartitions() instead of map()1.
8. Disable DEBUG and INFO Logging
Reduce logging overhead by setting the log level to WARN or ERROR in production environments2.

>> what is Repartiton and coalesce?

>> How to submit the spark job?

>> When do we need to do the repartition?
   Data Skew / Increasing Parallelism / Optimizing Joins / Writing to Disk: 


>> Difference between sortby and orderby?
   sortBy is used with RDDs.
   orderBy is used with DataFrames.

>> what is map and flatmap?

>> what is the default block size (128 MB) in hdfs and how to change it?
   Edit the hdfs-site.xml File:
   <property>
       <name>dfs.blocksize</name>
       <value>268435456</value> <!-- 256 MB in bytes -->
   </property>

>> what is data skewness and salting technique?
   df.repartition(10)
   df.rdd.getNumPartitions()
   df.select(spark_partition_id().alias("partid")).groupBy("partid").count()

>> Explain about out of memory issue in spark?
   Causes of Out of Memory Issues: 
   Driver Memory:
   Collect Operations: Using collect() to gather large datasets to the driver can cause memory overflow. The driver tries to merge all results into a single object, which      might be too large to fit into the driver's memory1.
   Large Broadcast Variables: Broadcasting large variables can consume significant memory on the driver.
   Executor Memory:
   Task Execution: Executors run tasks and store intermediate data. If the tasks require more memory than allocated, it can lead to out of memory errors1.
   Shuffling Data: During operations like groupBy or join, data shuffling can cause memory issues if the data size exceeds the executor's memory capacity2.
   Solutions to Out of Memory Issues
   Increase Driver Memory: Adjust the driver memory settings using spark.driver.memory to allocate more memory to the driver2.
   Limit Result Size: Use spark.driver.maxResultSize to limit the size of results collected to the driver1.
   Increase Executor Memory: Configure executor memory using spark.executor.memory to allocate sufficient memory for task execution2.
   Memory Overhead: Set spark.executor.memoryOverhead to account for additional memory required for JVM overhead3.
   Repartition Data: Repartition large datasets to reduce the size of data processed by each executor1.

>> What is difference between spark session and spark context?

>> what is collect list and collect set and when do we use it?
   result = df.groupBy("name").agg(collect_list("value").alias("values_list")) - allows duplicates in list
   result = df.groupBy("name").agg(collect_set("value").alias("values_set")) - no duplicates in list

>> Given a dataset of Indian cities with their respective populations, write a PySpark code snippet to find the top 5 most populous cities.
     +-------------+----------+
     | City    | Population|
     +-------------+----------+
    top_5_cities = df.orderBy(col("Population").desc()).limit(5)

>> Given a DataFrame containing employee details, write a PySpark code snippet to group employees by their department and calculate the average salary for each department.
    avg_salary_by_dept = df.groupBy("Department").agg(avg("Salary").alias("AverageSalary"))

>> Write a PySpark code snippet to remove duplicate records from a DataFrame based on a composite key consisting of 'customer_id' and 'transaction_date'.
     +------------+----------------+-------------------+
     | customer_id| transaction_id | transaction_date |
     +------------+----------------+-------------------+
    df_no_duplicates = df.dropDuplicates(["customer_id", "transaction_date"])
>> input = [(1,"Sagar-Prajapati"),(2,"Alex-John"),(3,"John Cena"),(4,"Kim Joe")]
    output 
   +---+---------------+----------+---------+
   | ID| Name|First_Name|Last_Name|
   +---+---------------+----------+---------+
   | 1|Sagar-Prajapati| Sagar|Prajapati|
   | 2| Alex-John| Alex| John|
   | 3| John Cena| John| Cena|
   | 4| Kim Joe| Kim| Joe|
   +---+---------------+----------+---------+
	df = df.withColumn("fn", split(df.Name, "[- ]").getItem(0)).withColumn("ln", split(df.Name, "[- ]")[1])

>> Find the top 3 highest-paid employees from each department.
   data = [(1, "Amit", "IT", 90000),

>> Write a PySpark code to remove duplicate records based on a specific column.
   data = [(101, "Mumbai", "Maharashtra"), ]

>> Write a PySpark query to calculate the moving average of sales over the last 3 months.
    window_spec = Window.orderBy("Month").rowsBetween(-2, 0)
    window_spec = Window.orderBy("Month").rowsBetween(Window.unboundedPreceding, Window.currentRow)
    df_with_moving_avg = df.withColumn("Moving_Avg", avg(col("Sales")).over(window_spec))

12. Write a PySpark query to count the number of null values in each column of a DataFrame.
14. Write a PySpark query to replace null values in a specific column with the previous non-null value.

>> Word count program in pyspark
    df1 = df.select(explode(split(df["name"], " ")).alias("word")).groupBy("word").agg(count("*").alias("count"))



