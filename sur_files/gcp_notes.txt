
>> Vertex AI: provides tools to implement ML workflow
   ingest, analyse, transform: using managed datasets, label & annotate data
   create and train the model: AutoML- for image, video, text, tablar data it works great & custom - for frameworks, architexture
   evaluate model for efficiency & opti: explainable AI (understand about model like which has more impact)
   deploy the model : scalable H/W
   predictions: get predections via cli, sdk, ui, api

>> 

============================

serverless vs fully-managed vs hybrid:
>> "pay as you use". No traffic, you pay nothing. 
   Ex: Cloud Run, Cloud Function, AppEngine standard, firestore, datastore, dataproc, dataflow, ai-platform 
>> managed but not serverless. 
   You always have a minimal number of VM/node up and you pay for these, traffic or not.           
   However, you have nothing to worry about: patching, updates, networking, backups, HA, redundancy(...) are managed for       you.  
   Ex:like Cloud SQL, BigTable or Spanner, AppEngine flex
>> hybrid product: 
   you pay as you use the processing (BigQuery) or the traffic (Cloud Storage), 
   storage is always billed if you have no traffic.
   ex: Cloud Storage or BigQuery:

================================
>> Managed services in GCP: IaaS, PaaS, CaaS, FaaS, serverless - no infra visibility, zero request zero cost, pay for requests not servers. 
   compute engine - IaaS
   App Engine - PaaS, CaaS (simple), serverless(1+2)
   cloud run - CaaS (simple), serverless
   cloud function - FaaS, serverless(1+2+3)
   GKE - CaaS (needs cluster)

>> Why containers: microservices (flexibility in diff lang) - but deployment complex - thats where container 
   docker is popular tool - create docker image with runtime, app code, dependcies - run image to create containers - adv: light weight, isolated, cloud nuetral
   container orchestration sol (k8s) - offer features: autoscaling, service discovery, load balance, self healing, zero downtime deployments
>> Cluster --> Node Pool --> node
   App --> Container --> Pod, deployment, service, security (iam), logging & Monitoring
>> 
1. gcloud container clusters create
2. login into cloud shell
3. connect cluster: gcloud container clusters get-credentials my-cluster --zone us-central1-c --project my-kubernetes-project-304910
4. deploy microservice to k8s:  
   kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE (docker image pushed to docker hub)
   kubectl get deployment
   expose deploy: 
   kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
   kubectl get services
   kubectl get services --watch
   curl 35.184.204.214:8080/hello-world
5. increase no of instances of microservice: kubectl scale deployment hello-world-rest-api --replicas=3
6. increase no of nodes of k8s cluster: gcloud container clusters resize my-cluster --node-pool default-pool --num-nodes=2 --zone=us-central1-c
7. Autoscaling for microservice
   kubectl autoscale deployment hello-world-rest-api --max=4 --cpu-percent=70
   kubectl get hpa
8. Autoscaling cluster
   gcloud container clusters update my-cluster --enable-autoscaling --min-nodes=1 --max-nodes=10
9. create config map - app to talk db
   kubectl create configmap hello-world-config --from-literal=RDS_DB_NAME=todos
   kubectl get configmap
   kubectl describe configmap hello-world-config
10. k8s secreats instead of config map
    kubectl create secret generic hello-world-secrets-1 --from-literal=RDS_PASSWORD=dummytodos
    kubectl get secret
    kubectl describe secret hello-world-secrets-1
   kubectl apply -f deployment.yaml
11. deploy new ms with gcpu
    gcloud container node-pools list --zone=us-central1-c --cluster=my-cluster
    kubectl get pods -o wide
12. delete microservice 
     kubectl delete service hello-world-rest-api
     kubectl delete deployment hello-world-rest-api
13. delete cluster
    gcloud container clusters delete my-cluster --zone us-central1-c

===============================

>> Secreat Manager: to store, manage and access screats.  use case- passwords, api keys, tls cert, etc..

===============================
pubsub - messaging system for stream analytics (kafca)
use cases - real time data ingestion, iot devices / parallel processing / replicate data among db 

publisher(IOT, App) --> sends message --> | Topic (here msg storage system) --> Subscription | -->
message --> subscriber(BQ)
subscriber ack message from subscription -  pull / push
one to many / many to many / many to one

IOT (publisher) --> pubsub (stream pipeline) --> dataflow (etl / subscriber) --> BQ--> Looker 
                        gcs (batch pipeline) -->

=================================

ways to authenticate GCP:
>> Application Default Credentials (ADC) in GCP: Default authentication in GCP-managed environments
   Local: Use gcloud auth application-default login for local development.
   Cloud: Deploy the code to GCP, where ADC will use the default service account.
   gcloud auth application-default set-quota-project powerful-layout-445408-p5 
   gcloud auth application-default login    (Quota project "powerful-layout-445408-p5" was added to ADC which can be used by Google client libraries for billing and quota.)
   gcloud auth application-default print-access-token (to verofy credintial)
>> Service Account Key file: Local scripts accessing GCP resources.  / used for on-prim app, other cloud, CI/CD pipelines
   in Bash:   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your-service-account-key.json"
   In Python: os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/path/to/service-account-key.json"
>> OAuth 2.0: For web apps requiring user interaction / You are building a web app that allows users to access their Google Drive.
>> Using User Credentials: Easy for local development
   gcloud auth login / gcloud auth application-default login
>> Workload Identity Federation: Your app is running on AWS Lambda, and you need it to access GCP Storage.

================================
Ways to interact with GCP:
UI / CLI - Cloud shell (preconfigured cloud SDK) & Terminal/cmd (install cloud sdk) / Client libraries / Rest api

================================

IAM
Identity: user, groups, SA, domain
Resource
Roles – primitive, predefined, custom
Policy – binding identity with role
SA- default, user-maintained, google-maintained

App on vm – accessing GCS bucket
On-prim - accessing GCS bucket (long-lived)
On-prim – GCP API (short-lived)
Static website - accessing GCS bucket (public access)

Bucket level accesses – IAM (uniform)
Object level accesses – ACL (fine-grained)
Signed url – user (gcp account not required) can read object from GCS using signurl for specified period.

==============================

GKE:
https://www.okteto.com/blog/kubernetes-basics/

==============================

data base concepts:
Fact tables - contain numerical data, 
dimension tables - provide context and background information.

Star schema contains a fact table surrounded by dimension tables.
Snowflake schema is surrounded by dimension table which are in turn surrounded by dimension table

===============================

Dataprep for data preparation (cleaning, wrangling) for analysis and ML 
- built by trifacta 3rd party tool – need to share data with trifacta (dis adv)
- serverless
- automatically detects schema and anomalies
 
Raw – prepare data using dataprep – run job on dataflow/trifacta(for small dataset) – dataflow job created – 

===============================

Could build – builds container image deployed to cloud run – cloudrun see metrics, versions, traffic
Pre-requisites:
Cloudfunctions api / cloud build api (its ci-cd tool to deploy function)/cloud logging api/ cloud pubsub api/ eventarc api

Project IAM admin role – 

===============================

Cloud composer:
Manages apache airflow env on GCP

===============================

Cloudrun:
Container to production in sec

Automate Python script execution on GCP

 
https://github.com/rafaello9472/c4ds/tree/main/Automate%20Python%20script%20execution%20on%20GCP%20

===============================





