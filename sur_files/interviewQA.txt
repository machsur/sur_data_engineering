

Python is the go-to language for data engineering, but it comes with hidden traps! Hereâ€™s how to crush them:
ğŸ”¹ ğŸ­. ğ—œğ—»ğ—²ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ—Ÿğ—¼ğ—¼ğ—½ğ˜€ ğ—¼ğ—» ğ—Ÿğ—®ğ—¿ğ—´ğ—² ğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜ğ˜€
âŒ Using for loops on millions of records.
âœ… Use vectorized operations with Pandas (df.apply(), df.groupby()), or leverage PySpark for big data.
ğŸ”¹ ğŸ®. ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—Ÿğ—²ğ—®ğ—¸ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ—Ÿğ—®ğ—¿ğ—´ğ—² ğ——ğ—®ğ˜ğ—®ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜€
âŒ Keeping huge DataFrames in memory.
âœ… Use chunking (pd.read_csv(chunksize=10000)) or Dask for parallel processing.
ğŸ”¹ ğŸ¯. ğ— ğ˜‚ğ˜ğ—®ğ—¯ğ—¹ğ—² ğ——ğ—²ğ—³ğ—®ğ˜‚ğ—¹ğ˜ ğ—”ğ—¿ğ—´ğ˜‚ğ—ºğ—²ğ—»ğ˜ğ˜€
âŒ Defining functions like def my_func(data=[]): (mutable lists persist across calls!)
âœ… Use None instead: def my_func(data=None): data = data or [].
ğŸ”¹ ğŸ°. ğ—œğ—´ğ—»ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—˜ğ˜…ğ—°ğ—²ğ—½ğ˜ğ—¶ğ—¼ğ—» ğ—›ğ—®ğ—»ğ—±ğ—¹ğ—¶ğ—»ğ—´ ğ—¶ğ—» ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²ğ˜€
âŒ Assuming everything will work smoothly.
âœ… Use try-except, logging, and monitoring tools like Airflow & Datadog.
ğŸ”¹ ğŸ±. ğ—£ğ—¼ğ—¼ğ—¿ ğ—£ğ—®ğ—¿ğ—®ğ—¹ğ—¹ğ—²ğ—¹ğ—¶ğ˜€ğ—º ğ—¦ğ˜ğ—¿ğ—®ğ˜ğ—²ğ—´ğ˜†
âŒ Using Pythonâ€™s multiprocessing in distributed environments (doesn't work well with Spark!).
âœ… Use Spark, Dask, or Ray for true parallel execution across clusters.
ğŸ”¹ ğŸ². ğ—™ğ—®ğ—¶ğ—¹ğ—¶ğ—»ğ—´ ğ˜ğ—¼ ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—² ğ—¦ğ—¤ğ—Ÿ ğ—¤ğ˜‚ğ—²ğ—¿ğ—¶ğ—²ğ˜€ ğ—¶ğ—» ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—»
âŒ Writing inefficient SQL queries inside Python (pandas.read_sql()).
âœ… Use EXPLAIN plans, indexing, and Snowflakeâ€™s query optimization.
ğŸ”¹ ğŸ³. ğ—¢ğ˜ƒğ—²ğ—¿ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—£ğ—®ğ—»ğ—±ğ—®ğ˜€ ğ—³ğ—¼ğ—¿ ğ—•ğ—¶ğ—´ ğ——ğ—®ğ˜ğ—®
âŒ Pandas struggles with GBs/TBs of data.
âœ… Use PySpark, Dask, or DuckDB instead for scalability.
ğŸ”¹ ğŸ´. ğ—›ğ—®ğ—¿ğ—±ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ—–ğ—¼ğ—»ğ—³ğ—¶ğ—´ğ˜‚ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€
âŒ Storing database credentials directly in code.
âœ… Use environment variables, .env files, or secrets management tools.
ğŸ”¹ ğŸµ. ğ—¡ğ—¼ğ˜ ğ—¨ğ˜€ğ—¶ğ—»ğ—´ ğ—§ğ˜†ğ—½ğ—² ğ—›ğ—¶ğ—»ğ˜ğ˜€ & ğ—Ÿğ—¶ğ—»ğ˜ğ—¶ğ—»ğ—´
âŒ Writing messy, unstructured code.
âœ… Use mypy for type checking, Black for formatting, and pylint for better code quality.
ğŸ”¹ ğŸ­ğŸ¬. ğ—¢ğ˜ƒğ—²ğ—¿ğ—¹ğ—¼ğ—¼ğ—¸ğ—¶ğ—»ğ—´ ğ—Ÿğ—¼ğ—´ğ—´ğ—¶ğ—»ğ—´ & ğ— ğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´
âŒ Debugging with print() statements.
âœ… Use logging module, centralized logging (like ELK, Datadog), and structured logs for debugging.


1. âœ… What is the difference between ETL and ELT?

2. âœ… How do you handle schema evolution in a data pipeline?

3. âœ… Explain the CAP theorem and its impact on distributed systems?

4. âœ… What are the different types of partitioning in databases, and why are they important?

ğŸ’¡ ğ—¦ğ—¤ğ—Ÿ & ğ——ğ—®ğ˜ğ—® ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¶ğ—»ğ—´:

5. âœ… How do you optimize complex SQL queries?

6. âœ… What is a slowly changing dimension (SCD), and how do you implement it?

7. âœ… How do you handle duplicate records in a large dataset?

8. âœ… What are common indexing techniques to improve query performance?

âš¡ ğ—•ğ—¶ğ—´ ğ——ğ—®ğ˜ğ—® & ğ—–ğ—¹ğ—¼ğ˜‚ğ—±:

9. âœ… Compare Apache Spark and Apache Flink for data processing?

10. âœ… How does Kafka ensure message durability and fault tolerance?

11. âœ… When would you choose Databricks over AWS Glue or Azure Data Factory?

12. âœ… Explain the differences between Delta Lake, Apache Iceberg, and Hudi?

ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» & ğ—–ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ—–ğ—µğ—®ğ—¹ğ—¹ğ—²ğ—»ğ—´ğ—²ğ˜€:

âœ… Write a Python script to process a 1TB CSV file efficiently?

âœ… How would you implement a real-time data deduplication system?

âœ… Explain the use of Pandas, NumPy, and PySpark for data engineering?

ğŸ” ğ—¦ğ—°ğ—²ğ—»ğ—®ğ—¿ğ—¶ğ—¼-ğ—•ğ—®ğ˜€ğ—²ğ—± ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€:

âœ… What steps would you take if an incremental data load pipeline fails?

âœ… How would you design a scalable data pipeline for streaming and batch processing?

âœ… You need to migrate petabytes of data from an on-premises system to the cloudâ€”what approach would you take?
================================
python:
>> Given an array, find the minimum and maximum values.
    ans. builtin mtds, sorting, python code
>> Given a string, count the char occurrences of word in a dictionary:
    Input: 'aaabbbccddeeeee'
    Output: { â€œaâ€ : 3 , â€œbâ€ : 3, â€œcâ€ : 2, â€œdâ€ : 2 , â€œeâ€ : 5 }
>> Write a Python script to count the frequency of characters in a text file.
>> Write a Python program to count the frequency of an word in a text file.
>> Reversing a String using an Extended Slicing techniques. builtin / slicing / code
>> Count Vowels from Given words .
>> Find the highest occurrences of each word from string and sort them in order.
>> Remove Duplicates from List.
>> Sort a List without using Sort keyword.
>> Find the pair of numbers in this list whose sum is n no.
>> Find the max and min no in the list without using inbuilt functions.
>> Read and print values from an Excel file using Pandas.
    df = pd.read_excel('your_file.xlsx')
>> Handle file exceptions (missing or corrupted Excel files) gracefully.
>> Calculate the Intersection of Two Lists without using Built-in Functions

>> What is a docstring in Python?
    special kind of comment used to document what a function, class, or module does. 
    appears right after the definition of a function, method, class, or module. 
    Docstrings are enclosed in triple quotes (""" or ''') and can span multiple lines.
>> What is pass in Python? When is it used?
    Placeholder for future class/function/loop definition
    It does nothing when executed 
>> Which data structure occupies more memory: list or tuple? Why?
    In Python, lists generally occupy more memory than tuples. 
    This is because lists are mutable, meaning their size and contents can change, while tuples are immutable, meaning their size and contents are fixed once created.


Write a Python script to create a palindrome with a given number of alphabets.
    Example: For n=3 (alphabets: a, b, c) â†’ Palindrome: abcba
>> Write Python code to make API requests to a public API (e.g., weather API) and process the JSON response.
>> Implement a function to fetch data from a database table, perform data manipulation, and update the database.

===============================================================================================================
pyspark QA:

2. What is the difference between cache() and persist() in PySpark?
3. How does Lazy Evaluation work in PySpark?
4. What are wide and narrow transformations in PySpark?
5. Explain shuffle operations in PySpark and their impact on performance.
6. What are the different persistence levels available in PySpark?
7. How does PySpark handle schema evolution in DataFrames?
8. What is broadcast join, and when should we use it?
9. Explain the difference between groupBy() and reduceByKey() in PySpark.
10. What is the use of explode() function in PySpark?
11. Find the top 3 highest-paid employees from each department.
Sample Data:
data = [
(1, "Amit", "IT", 90000),
(2, "Neha", "HR", 50000),
(3, "Raj", "IT", 85000),
(4, "Priya", "HR", 60000),
(5, "Suresh", "Finance", 75000),
(6, "Anjali", "Finance", 80000),
(7, "Vikas", "IT", 92000),
(8, "Rohan", "HR", 58000),
(9, "Meera", "Finance", 82000)
]
12. Write a PySpark query to count the number of null values in each column of a DataFrame.

13. Write a PySpark code to remove duplicate records based on a specific column.
Sample Data:
data = [
(101, "Mumbai", "Maharashtra"),
(102, "Delhi", "Delhi"),
(103, "Bangalore", "Karnataka"),
(101, "Mumbai", "Maharashtra"),
(104, "Pune", "Maharashtra")
]

14. Write a PySpark query to replace null values in a specific column with the previous non-null value.

15. Write a PySpark query to calculate the moving average of sales over the last 3 months.
Basic Pyspark Questions:

ğŸ“ What is PySpark, and how is it different from Apache Spark?
ğŸ“ How do you initialize a SparkSession in PySpark?
ğŸ“ What are RDDs in PySpark, and how are they created?
ğŸ“ Explain transformations and actions in RDDs.
ğŸ“ How do you perform basic operations on DataFrames in PySpark?
ğŸ“ What is SparkSQL, and how do you perform SQL operations on DataFrames?
ğŸ“ How do you read and write data in different formats (CSV, JSON, Parquet) using PySpark?
ğŸ“ What is lazy evaluation in PySpark, and why is it important?
ğŸ“ Explain the role of caching and persistence in PySpark.
>> Given a DataFrame, split the data into two columns (Even, Odd) where:
    Ans. df2 = df1.withColumn('even', when(col('id') % 2 == 0, col('id'))).withColumn('odd', when(col('id')%2!=0, col('id')))
>> Read a CSV file and create a DataFrame with properties.
>> Create a DataFrame with two columns:
    Column 1: Default String
    Column 2: Default Integer
>> Explain the key differences between Apache Spark's DataFrame and RDD APIs. In which scenarios would you prefer one over the other?
    esay of use, performance, schema
>> Describe the concept of lazy evaluation in PySpark. How does it impact the execution of Spark jobs?
    Lazy evaluation allows Spark to optimize the execution plan.
>> What is the difference between RDD, DataFrame, and Dataset (best of rdd & df) ? 3 api to intract with spark for developers
    similarities in 3 apis : foult tolerant, distributed, in-memory computation, immutable, lazy evaluation, internally processing as rdd for all 3 codes
    RDD: low-level api / no optimizer / oops style api / complie time error (strong type safety) / 4 lang / no schema /
    DF: high-level api / catalyst optimizer / sql style api (user friendly) / run time error (less type safety) / 4 lang / schema structured /
    DS: high-level api / optimizer (best plan execution)/ oops / complie time error (strong type safety) / 2 lang / schema structured / 
>> What is the difference between map() and flatMap() inPySpark?
    map() when you want to apply a function that returns a single value for each input element
    flatMap() when the function can return multiple values for each input element
>> How do you handle missing values in PySpark?
    select(): Use when you need to select columns without transformations.
    selectExpr(): Use when you need to apply SQL expressions or transformations while selecting columns.
>> What are transformations and actions in PySpark? Giveexamples.

3. What are broadcast variables in PySpark, and how do they optimize join operations? Provide an example scenario where a broadcast join would be beneficial.

4. Discuss the various persistence levels available in PySpark's caching mechanism. When would you use `MEMORY_ONLY` versus `MEMORY_AND_DISK_SER`?

5. How does PySpark handle data skew, and what strategies can be employed to mitigate its effects in a large dataset?

6. Explain the role of the Catalyst optimizer in PySpark. How does it improve the performance of Spark SQL queries?

7. What is the significance of partitioning in PySpark, and how does it affect the performance of data processing tasks?

8. Describe the process of handling real-time streaming data using PySpark's Structured Streaming. What are the key components involved?

9. How would you implement window functions in PySpark to calculate a moving average over a specific time window?

10. Explain the concept of checkpointing in PySpark. Why is it important in streaming applications?

>> Given a dataset of Indian cities with their respective populations, write a PySpark code snippet to find the top 5 most populous cities.
     +-------------+----------+
     | City    | Population|
     +-------------+----------+
    top_5_cities = df.orderBy(col("Population").desc()).limit(5)
>> Given a DataFrame containing employee details, write a PySpark code snippet to group employees by their department and calculate the average salary for each department.
    avg_salary_by_dept = df.groupBy("Department").agg(avg("Salary").alias("AverageSalary"))

>> Write a PySpark code snippet to remove duplicate records from a DataFrame based on a composite key consisting of 'customer_id' and 'transaction_date'.
     +------------+----------------+-------------------+
     | customer_id| transaction_id | transaction_date |
     +------------+----------------+-------------------+
    df_no_duplicates = df.dropDuplicates(["customer_id", "transaction_date"])


ğ‘ğ¨ğ®ğ§ğ ğŸ: ğ“ğğœğ¡ğ§ğ¢ğœğšğ¥ - ğŸ

ğ’ğ©ğšğ«ğ¤ (ğğ²ğ’ğ©ğšğ«ğ¤):

1)Modify the word count code to display results in descending order of frequency.
2)Why is reduceByKey preferred over groupByKey?
3)What is lineage in Spark?
4)Difference between cache and persist in Spark.
5)Is fault tolerance the same in Spark and Hadoop?

 ğ’ğğ‹

6)Explain query execution order.
7)What are the different types of joins in SQL?
8)Explain the difference between DENSE_RANK and RANK.
9)What is a cursor in SQL?10)What is a stored procedure in SQL?


1ï¸âƒ£ ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¢ğ§ğ  ğ¢ğ§ ğ’ğ©ğšğ«ğ¤, ğšğ§ğ ğ°ğ¡ğ² ğ¢ğ¬ ğ¢ğ­ ğ¢ğ¦ğ©ğ¨ğ«ğ­ğšğ§ğ­?
ğ‡ğ¢ğ§ğ­: Discuss how partitioning divides data into smaller chunks for parallel processing and its role in minimizing shuffling.
2ï¸âƒ£ ğ‡ğ¨ğ° ğœğšğ§ ğ²ğ¨ğ® ğœğ¨ğ§ğ­ğ«ğ¨ğ¥ ğ­ğ¡ğ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬ ğ¢ğ§ ğšğ§ ğ‘ğƒğƒ?
ğ‡ğ¢ğ§ğ­: repartition() and coalesce() are key methods.
3ï¸âƒ£ ğ–ğ¡ğšğ­ ğ¡ğšğ©ğ©ğğ§ğ¬ ğ¢ğŸ ğ­ğ¡ğ ğ©ğšğ«ğ­ğ¢ğ­ğ¢ğ¨ğ§ğ¬ ğšğ«ğ ğ§ğ¨ğ­ ğğ¢ğ¬ğ­ğ«ğ¢ğ›ğ®ğ­ğğ ğğ¯ğğ§ğ¥ğ² ğšğœğ«ğ¨ğ¬ğ¬ ğ­ğ¡ğ ğœğ¥ğ®ğ¬ğ­ğğ«?
ğ‡ğ¢ğ§ğ­: Talk about data skew and its impact on performance.

ğŸ” ğŠğğ² ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬: ğ‹ğšğ³ğ² ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§:
4ï¸âƒ£ ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ¥ğšğ³ğ² ğğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğ’ğ©ğšğ«ğ¤, ğšğ§ğ ğ¡ğ¨ğ° ğğ¨ğğ¬ ğ¢ğ­ ğ¨ğ©ğ­ğ¢ğ¦ğ¢ğ³ğ ğ©ğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ?
ğ‡ğ¢ğ§ğ­: Transformations are only evaluated when an action is invoked, allowing Spark to build an optimized DAG.
5ï¸âƒ£ ğğšğ¦ğ ğš ğŸğğ° ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§ğ¬ ğšğ§ğ ğšğœğ­ğ¢ğ¨ğ§ğ¬ ğ¢ğ§ ğ’ğ©ğšğ«ğ¤. ğ‡ğ¨ğ° ğğ¨ ğ­ğ¡ğğ² ğğ¢ğŸğŸğğ«?
ğ‡ğ¢ğ§ğ­: Examples include map() and filter() (transformations) vs. count() and collect() (actions).

ğŸ” Key Questions: Lineage
6ï¸âƒ£ ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ¥ğ¢ğ§ğğšğ ğ ğ¢ğ§ ğ’ğ©ğšğ«ğ¤, ğšğ§ğ ğ°ğ¡ğ² ğ¢ğ¬ ğ¢ğ­ ğ¢ğ¦ğ©ğ¨ğ«ğ­ğšğ§ğ­ ğŸğ¨ğ« ğŸğšğ®ğ¥ğ­ ğ­ğ¨ğ¥ğğ«ğšğ§ğœğ?
ğ‡ğ¢ğ§ğ­: Lineage helps Spark track the series of transformations, enabling recomputation of lost partitions.
7ï¸âƒ£ ğ‡ğ¨ğ° ğğ¨ğğ¬ ğ’ğ©ğšğ«ğ¤ ğ«ğğœğ¨ğ¦ğ©ğ®ğ­ğ ğ¦ğ¢ğ¬ğ¬ğ¢ğ§ğ  ğğšğ­ğš ğ®ğ¬ğ¢ğ§ğ  ğ¥ğ¢ğ§ğğšğ ğ?
ğ‡ğ¢ğ§ğ­: It rebuilds only the affected partition based on the transformation chain.

ğŸ” ğŠğğ² ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬: ğƒğ€ğ† (ğƒğ¢ğ«ğğœğ­ğğ ğ€ğœğ²ğœğ¥ğ¢ğœ ğ†ğ«ğšğ©ğ¡)
8ï¸âƒ£ ğ„ğ±ğ©ğ¥ğšğ¢ğ§ ğ­ğ¡ğ ğ«ğ¨ğ¥ğ ğ¨ğŸ ğƒğ€ğ† ğ¢ğ§ ğ’ğ©ğšğ«ğ¤. ğ‡ğ¨ğ° ğğ¨ğğ¬ ğ¢ğ­ ğğ¢ğŸğŸğğ« ğŸğ«ğ¨ğ¦ ğŒğšğ©ğ‘ğğğ®ğœğ?
ğ‡ğ¢ğ§ğ­: DAG allows Spark to execute operations as a graph of stages, enabling better optimization compared to MapReduce.
9ï¸âƒ£ ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ­ğ¡ğ ğ¬ğ¢ğ ğ§ğ¢ğŸğ¢ğœğšğ§ğœğ ğ¨ğŸ ğ¬ğ­ğšğ ğğ¬ ğ¢ğ§ ğƒğ€ğ† ğğ±ğğœğ®ğ­ğ¢ğ¨ğ§?
ğ‡ğ¢ğ§ğ­: Stages are determined by shuffle boundaries, and each stage can be executed in parallel.

ğŸ” ğŠğğ² ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬: ğğğ«ğ¬ğ¢ğ¬ğ­ğğ§ğœğ/ğ’ğ­ğ¨ğ«ğšğ ğ ğ‹ğğ¯ğğ¥ğ¬

ğŸ”Ÿ ğ–ğ¡ğšğ­ ğšğ«ğ ğ­ğ¡ğ ğ¬ğ­ğ¨ğ«ğšğ ğ ğ¥ğğ¯ğğ¥ğ¬ ğšğ¯ğšğ¢ğ¥ğšğ›ğ¥ğ ğ¢ğ§ ğ’ğ©ğšğ«ğ¤, ğšğ§ğ ğ°ğ¡ğğ§ ğ°ğ¨ğ®ğ¥ğ ğ²ğ¨ğ® ğ®ğ¬ğ ğğšğœğ¡?
ğ‡ğ¢ğ§ğ­: MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc., depending on memory availability and use case.

Here are some interview questions to test your knowledge of transformations:
1ï¸âƒ£ Why are transformations lazy in Spark?
2ï¸âƒ£ What are narrow and wide transformations? Provide examples.
3ï¸âƒ£ What is the difference between .map() and .flatMap()?
4ï¸âƒ£ Explain .filter() transformation with a real-world example.
5ï¸âƒ£ When would you use .reduceByKey() vs .groupByKey()?
6ï¸âƒ£ What is the difference between .union() and .intersection() transformations?
7ï¸âƒ£ Can you explain the .join() operation on RDDs?

1ï¸âƒ£ What are the most commonly used RDD actions, and how do they differ?
2ï¸âƒ£ How does .ğœğ¨ğ¥ğ¥ğğœğ­() work, and when should it be avoided?
3ï¸âƒ£ Explain .countByKey() with an example.
4ï¸âƒ£ What are some use cases for .take() and .takeOrdered()?
5ï¸âƒ£ How does .saveAsTextFile() handle data partitioning during output?
6ï¸âƒ£ What is the role of .foreach() in Spark?
7ï¸âƒ£ What is the difference between .countByValue() and .countByKey()

ğ‘ğƒğƒ (ğ‘ğğ¬ğ¢ğ¥ğ¢ğğ§ğ­ ğƒğ¢ğ¬ğ­ğ«ğ¢ğ›ğ®ğ­ğğ ğƒğšğ­ğšğ¬ğğ­) :

1)What is an RDD in Spark, and why is it called resilient?
2)Explain how RDDs achieve fault tolerance.
3What are the two types of RDD operations, and how do they differ?
4)What are narrow and wide transformations? Provide examples.
5)How is data partitioning handled in RDDs, and why is it important?
6)What are the limitations of RDDs compared to DataFrames and Datasets?
7)How do you create an RDD from an external file or in-memory collection?
8)Explain the difference between map() and flatMap() transformations in RDD.
9)How does caching or persistence work in RDDs, and why is it used?10)What are the pros and cons of RDDs compared to DataFrames and Datasets?.

ğƒğšğ­ğšğ…ğ«ğšğ¦ğğ¬:

11)What is a DataFrame in Spark, and how is it different from an RDD?
12)Explain the role of Catalyst Optimizer in Spark DataFrames.
13)What are the advantages of using DataFrames over RDDs?
14)How can you create a DataFrame from a CSV, JSON, or Parquet file?
15)What are the common operations performed on DataFrames, such as select, filter, and groupBy?
16)How do you handle schema in DataFrames?
17)What is the difference between createOrReplaceTempView and createGlobalTempView in DataFrames?
18)Explain the integration of DataFrames with Spark SQL.
19)How do you optimize DataFrame operations for better performance?
20)Can you explain the difference between DataFrame.cache() and DataFrame.persist()?

ğƒğšğ­ğšğ¬ğğ­ğ¬:

21)What is a Dataset in Spark, and how is it different from a DataFrame?
22)Explain the significance of type safety in Datasets.
23)What are the key advantages of Datasets over DataFrames and RDDs?
24)How can you create a Dataset in Spark?
25)What is the difference between map() in RDDs and Datasets?
26)xplain the concept of Encoders in Spark Datasets.
27)What are some use cases where you would prefer a Dataset over a DataFrame?



====================================================================================
SQL QA:
>> Given two tables, find the count of records for Left Outer Join and Inner Join:
Table A: Table B:
1 1
1 1
1 1 
1 
Ans. 12 & 12
>> Give the output for DENSE_RANK() and RANK() functions for the below dataset:
Nums 
85 
85 
80 
75 
75 
70

>> Given a table with column 'Country', select data in the below sequence:
Table: Matches
Country 
India 
Australia 
Pakistan 
Output:
India vs Australia 
India vs Pakistan 
Australia vs Pakistan
Ans. 
    SELECT a.countryc || ' vs ' || b.countryc as country
    FROM
      `powerful-layout-445408-p5.sur_test_ds.countryt` a
    join
      `powerful-layout-445408-p5.sur_test_ds.countryt` b
    on a.countryc > b.countryc;  -----> or <
>> Given two tables, output the result of INNER, LEFT, RIGHT, FULL JOINS.
Table1:
col1 
---- 
1 
1 
Table2:
---- 
b 
a 
1 
>> Find the 777th highest salary from a table.
ğŸ”¹ Identify customers who placed orders in consecutive months.
ğŸ”¹ Query to get the total number of patients per doctor, including unassigned patients.
>> Handling NULL values in employee salary using the average salary.
    IFNULL(expression, replacement_value) -->> It takes two arguments. If the first argument is NULL, it returns the second argument.
    COALESCE(expression1, expression2, ..., expressionN) -->> It can take multiple arguments. It returns the first non-NULL argument from the list.
>> Difference between Subquery and Materialized Views.
    Performance: Materialized views can improve query performance by storing precomputed results, while subqueries are recalculated each time the main query runs.
    Storage: Materialized views consume storage space to store the precomputed results, whereas subqueries do not.
    Maintenance: Materialized views need to be refreshed to stay up-to-date with the underlying data, while subqueries always reflect the current state of the data.
>> CTE vs Subquery in SQL and their performance impact.
    Readability and Maintainability:
    Optimization: Subquery Treated similarly to CTEs in terms of optimization. The query optimizer will attempt to optimize the entire query, including the subqueries.
    Reusability:
ğŸ”¹ Steps to debug a slow SQL query.





===============================================================================================
others:

âœ… Tell me about yourself, your projects, and the tech stack you have used.
âœ… What does your day-to-day work look like?
âœ… Why are you using the tech stack you are using?
âœ… What is an alternative to Medallion Architecture?
âœ… What is the kind and size of data you deal with on a daily basis?
âœ… If the business is using JSON as a file format, how would you convince them to use Parquet instead?
âœ… Tell me about yourself, projects, and tech stack.
âœ… Explain the Spark architecture.
âœ… Explain how jobs run in Spark.
âœ… Follow-up: What does the Catalyst Optimizer do?
âœ… Difference between Logical Plan and Physical Plan.
âœ… ORC vs. Parquet â€“ Whatâ€™s the difference?
ğ‘ğ¨ğ®ğ§ğ ğŸ‘ â€“ ğ‡ğ¢ğ«ğ¢ğ§ğ  ğŒğšğ§ğšğ ğğ« ğƒğ¢ğ¬ğœğ®ğ¬ğ¬ğ¢ğ¨ğ§
âœ…Difference between Data Lake and Delta Lake.
âœ… Why did you quit your previous job?
âœ… Even though you have an offer in hand, why did you apply again?
âœ… If we offer you the same salary as your current offer, will you accept it?
âœ… You are settled in Hyderabad; why are you willing to relocate to Bangalore?
