=============================================================================
Python:
>> Interpreter: The programme that runs your code.
   Virtual Environment: A separate "workspace" that uses a Python interpreter but has its own packages and dependencies.
>> pip - tool for install python packages
   pypi - centralized repository for py packages
>>  Create a Virtual Environment, Activate the Virtual Environment, Deactivate the Virtual Environment
    install packages into virtual env, download packages from venv
>> terminate the python programme, execute the file using the specified program or interpreter (#!/usr/bin/python3.5)
>> logging
>> Hiding Passwords and Secret Keys in Environment Variables (linux/mac/Windows) and access them into code -------> os.environ.get("key")
>> try: / except / else / finally:
>> with open('sur.log', 'r') as file:
>> Types of Config Files, requests module, unique identifiers (UUIDs)
>> Modules (py file) vs package (collection of modules)
>> Handlling errors: compile time error: Syntatical - missing /Logical error: wrong output 2+3 = 7 /run time error:  5/0.
>>  break / continue / pass
>> if, if else, nested if, if elif else / while loop / for loop / for else
>> List, tuple, set, dictionary, array
>> functions: reuse block of code / 
    positional / keyworded / variable length: def sum(a, *b): / keyword variable length: def person(name, **data)
    Global variable (outside fun) / local variable (inside fun)
    recursion: function calling it self ex: factorial
    anonymous function: x = lambda a: a * a / print(x(5))
    filter: list(filter(lambda a: a%2 == 0, lt))
    map: list(map(lambda a: a * 2, lt))
    reduce: from functools import reduce / sum = reduce(lambda a, b: a + b, lt) /
>> create python packages
    -- package python code as .whl file
    -- setuptools (build and package projects), whell (create & install .whl files), twine (upload packages to pypi)
    -- create code, __init__.py, setup.py 
        python setup.py bdist_wheel - created wheel file
        install whl package

>> memory shared tasklist of each programe, python programme is one of task list - within this portion memory used.
    Threading: by default, the python programee executes on single threading
    Multi threading: no control in execution, more memory and faster execution - NO (program execute on one thread and             other threads are pause)

=================================
NumPY and Pandas:

>>  Array: same data type (1-D) - Simple, basic collections (like lists) for small collections.
    NumPy: N-D - leniar algebra library - numerical calculations for large datasets.
    Pandas: Data analysis library for handling and manipulating structured data, great for dataframes (tables), series.

>> Data scientist or data analytics - Answer the quetions on large data points

>> why Python Pandas
    Excel - Can not handle large sets of data
    Python - Need to write code to find answers
    Pandas dataframe: its python modules - makes data scientist life easier and effective

>> Series is very similar to a NumPy array. 
    Series can have axis labels. 
    it can hold any arbitrary Python Object.
    Dataframe : combination of series.


Create dataframe:
labels = ['a','b','c']
my_list = [10,20,30]
arr = np.array([10,20,30])
d = {'a':10,'b':20,'c':30}

df = pd.read_csv('Salar2.csv')
df1 = client.query(sql).to_dataframe(create_bqstorage_client=False)
df2 = pd.DataFrame(data = [['sure', 20], ['ses', 25], ['siv', 30]], index = ['a', 'b', 'd'], columns = ['name', 'age']) or
from numpy.random import randn
df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split())

----------

df.head(2)
df.set_index("Id", inplace=True)
df = df.reindex(new_index)

df['W'] / df[['W', 'Y']] / df['new'] = df['W'] + df['Y']
df.loc['C'] / df.loc[['C', 'D']] / df.loc['C', 'X'] / df.loc[['C', 'E'], ['X', 'Z']]
type(df) / type(df['BasePay'])
df.drop('new',axis=1,inplace=True)
df[df['W']>0][['Y','X']]
df[(df['W']>0) &| (df['Y'] > 1)]

>>  Cleaning Missing values:
    df.dropna(axis=0, how='any', thresh=2, subset=['col1', 'col2'], inplace=False)
    df.fillna(value=2 or df['col1'].mean() or {'col1': 2, 'col2': 4}, method='ffill', axis=0, inplace=False, limit=2)
    df.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=None)

>>  df.groupby('Company')['column'].mean()/std()/min()/max()/count()
    df.groupby('Company').describe().transpose()

>> combining data frames: concat, merge, join
    pd.concat([df1,df2,df3], axis=1) 
    pd.merge(left, right, how='outer/inner/left/right', on=['key1', 'key2']) - based on keys
    left.join(right, how='outer') - combining the columns of two potentially differently-indexed DataFrames 

>> df['col2'].unique() / df['col2'].nunique() / df['col2'].value_counts()
    df.drop_duplicates(subset='Name') / df.drop_duplicates(subset=['col2'], keep='last').reset_index(drop=True) / df.duplicated().sum() / 
    newdf = df[(df['col1']>2) & (df['col2']==444)]
    df['col1'].apply(lambda x: x * 2 (or) times2 function) /  df['col3'].apply(len) /
    del df['col1'] / df.columns / df.index /  df.isnull() /
    df.sort_values(by='col2') #inplace=False by default /
    df.pivot_table(values='D',index=['A', 'B'],columns=['C'])

>> Spiting:

df[['fn', 'ln']] = df['EmployeeName'].str.split(' ', expand = True)
or
df[['ffn', 'lln']] = df['EmployeeName'].apply(lambda x: pd.Series(str(x).split(' ')))

df.head(2)


---------------



--------------
data type casting:

df["Date"] = pd.to_datetime(df["Date"], format='%d/%m/%Y') or df["Date"] = df["Date"].astype('datetime64[ns]')
df["Id"] = df["Id"].astype(str)
df["Id"] = df["Id"].astype(float)
df["Id"] = df["Id"].astype(int)

----------------

cleaning wrong data:

df.loc[5, 'BasePay'] = 110000

for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.drop(i, inplace = True)
        
for i in df.index:
    if df.loc[i, 'BasePay'] > 150000:
        df.loc[i, 'BasePay'] = 10000
        

=======================================

df.duplicated()
df.drop_duplicates(inplace = True)

=======================================

df2.corr()

============================

import matplotlib.pyplot as plt
df2.plot(kind='scatter', x = 'Year', y = 'BasePay')
plt.show()

===========================
data structures (containers storing data in memory) + code instructions = software apply
selecting right data structure for a given problem - makes efficient programme

bigo - measure running time or space rwuirements for programme as input size grows

time complexity (o(1) // log(n) //  o(n) // o(n2) //

=============================================
pyspark:

>> Big data problem: volume, varity, velocity (rdbms failed to handle)
   Big data solution
	Monolithic approach: massive resource (massive system) - vartical scalable, not foult tolerant, expensive
	distributed approach: resource pool (cluster) - Horizantal scalable, foult tolerable & HA, economica
   Haddop came as disributed big data processing platform
	Yarn (cluster operating system), HDFS (distributed storage), HMR (distributed computing)
	other tools developed on top hadoop core platform:
	HBASE hive - databases
	Sqoop - data ingetion tool
	pig - scripting language
	Flume - move large vol of web server log, event data to HDFS or Hbase
	Oozie - workflow scheduling tool for Hadoop jobs

>> Why Apache spark:
	- opensource in-memory distributed/cluster/parallel processing engine for big data. (cluster - Haddop Yarn/ k8s / apache mesos/ apache standlone
	- provides api in java, scala, python, r
	- supports batch processing, real-time analytics, ml and graph processing

>> Overview of spark and its architexture

-- data processing: source (files,db, rest payloads) -----> process (sql, libraries/pandas/dask, frameworks/spark) ------->
    target(files,db, rest payloads)

-- data processing libraries: python pandas/limitations whth large datasets - introduces python dask 
(uses server efficiently and multi nodes)  - pyspark works well spark clusers with multiple nodes when compared to dask 

-- distributed computung: ex: hadoop and spark
    single server (etl) - problem is sla in prod and capcity issues even if you use efficiently with multithreading
    cluster (have multiple server)

-- databricks have spark based runtime - AWS (EMR & GLUE), both spark embedded in them - in gcp dataproc, we got hadoop and spark - Azure synaps, spark embedded it - 
    snow flake most popular DWH they introduced snowpark based on spark

-- spark: multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
    key features: batch/streaming data - sql analytics  - datascience at scale - machine learning
    architexture: https://spark.apache.org/docs/latest/cluster-overview.html
    cluster capacity, spark cluster: cluster(D,W) nodes/OS/Spark engine/executors(JavaVM) in worker, vcpu or slots unused capacity, tasks used capacity/network/


=====================
- problem: big data processing - cant handle by excel , sql db, etc.
- Sol : parallel processing - spark 
        https://spark.apache.org/
- Spark : 
        > dirver node (spark contest) - cluster/resource manager - worker nodes (executor, task, cache).
        > ram in worker nodes has divided into partitions - these partitions can be inside specific data structure RDD (write code in py) / DF (strucre databases).
        > RDD - read only - immutable - fault tolerant using DAG concept - 1. transformation (new rdd - not human readble) 2. action (to read)

why PySpark not python for bigdata? :
python - take sample of data and apply statistics to develop histograms
pyrhon - can process large datasets with multithreads 
problem - when data is big and stays in memory / we cant take entire data on compute and use python on it.
        - data cant stay in one computer and it can be distributed.
- Platforms for pyspark:
        > https://colab.research.google.com/
        > VM vare on desktop version
        > databricks community edition / on cloud
        > dataproc on GCP



=========
https://github.com/itversity
https://github.com/dgadiraju?tab=repositories
https://spark.apache.org/

=================

==========================
>> Why linux rather than winndows, macos:  open source & free to use, multi-users work on shared system , good security (no antivirus req), stable and reliable

>> user --> give commands on terminal --> shell(program/CLI) interprets and executes/passes them on --> OS
  type of shell: bash, etc.
  why bash shell: feature-rich, fast, very common

>> Bash Script structure:
    shebang line - tell to shell what type of script running like bash, py, etc
    commands
    exit statement - 0 successful; 1-255 unsuccessful
    chmod + to give executable permissions for script

    script more professional: comments (#) / Author: / Date Created / Date Modified / Description / Usage

>> 
-- sudo su - , useradd - seshuu , passwd seshuu, su - seshuu , sudo apt install ncal
-- nproc, cal 2023, cat /etc/shells, echo $SHELL
-- touch FN, vi/vim/nano FN
-- execute script: bash script_name, script_name (once user has X permission) 
-- convert shell script onto bash script (shebang) -- #!/bin/bash (in first line of script)
-- editing path variable (./hello.py - hello.py works now) --- echo $PATH, export PATH=$PATH:/home/surmacha/scripts
-- adding comments #
-- variables: 
	make script generic & dynamic 
	datatype is blind/untyped, 
	constant variable (cant reassign later ex-PI) - readonly PI=3.1452
	command substution -- today=$(command) or `command` -- echo $today


=================================================================================================================================

Q&A

>> Given an array, find the minimum and maximum values.
    ans. builtin mtds, sorting, python code
>> Given a string, count the char occurrences of word in a dictionary:
    Input: 'aaabbbccddeeeee'
    Output: { “a” : 3 , “b” : 3, “c” : 2, “d” : 2 , “e” : 5 }
>> Write a Python script to count the frequency of characters in a text file.
>> Write a Python program to count the frequency of an word in a text file.
>> Reversing a String using an Extended Slicing techniques. builtin / slicing / code
>> Count Vowels from Given words .
>> Find the highest occurrences of each word from string and sort them in order.
>> Remove Duplicates from List.
>> Sort a List without using Sort keyword.
>> Find the pair of numbers in this list whose sum is n no.
>> Find the max and min no in the list without using inbuilt functions.
>> Read and print values from an Excel file using Pandas.
    df = pd.read_excel('your_file.xlsx')
>> Handle file exceptions (missing or corrupted Excel files) gracefully.
>> Calculate the Intersection of Two Lists without using Built-in Functions

>> What is a docstring in Python?
    special kind of comment used to document what a function, class, or module does. 
    appears right after the definition of a function, method, class, or module. 
    Docstrings are enclosed in triple quotes (""" or ''') and can span multiple lines.
>> What is pass in Python? When is it used?
    Placeholder for future class/function/loop definition
    It does nothing when executed 
>> Which data structure occupies more memory: list or tuple? Why?
    In Python, lists generally occupy more memory than tuples. 
    This is because lists are mutable, meaning their size and contents can change, while tuples are immutable, meaning their size and contents are fixed once created.


Write a Python script to create a palindrome with a given number of alphabets.
    Example: For n=3 (alphabets: a, b, c) → Palindrome: abcba
>> Write Python code to make API requests to a public API (e.g., weather API) and process the JSON response.
>> Implement a function to fetch data from a database table, perform data manipulation, and update the database.

===============================================================================================================


